1，ID协同过滤，只能考虑交互，无法纳入多模态信息，例如点赞，差评，关注等信息-----A Behavior-aware Graph Convolution Network
Model for Video Recommendation
2，下一步要做的BERT4Rec
加入多模态，MicroLens
加入其他多样化信息，Tencent
优化负样本的选取

原先方式：BRP+若干负样本1、5、10. HR且尝试多种model调整方式，包括增加隐藏层，提高初始化维度等方式HR@10接近热门推荐
现在方式：同批次内交叉熵 相比于前者，提升近10倍
原因：待分析

lightgcn中训练时，发现100epoch后，损失降低很多，但是最终HR@10指标（0.0211）反而较20epoch低（0.0280），10epoch(0.0321), 5epoch(0.0328), 3epoch(0.0312), 1epoch (0.0262)

SASRec hr@10指标解决popular，同时发现交叉熵损失下降极其缓慢，相较于其他Seq模型来说（推测是SASRec模型太多复杂，收敛极其缓慢），最终发现是平均交互数只有7，我开始设置最大交互数为20，这意味着大量的训练不可避免的变成了用padding预测padding这样的无效训练，反而干扰了模型的收敛和学习

一个值得探讨的问题是，为什么同为SEQ的模型GRU4REC和NEXTITEMNET没有严重受到影响。

在MMGCN的图中已经证明了，完全不依赖多模态的，和依赖多模态的，效果几乎差不多！合理怀疑，在涉及多模态的模型中，多模态相关参数相比于交互相关参数是严重弱化的，不对结果产生限制影响。
一个值得探讨的问题是：用户的交互记录已经天然受到多模态信息的影响，由此产生的用户向量和物品向量已经由通过统计学意义上的用户行为把多模态信息纳入其中了。在此基础上再次拼接，很可能影响非常有限，并且使得最终的用户向量和物品向量偏离原有的以交互为主导的训练结果。而交互或者说点击率才是推荐行为的落脚点。

虽说我们在这里对

关于我的测试结果，刚开始采用cosine相似度，产生了收敛缓慢的问题。BRP作为损失函数，训练结果很差。推测为负采样数量远远不够，在选用cosine相似度的情况下，会训练出大量模长极大的向量。而在softmax模式下，假设每batch采样1024个，其中对于每个真实交互来说，仅有一次作为正样本出现，其他1023次都作为负样本出现，如果该向量模长极大，虽然作为正样本来说能获得较高命中率，但是其余1023次作为负样本来说，都显著提高了损失函数，所以有较大压力对大模长进行惩罚，不需要太担心大模长产生的过拟合问题。

需要介绍的有协同过滤，MF，DSSM等五种，BRP损失函数，softmax方法，负采样技术，lmdb，faiss，向量快速查找技术，MMGCN，HoME .
MMGCN中，对于不同模态分别考虑，比如说对于图像模态，选用预提取的图像嵌入，初始化一个图像模态专用的用户嵌入，通过图的传播，更新图像模态的用户嵌入，使其接近有交互关系的图像嵌入，（附加公式），最后对于物品侧，将不同模态的物品表示进行相加。用户侧，将不同模态的用户表示进行相加。

{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from google.colab import drive\n",
    "import shutil\n",
    "import os\n",
    "def copy_from_drive(src_path, dst_path):\n",
    "\n",
    "    if os.path.exists(dst_path):\n",
    "        print(f\"skip:{dst_path} exists\")\n",
    "        return\n",
    "\n",
    "    if os.path.isdir(src_path):\n",
    "        shutil.copytree(src_path, dst_path)\n",
    "    elif os.path.isfile(src_path):\n",
    "        shutil.copy(src_path, dst_path)\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "copy_from_drive('/content/drive/MyDrive/tool', '/content/tool')\n",
    "copy_from_drive('/content/drive/MyDrive/MicroLens-50k_pairs.csv','/content/MicroLens-50k_pairs.csv')"
   ],
   "id": "365b909de4764032"
  },
  {
   "cell_type": "code",
   "id": "b828e981",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T12:50:33.313152Z",
     "start_time": "2025-06-07T12:50:24.994875Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from networkx.readwrite.json_graph import adjacency\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tool import preprocess\n",
    "from tool import customdataset\n",
    "from tool import evaluate\n",
    "!pip install faiss-cpu\n",
    "import faiss\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "e99a31348e0a553",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T12:50:33.369715Z",
     "start_time": "2025-06-07T12:50:33.324174Z"
    }
   },
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "76a2087d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T12:50:34.182839Z",
     "start_time": "2025-06-07T12:50:34.035344Z"
    }
   },
   "source": [
    "path = 'MicroLens-50k_pairs.csv'\n",
    "user = 'user'\n",
    "item = 'item'\n",
    "user_id = 'user_id'\n",
    "item_id = 'item_id'\n",
    "timestamp = 'timestamp'\n",
    "save_dir = './embeddings'\n",
    "top_k = 10\n",
    "num_workers = 10\n",
    "k_neg = 10\n",
    "# path = pd.read_csv('MicroLens-50k_pairs.csv')"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "943fc6c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T12:50:34.217714Z",
     "start_time": "2025-06-07T12:50:34.201507Z"
    }
   },
   "source": "dataset_pd,num_users,num_items = preprocess.openAndSort(path,user_id=user,item_id=item,timestamp='timestamp')",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    user   item      timestamp\n",
       "0  36121   9580  1583378629552\n",
       "1  26572   9580  1583436719018\n",
       "2  37550   9580  1584412681021\n",
       "3  14601   9580  1584848802432\n",
       "4  15061   9580  1585388171106\n",
       "5   6364   9580  1585390736041\n",
       "6   3542   9580  1585404918503\n",
       "7  21038   9580  1590144594477\n",
       "8  12538  14631  1634867362929\n",
       "9  47592  14631  1634872254913"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36121</td>\n",
       "      <td>9580</td>\n",
       "      <td>1583378629552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26572</td>\n",
       "      <td>9580</td>\n",
       "      <td>1583436719018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37550</td>\n",
       "      <td>9580</td>\n",
       "      <td>1584412681021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14601</td>\n",
       "      <td>9580</td>\n",
       "      <td>1584848802432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15061</td>\n",
       "      <td>9580</td>\n",
       "      <td>1585388171106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6364</td>\n",
       "      <td>9580</td>\n",
       "      <td>1585390736041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3542</td>\n",
       "      <td>9580</td>\n",
       "      <td>1585404918503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>21038</td>\n",
       "      <td>9580</td>\n",
       "      <td>1590144594477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12538</td>\n",
       "      <td>14631</td>\n",
       "      <td>1634867362929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>47592</td>\n",
       "      <td>14631</td>\n",
       "      <td>1634872254913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "6d42c375",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T12:50:34.448028Z",
     "start_time": "2025-06-07T12:50:34.415887Z"
    }
   },
   "source": [
    "train_df, test_df = preprocess.split(dataset_pd,user, item, timestamp)\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Test size: {len(test_df)}\")"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "8b8ae0c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T12:50:34.672397Z",
     "start_time": "2025-06-07T12:50:34.523061Z"
    }
   },
   "source": [
    "# maintain a map from new id to old id, new id for constructing matrix\n",
    "user2id = {u: i for i, u in enumerate(dataset_pd[user].unique())}\n",
    "item2id = {i: j for j, i in enumerate(dataset_pd[item].unique())}\n",
    "\n",
    "# apply to train_df and test_df\n",
    "train_df[user_id] = train_df[user].map(user2id)\n",
    "train_df[item_id] = train_df[item].map(item2id)\n",
    "test_df[user_id] = test_df[user].map(user2id)\n",
    "test_df[item_id] = test_df[item].map(item2id)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T12:50:36.206199Z",
     "start_time": "2025-06-07T12:50:36.185327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------- 超参数 ----------\n",
    "MAX_SEQ_LEN   = 20          # 输入序列长度\n",
    "EMBEDDING_DIM = 64          # item / user embedding 维度\n",
    "HIDDEN_SIZE   = 64          # GRU 隐藏维度（可与 EMBEDDING_DIM 相同）\n",
    "NEG_SAMPLE    = 5           # 训练时每个正样本采负样本数\n",
    "BATCH_SIZE    = 256\n",
    "EPOCHS        = 10\n",
    "LR            = 1e-3\n",
    "SEED          = 42\n",
    "# ----------------------------\n",
    "\n",
    "# ---------- 随机种子 ----------\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "# ----------------------------\n",
    "\n",
    "# ---------- 常量 ----------\n",
    "PAD_IDX  = num_items              # padding 专用 id（不与真实 item 冲突）\n",
    "N_ITEMS  = num_items + 1          # Embedding 行数（含 PAD）\n",
    "# ----------------------------\n",
    "\n"
   ],
   "id": "6e56658ac26b07e2",
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "a7aba75c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T12:50:36.269499Z",
     "start_time": "2025-06-07T12:50:36.261103Z"
    }
   },
   "source": [
    "# ======== 模型 ======== #\n",
    "class GRU4RecBPR(nn.Module):\n",
    "    def __init__(self, n_items=N_ITEMS,\n",
    "                 embedding_dim=EMBEDDING_DIM, hidden_size=HIDDEN_SIZE,\n",
    "                 num_layers=1, pad_idx=PAD_IDX):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_items, embedding_dim, padding_idx=pad_idx)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True)\n",
    "        self.proj = (nn.Linear(hidden_size, embedding_dim, bias=False)\n",
    "                     if hidden_size != embedding_dim else nn.Identity())\n",
    "\n",
    "    def forward(self, seq):\n",
    "        \"\"\"\n",
    "        seq : (B, T) → 返回用户向量 h_hat : (B, D)\n",
    "        \"\"\"\n",
    "        emb  = self.embedding(seq)            # (B, T, D)\n",
    "        out, _ = self.gru(emb)                # (B, T, H)\n",
    "        h     = out[:, -1, :]                 # (B, H) 最后一步\n",
    "        return self.proj(h)                   # (B, D)\n",
    "\n",
    "\n",
    "    def get_items_embedding(self,item_ids,l2_norm=True):\n",
    "        i = self.item_emb(item_ids)          # (B, emb_dim)\n",
    "\n",
    "        i_vec = self.mlp_item(i)            # (B, d)\n",
    "\n",
    "        if l2_norm:\n",
    "            i_vec = F.normalize(i_vec, p=2, dim=1)\n",
    "        return i_vec\n",
    "\n",
    "    def save_embeddings(self, num_users, num_items, device, save_dir='./embeddings'):\n",
    "        import os\n",
    "        import faiss\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        self.eval()\n",
    "        self.to(device)\n",
    "\n",
    "        item_ids = torch.arange(num_items, dtype=torch.long, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            item_embeds = self.get_items_embedding(item_ids, l2_norm=True)\n",
    "\n",
    "        item_embeds = item_embeds.cpu().numpy().astype(np.float32)\n",
    "\n",
    "        # 保存向量\n",
    "        np.save(f\"{save_dir}/item_embeddings.npy\", item_embeds)\n",
    "\n",
    "        # 构建 FAISS index（使用内积）\n",
    "        dim = item_embeds.shape[1]\n",
    "        index = faiss.IndexFlatIP(dim)\n",
    "        index.add(item_embeds)\n",
    "\n",
    "        faiss.write_index(index, f\"{save_dir}/item_index.faiss\")\n",
    "        print(\"Saved user/item embeddings and FAISS index.\")\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "f21704ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T12:50:36.284727Z",
     "start_time": "2025-06-07T12:50:36.279522Z"
    }
   },
   "source": [
    "# # ======== 损失 ======== #\n",
    "# def bpr_loss(pos_score, neg_score):\n",
    "#     \"\"\"\n",
    "#     BPR  pairwise  loss\n",
    "#     pos_score : (B,)\n",
    "#     neg_score : (B, n_neg)\n",
    "#     \"\"\"\n",
    "#     diff = pos_score.unsqueeze(-1) - neg_score\n",
    "#     return -torch.log(torch.sigmoid(diff) + 1e-8).mean()"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "07e268c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T12:50:36.330578Z",
     "start_time": "2025-06-07T12:50:36.319538Z"
    }
   },
   "source": [
    "# ======== 训练流程 ======== #\n",
    "def train_model(model, train_df, epochs,lr , batch_size, test_df=None, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "    train_loader  = customdataset.build_seq_loader(train_df, batch_size=batch_size,\n",
    "                         shuffle=True, num_workers=10,pad_idx=PAD_IDX,max_len=MAX_SEQ_LEN,user_id=user,item_id=item_id)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        dt_start = datetime.now()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            hist, pos = batch\n",
    "            hist, pos = hist.to(device), pos.to(device)\n",
    "\n",
    "            # 1. 前向传播（返回预测向量）\n",
    "            h = model(hist)                      # (B, D)\n",
    "\n",
    "            # 2. 得分矩阵：每个 user 对所有正 item 的打分\n",
    "            logits = torch.matmul(h, pos.T)  # shape: (B, B)\n",
    "\n",
    "            # 3. 构造标签：每个 user 的正确 item 在对角线（即位置 i）\n",
    "            labels = torch.arange(logits.size(0), device=device)  # [0, 1, ..., B-1]\n",
    "\n",
    "            # 4. Cross Entropy Loss\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "            # 5. 反向传播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # 日志\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        dt_end = datetime.now()\n",
    "        dt = dt_end - dt_start\n",
    "\n",
    "        print(f\"[Epoch {epoch:02d}/{epochs}] avg InBatch Softmax Loss = {avg_loss:.4f}, time = {dt.total_seconds():.2f}s\")\n",
    "\n",
    "    return\n"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T12:50:36.371982Z",
     "start_time": "2025-06-07T12:50:36.365209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ======== 构建 (user → 历史张量) ======== #\n",
    "def build_hist_tensors(df, max_len=MAX_SEQ_LEN, pad_idx=PAD_IDX,user_id=user_id,item_id=item_id):\n",
    "    user_hist_t = {}\n",
    "    for uid, items in df.groupby(user_id)[item_id]:\n",
    "        seq = items.tolist()[-max_len:]\n",
    "        seq = [pad_idx] * (max_len - len(seq)) + seq\n",
    "        user_hist_t[uid] = torch.tensor(seq, dtype=torch.long).unsqueeze(0)  # (1,T)\n",
    "    return user_hist_t"
   ],
   "id": "9bc49db2aee62849",
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "10c90823",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T12:50:36.462699Z",
     "start_time": "2025-06-07T12:50:36.450337Z"
    }
   },
   "source": [
    "\n",
    "# def evaluate_ranking(\n",
    "#         test_df,              # DataFrame, 必含 user_id / item_id\n",
    "#         train_df,             # DataFrame, 用来构建用户→已交互物品集合\n",
    "#         score_fn,             # callable(users_tensor, items_tensor) → np.array\n",
    "#         num_items,            # 物品总数\n",
    "#         k=10,                 # Hit@K / NDCG@K\n",
    "#         num_neg=100,          # 每个正样本采多少负样本\n",
    "#         user_col='user_id',\n",
    "#         item_col='item_id',\n",
    "#         seed=42\n",
    "#     ):\n",
    "#     \"\"\"\n",
    "#     不依赖具体模型，只要提供 score_fn 就能评估。\n",
    "#     score_fn: 接收 (user_tensor, item_tensor) 并返回同长度的 Numpy 分数向量。\n",
    "#     \"\"\"\n",
    "#     random.seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#\n",
    "#     # 用户历史，用于采负样本 & 过滤\n",
    "#     train_user_dict = (\n",
    "#         train_df.groupby(user_col)[item_col].apply(set).to_dict()\n",
    "#     )\n",
    "#\n",
    "#     hits, ndcgs = [], []\n",
    "#\n",
    "#     for _, row in test_df.iterrows():\n",
    "#         u = int(row[user_col])\n",
    "#         pos_item = int(row[item_col])\n",
    "#\n",
    "#         # ---------- 负采样 ----------\n",
    "#         neg_items = set()\n",
    "#         while len(neg_items) < num_neg:\n",
    "#             neg = random.randint(0, num_items - 1)\n",
    "#             if neg not in train_user_dict.get(u, set()) and neg != pos_item:\n",
    "#                 neg_items.add(neg)\n",
    "#\n",
    "#         item_candidates = list(neg_items) + [pos_item]\n",
    "#\n",
    "#         # ---------- 评分 ----------\n",
    "#         users_t  = torch.LongTensor([u] * len(item_candidates))\n",
    "#         items_t  = torch.LongTensor(item_candidates)\n",
    "#         scores   = score_fn(users_t, items_t)        # ← 只依赖 score_fn\n",
    "#         rank_idx = np.argsort(scores)[::-1]          # 降序\n",
    "#         ranked_items = [item_candidates[i] for i in rank_idx]\n",
    "#\n",
    "#         # ---------- 指标 ----------\n",
    "#         if pos_item in ranked_items[:k]:\n",
    "#             hits.append(1)\n",
    "#             rank_pos = ranked_items.index(pos_item)\n",
    "#             ndcgs.append(1 / np.log2(rank_pos + 2))\n",
    "#         else:\n",
    "#             hits.append(0)\n",
    "#             ndcgs.append(0)\n",
    "#\n",
    "#     hit_rate = float(np.mean(hits))\n",
    "#     ndcg     = float(np.mean(ndcgs))\n",
    "#     return hit_rate, ndcg"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "cd29a40c68841c9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T13:24:01.057232Z",
     "start_time": "2025-06-07T12:50:36.508711Z"
    }
   },
   "source": [
    "    # ------------------ 训练 ------------------\n",
    "model = GRU4RecBPR(n_items=N_ITEMS,\n",
    "                 embedding_dim=EMBEDDING_DIM, hidden_size=HIDDEN_SIZE,\n",
    "                 num_layers=1, pad_idx=PAD_IDX)\n",
    "device = model.to(device)\n",
    "train_model(model=model,epochs=50, train_df=train_df,batch_size=1024,lr=LR,test_df=test_df,device=device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Step 1/1015\n",
      "[Epoch 1] Step 100/1015\n",
      "[Epoch 1] Step 200/1015\n",
      "[Epoch 1] Step 300/1015\n",
      "[Epoch 1] Step 400/1015\n",
      "[Epoch 1] Step 500/1015\n",
      "[Epoch 1] Step 600/1015\n",
      "[Epoch 1] Step 700/1015\n",
      "[Epoch 1] Step 800/1015\n",
      "[Epoch 1] Step 900/1015\n",
      "[Epoch 1] Step 1000/1015\n",
      "Epoch 01 | BPR loss = 0.7656\n",
      "[Epoch 2] Step 1/1015\n",
      "[Epoch 2] Step 100/1015\n",
      "[Epoch 2] Step 200/1015\n",
      "[Epoch 2] Step 300/1015\n",
      "[Epoch 2] Step 400/1015\n",
      "[Epoch 2] Step 500/1015\n",
      "[Epoch 2] Step 600/1015\n",
      "[Epoch 2] Step 700/1015\n",
      "[Epoch 2] Step 800/1015\n",
      "[Epoch 2] Step 900/1015\n",
      "[Epoch 2] Step 1000/1015\n",
      "Epoch 02 | BPR loss = 0.6004\n",
      "[Epoch 3] Step 1/1015\n",
      "[Epoch 3] Step 100/1015\n",
      "[Epoch 3] Step 200/1015\n",
      "[Epoch 3] Step 300/1015\n",
      "[Epoch 3] Step 400/1015\n",
      "[Epoch 3] Step 500/1015\n",
      "[Epoch 3] Step 600/1015\n",
      "[Epoch 3] Step 700/1015\n",
      "[Epoch 3] Step 800/1015\n",
      "[Epoch 3] Step 900/1015\n",
      "[Epoch 3] Step 1000/1015\n",
      "Epoch 03 | BPR loss = 0.5054\n",
      "[Epoch 4] Step 1/1015\n",
      "[Epoch 4] Step 100/1015\n",
      "[Epoch 4] Step 200/1015\n",
      "[Epoch 4] Step 300/1015\n",
      "[Epoch 4] Step 400/1015\n",
      "[Epoch 4] Step 500/1015\n",
      "[Epoch 4] Step 600/1015\n",
      "[Epoch 4] Step 700/1015\n",
      "[Epoch 4] Step 800/1015\n",
      "[Epoch 4] Step 900/1015\n",
      "[Epoch 4] Step 1000/1015\n",
      "Epoch 04 | BPR loss = 0.4796\n",
      "[Epoch 5] Step 1/1015\n",
      "[Epoch 5] Step 100/1015\n",
      "[Epoch 5] Step 200/1015\n",
      "[Epoch 5] Step 300/1015\n",
      "[Epoch 5] Step 400/1015\n",
      "[Epoch 5] Step 500/1015\n",
      "[Epoch 5] Step 600/1015\n",
      "[Epoch 5] Step 700/1015\n",
      "[Epoch 5] Step 800/1015\n",
      "[Epoch 5] Step 900/1015\n",
      "[Epoch 5] Step 1000/1015\n",
      "Epoch 05 | BPR loss = 0.4728\n",
      "[Epoch 6] Step 1/1015\n",
      "[Epoch 6] Step 100/1015\n",
      "[Epoch 6] Step 200/1015\n",
      "[Epoch 6] Step 300/1015\n",
      "[Epoch 6] Step 400/1015\n",
      "[Epoch 6] Step 500/1015\n",
      "[Epoch 6] Step 600/1015\n",
      "[Epoch 6] Step 700/1015\n",
      "[Epoch 6] Step 800/1015\n",
      "[Epoch 6] Step 900/1015\n",
      "[Epoch 6] Step 1000/1015\n",
      "Epoch 06 | BPR loss = 0.4703\n",
      "[Epoch 7] Step 1/1015\n",
      "[Epoch 7] Step 100/1015\n",
      "[Epoch 7] Step 200/1015\n",
      "[Epoch 7] Step 300/1015\n",
      "[Epoch 7] Step 400/1015\n",
      "[Epoch 7] Step 500/1015\n",
      "[Epoch 7] Step 600/1015\n",
      "[Epoch 7] Step 700/1015\n",
      "[Epoch 7] Step 800/1015\n",
      "[Epoch 7] Step 900/1015\n",
      "[Epoch 7] Step 1000/1015\n",
      "Epoch 07 | BPR loss = 0.4687\n",
      "[Epoch 8] Step 1/1015\n",
      "[Epoch 8] Step 100/1015\n",
      "[Epoch 8] Step 200/1015\n",
      "[Epoch 8] Step 300/1015\n",
      "[Epoch 8] Step 400/1015\n",
      "[Epoch 8] Step 500/1015\n",
      "[Epoch 8] Step 600/1015\n",
      "[Epoch 8] Step 700/1015\n",
      "[Epoch 8] Step 800/1015\n",
      "[Epoch 8] Step 900/1015\n",
      "[Epoch 8] Step 1000/1015\n",
      "Epoch 08 | BPR loss = 0.4686\n",
      "[Epoch 9] Step 1/1015\n",
      "[Epoch 9] Step 100/1015\n",
      "[Epoch 9] Step 200/1015\n",
      "[Epoch 9] Step 300/1015\n",
      "[Epoch 9] Step 400/1015\n",
      "[Epoch 9] Step 500/1015\n",
      "[Epoch 9] Step 600/1015\n",
      "[Epoch 9] Step 700/1015\n",
      "[Epoch 9] Step 800/1015\n",
      "[Epoch 9] Step 900/1015\n",
      "[Epoch 9] Step 1000/1015\n",
      "Epoch 09 | BPR loss = 0.4676\n",
      "[Epoch 10] Step 1/1015\n",
      "[Epoch 10] Step 100/1015\n",
      "[Epoch 10] Step 200/1015\n",
      "[Epoch 10] Step 300/1015\n",
      "[Epoch 10] Step 400/1015\n",
      "[Epoch 10] Step 500/1015\n",
      "[Epoch 10] Step 600/1015\n",
      "[Epoch 10] Step 700/1015\n",
      "[Epoch 10] Step 800/1015\n",
      "[Epoch 10] Step 900/1015\n",
      "[Epoch 10] Step 1000/1015\n",
      "Epoch 10 | BPR loss = 0.4671\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model.save_embeddings(num_users=num_users,num_items=num_items,device=device,save_dir=save_dir)",
   "id": "9db4fac4ac9a88f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_loader = customdataset.build_test_loader(test_df, num_items ,user_col = user_id, item_col = item_id, batch_size=1024, num_workers=num_workers)\n",
    "item_pool = list(range(num_items))\n",
    "faiss_index = faiss.read_index(f\"{save_dir}/item_index.faiss\")\n",
    "hist_tensors = build_hist_tensors(train_df, max_len=MAX_SEQ_LEN, pad_idx=PAD_IDX)"
   ],
   "id": "236d0215db2d03ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "hr_r, ndcg_r = evaluate.evaluate_random(test_loader, item_pool ,top_k=top_k)\n",
    "print(f\"Random HR@{top_k} = {hr_r:.4f}, nDCG@{top_k} = {ndcg_r:.4f}\")\n",
    "hr_p, ndcg_p = evaluate.evaluate_popular(test_loader, train_df,top_k=top_k)\n",
    "print(f\"Popular HR@{top_k} = {hr_p:.4f}, nDCG@{top_k} = {ndcg_p:.4f}\")\n",
    "hr_m, ndcg_m = evaluate.evaluate_seq_model(test_loader, model, faiss_index, device,top_k=top_k,hist_tensors=hist_tensors)\n",
    "print(f\"Model   HR@{top_k} = {hr_m:.4f}, nDCG@{top_k} = {ndcg_m:.4f}\")"
   ],
   "id": "abe3041361cf8ecd"
  },
  {
   "cell_type": "code",
   "id": "900b523c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T13:24:01.256452Z",
     "start_time": "2025-06-07T13:24:01.248933Z"
    }
   },
   "source": [
    "# def make_popularity_score_fn(train_df, item_col='item_id'):\n",
    "#     item_cnt = Counter(train_df[item_col])\n",
    "#     default_score = min(item_cnt.values()) - 1  # 给没出现过的物品一个更低分\n",
    "#     def _score_fn(users_t, items_t):\n",
    "#         return np.array([item_cnt.get(int(i), default_score) for i in items_t])\n",
    "#     return _score_fn"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "994e2156",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T13:24:01.278257Z",
     "start_time": "2025-06-07T13:24:01.271649Z"
    }
   },
   "source": [
    "# def random_score_fn(users_t, items_t):\n",
    "#     # 随机给每个 items_t 一个分数；users_t 不使用，但必须接收\n",
    "#     return np.random.rand(len(items_t))"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "4ef6ceb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T13:24:01.333302Z",
     "start_time": "2025-06-07T13:24:01.321914Z"
    }
   },
   "source": [
    "# # ======== make_score_fn，用于 evaluate_ranking ======== #\n",
    "# def make_score_fn(model, hist_tensors, device=\"cpu\"):\n",
    "#     model.eval()\n",
    "#     item_emb_table = model.embedding.weight.data  # 含 PAD\n",
    "#     @torch.no_grad()\n",
    "#     def score_fn(users_t, items_t):\n",
    "#         users_t = users_t.to(device)\n",
    "#         items_t = items_t.to(device)\n",
    "#\n",
    "#         # 拼接用户历史 batch\n",
    "#         hist_batch = torch.cat([hist_tensors[int(u)] for u in users_t], dim=0).to(device)\n",
    "#         h = model(hist_batch)                       # (B,D)\n",
    "#         item_emb = item_emb_table[items_t]          # (B,D)\n",
    "#         scores = (h * item_emb).sum(-1)             # 点积\n",
    "#         return scores.cpu().numpy()\n",
    "#     return score_fn"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "bbd4f9d7af8b5c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T13:30:19.696376Z",
     "start_time": "2025-06-07T13:24:01.379473Z"
    }
   },
   "source": [
    "\n",
    "# # ---------------- GRU（或其他模型）------------\n",
    "# score_fn_gru = make_score_fn(model,hist_tensors=build_hist_tensors(train_df),device=device)\n",
    "# hit_gru, ndcg_gru = evaluate_ranking(\n",
    "#     test_df, train_df, score_fn_gru,\n",
    "#     num_items=num_items, k=10\n",
    "# )\n",
    "# print(f\"GRU   Hit@10={hit_gru:.4f}  NDCG@10={ndcg_gru:.4f}\")\n",
    "#\n",
    "# # ---------------- baseline：Popular ----------------\n",
    "# pop_score_fn  = make_popularity_score_fn(train_df)\n",
    "# hit_pop, ndcg_pop = evaluate_ranking(\n",
    "#     test_df, train_df, pop_score_fn,\n",
    "#     num_items=num_items, k=10\n",
    "# )\n",
    "# print(f\"Popular  Hit@10={hit_pop:.4f}  NDCG@10={ndcg_pop:.4f}\")\n",
    "#\n",
    "# # ---------------- baseline：Random -----------------\n",
    "# hit_rand, ndcg_rand = evaluate_ranking(\n",
    "#     test_df, train_df, random_score_fn,\n",
    "#     num_items=num_items, k=10\n",
    "# )\n",
    "# print(f\"Random   Hit@10={hit_rand:.4f}  NDCG@10={ndcg_rand:.4f}\")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU   Hit@10=0.2895  NDCG@10=0.1460\n",
      "Popular  Hit@10=0.2528  NDCG@10=0.1262\n",
      "Random   Hit@10=0.0996  NDCG@10=0.0455\n"
     ]
    }
   ],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DP1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

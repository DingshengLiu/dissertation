{
 "cells": [
  {
   "metadata": {
    "id": "365b909de4764032",
    "outputId": "96068d4a-633b-40f0-bad7-59cb1a9cd94e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "skip:/content/tool exists\n",
      "skip:/content/MicroLens-50k_pairs.csv exists\n"
     ]
    }
   ],
   "execution_count": 115,
   "source": [
    "from google.colab import drive\n",
    "import shutil\n",
    "import os\n",
    "def copy_from_drive(src_path, dst_path):\n",
    "\n",
    "    if os.path.exists(dst_path):\n",
    "        print(f\"skip:{dst_path} exists\")\n",
    "        return\n",
    "\n",
    "    if os.path.isdir(src_path):\n",
    "        shutil.copytree(src_path, dst_path)\n",
    "    elif os.path.isfile(src_path):\n",
    "        shutil.copy(src_path, dst_path)\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "copy_from_drive('/content/drive/MyDrive/tool', '/content/tool')\n",
    "copy_from_drive('/content/drive/MyDrive/MicroLens-50k_pairs.csv','/content/MicroLens-50k_pairs.csv')"
   ],
   "id": "365b909de4764032"
  },
  {
   "cell_type": "code",
   "id": "b828e981",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T12:50:33.313152Z",
     "start_time": "2025-06-07T12:50:24.994875Z"
    },
    "id": "b828e981",
    "outputId": "4dc0e321-a8bf-499a-cf5d-cd6fddfc9463",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "\n",
    "import random\n",
    "from tool import preprocess\n",
    "from tool import customdataset\n",
    "from tool import evaluate\n",
    "!pip install faiss-cpu\n",
    "import faiss\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n"
     ]
    }
   ],
   "execution_count": 116
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "preprocess.set_seed(42)",
   "id": "78f0ef2440dcdd21"
  },
  {
   "cell_type": "code",
   "id": "e99a31348e0a553",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T12:50:33.369715Z",
     "start_time": "2025-06-07T12:50:33.324174Z"
    },
    "id": "e99a31348e0a553"
   },
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "execution_count": 117
  },
  {
   "cell_type": "code",
   "id": "76a2087d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T12:50:34.182839Z",
     "start_time": "2025-06-07T12:50:34.035344Z"
    },
    "id": "76a2087d"
   },
   "source": [
    "path = 'MicroLens-50k_pairs.csv'\n",
    "user = 'user'\n",
    "item = 'item'\n",
    "user_id = 'user_id'\n",
    "item_id = 'item_id'\n",
    "timestamp = 'timestamp'\n",
    "save_dir = './embeddings'\n",
    "top_k = 10\n",
    "num_workers = 10\n",
    "k_neg = 10\n",
    "# path = pd.read_csv('MicroLens-50k_pairs.csv')"
   ],
   "outputs": [],
   "execution_count": 118
  },
  {
   "cell_type": "code",
   "id": "943fc6c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T12:50:34.217714Z",
     "start_time": "2025-06-07T12:50:34.201507Z"
    },
    "id": "943fc6c5",
    "outputId": "390f8dd5-22ae-4d7c-cb5c-6f5ab1ef781a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "dataset_pd,num_users,num_items = preprocess.openAndSort(path,user_id=user,item_id=item,timestamp='timestamp')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dataset base information：\n",
      "- number of users：50000\n",
      "- number of items：19220\n",
      "- number of rows：359708\n"
     ]
    }
   ],
   "execution_count": 119
  },
  {
   "cell_type": "code",
   "id": "6d42c375",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T12:50:34.448028Z",
     "start_time": "2025-06-07T12:50:34.415887Z"
    },
    "id": "6d42c375",
    "outputId": "7b8a6201-2de9-46d1-a68e-119f5e30b94d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "train_df, test_df = preprocess.split(dataset_pd,user, item, timestamp)\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Test size: {len(test_df)}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train size: 309708\n",
      "Test size: 49424\n"
     ]
    }
   ],
   "execution_count": 120
  },
  {
   "cell_type": "code",
   "id": "8b8ae0c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T12:50:34.672397Z",
     "start_time": "2025-06-07T12:50:34.523061Z"
    },
    "id": "8b8ae0c7"
   },
   "source": [
    "# maintain a map from new id to old id, new id for constructing matrix\n",
    "user2id = {u: i for i, u in enumerate(dataset_pd[user].unique())}\n",
    "item2id = {i: j for j, i in enumerate(dataset_pd[item].unique())}\n",
    "\n",
    "# apply to train_df and test_df\n",
    "train_df[user_id] = train_df[user].map(user2id)\n",
    "train_df[item_id] = train_df[item].map(item2id)\n",
    "test_df[user_id] = test_df[user].map(user2id)\n",
    "test_df[item_id] = test_df[item].map(item2id)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 121
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T12:50:36.206199Z",
     "start_time": "2025-06-07T12:50:36.185327Z"
    },
    "id": "6e56658ac26b07e2"
   },
   "cell_type": "code",
   "source": [
    "# ---------- 超参数 ----------\n",
    "MAX_SEQ_LEN   = 20          # 输入序列长度\n",
    "EMBEDDING_DIM = 64          # item / user embedding 维度\n",
    "HIDDEN_SIZE   = 64          # GRU 隐藏维度（可与 EMBEDDING_DIM 相同）\n",
    "NEG_SAMPLE    = 5           # 训练时每个正样本采负样本数\n",
    "BATCH_SIZE    = 256\n",
    "EPOCHS        = 10\n",
    "LR            = 1e-3\n",
    "SEED          = 42\n",
    "L2_NORM =False\n",
    "# ----------------------------\n",
    "\n",
    "# ---------- 随机种子 ----------\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "# ----------------------------\n",
    "\n",
    "# ---------- 常量 ----------\n",
    "PAD_IDX  = num_items              # padding 专用 id（不与真实 item 冲突）\n",
    "N_ITEMS  = num_items + 1          # Embedding 行数（含 PAD）\n",
    "# ----------------------------\n",
    "\n"
   ],
   "id": "6e56658ac26b07e2",
   "outputs": [],
   "execution_count": 122
  },
  {
   "cell_type": "code",
   "id": "a7aba75c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T12:50:36.269499Z",
     "start_time": "2025-06-07T12:50:36.261103Z"
    },
    "id": "a7aba75c"
   },
   "source": [
    "# ======== 模型 ======== #\n",
    "class GRU4RecBPR(nn.Module):\n",
    "    def __init__(self, n_items=N_ITEMS,\n",
    "                 embedding_dim=EMBEDDING_DIM, hidden_size=HIDDEN_SIZE,\n",
    "                 num_layers=1, pad_idx=PAD_IDX):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_items, embedding_dim, padding_idx=pad_idx)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True)\n",
    "        self.proj = (nn.Linear(hidden_size, embedding_dim, bias=False)\n",
    "                     if hidden_size != embedding_dim else nn.Identity())\n",
    "\n",
    "    def forward(self, seq):\n",
    "        \"\"\"\n",
    "        seq : (B, T) → 返回用户向量 h_hat : (B, D)\n",
    "        \"\"\"\n",
    "        emb  = self.embedding(seq)            # (B, T, D)\n",
    "        out, _ = self.gru(emb)                # (B, T, H)\n",
    "        h     = out[:, -1, :]                 # (B, H) 最后一步\n",
    "        if L2_NORM:\n",
    "            h = F.normalize(h, p=2, dim=1)\n",
    "        return self.proj(h)                   # (B, D)\n",
    "\n",
    "\n",
    "    def get_items_embedding(self,item_ids,l2_norm=False):\n",
    "        i_vec = self.embedding(item_ids)          # (B, emb_dim)\n",
    "        if l2_norm:\n",
    "            i_vec = F.normalize(i_vec, p=2, dim=1)\n",
    "        return i_vec\n",
    "\n",
    "    def save_embeddings(self, num_users, num_items, device, save_dir='./embeddings',l2_norm = L2_NORM):\n",
    "        import os\n",
    "        import faiss\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        self.eval()\n",
    "        self.to(device)\n",
    "\n",
    "        item_ids = torch.arange(num_items, dtype=torch.long, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            item_embeds = self.get_items_embedding(item_ids, l2_norm=l2_norm)\n",
    "\n",
    "        item_embeds = item_embeds.cpu().numpy().astype(np.float32)\n",
    "\n",
    "        # 保存向量\n",
    "        np.save(f\"{save_dir}/item_embeddings.npy\", item_embeds)\n",
    "\n",
    "        # 构建 FAISS index（使用内积）\n",
    "        dim = item_embeds.shape[1]\n",
    "        index = faiss.IndexFlatIP(dim)\n",
    "        index.add(item_embeds)\n",
    "\n",
    "        faiss.write_index(index, f\"{save_dir}/item_index.faiss\")\n",
    "        print(\"Saved user/item embeddings and FAISS index.\")\n"
   ],
   "outputs": [],
   "execution_count": 123
  },
  {
   "cell_type": "code",
   "id": "07e268c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T12:50:36.330578Z",
     "start_time": "2025-06-07T12:50:36.319538Z"
    },
    "id": "07e268c2"
   },
   "source": [
    "# ======== 训练流程 ======== #\n",
    "def train_model(model, train_df, epochs,lr , batch_size, test_df=None, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "    train_loader  = customdataset.build_seq_loader(train_df, batch_size=batch_size,\n",
    "                         shuffle=True, num_workers=10,pad_idx=PAD_IDX,max_len=MAX_SEQ_LEN,user_id=user,item_id=item_id)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        dt_start = datetime.now()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            hist, pos = batch\n",
    "            hist, pos = hist.to(device), pos.to(device)\n",
    "\n",
    "            # 1. 前向传播（返回预测向量）\n",
    "            predict = model(hist)                      # (B, D)\n",
    "            i_vec = model.get_items_embedding(pos,l2_norm=False)\n",
    "\n",
    "            # 2. 得分矩阵：每个 user 对所有正 item 的打分\n",
    "            logits = torch.matmul(predict, i_vec.T)  # shape: (B, B)\n",
    "\n",
    "            # 3. 构造标签：每个 user 的正确 item 在对角线（即位置 i）\n",
    "            labels = torch.arange(logits.size(0), device=device)  # [0, 1, ..., B-1]\n",
    "\n",
    "            # 4. Cross Entropy Loss\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "            # 5. 反向传播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # 日志\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        dt_end = datetime.now()\n",
    "        dt = dt_end - dt_start\n",
    "\n",
    "        print(f\"[Epoch {epoch:02d}/{epochs}] avg InBatch Softmax Loss = {avg_loss:.4f}, time = {dt.total_seconds():.2f}s\")\n",
    "\n",
    "    return\n"
   ],
   "outputs": [],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T12:50:36.371982Z",
     "start_time": "2025-06-07T12:50:36.365209Z"
    },
    "id": "9bc49db2aee62849"
   },
   "cell_type": "code",
   "source": [
    "def build_hist_matrix(df,\n",
    "                      num_users,\n",
    "                      max_len=MAX_SEQ_LEN,\n",
    "                      pad_idx=PAD_IDX,\n",
    "                      user_col=user_id,\n",
    "                      item_col=item_id):\n",
    "    \"\"\"\n",
    "    返回形状为 (num_users, max_len) 的 LongTensor。\n",
    "    第 i 行是用户 i 的历史序列，左侧 PAD，右对齐。\n",
    "    不存在历史的用户整行都是 pad_idx。\n",
    "    \"\"\"\n",
    "    # 先全部填 PAD\n",
    "    hist = torch.full((num_users, max_len), pad_idx, dtype=torch.long)\n",
    "\n",
    "    # groupby 遍历每个用户已有交互\n",
    "    for uid, items in df.groupby(user_col)[item_col]:\n",
    "        seq = items.to_numpy()[-max_len:]             # 取最近 max_len 条\n",
    "        hist[uid, -len(seq):] = torch.as_tensor(seq, dtype=torch.long)\n",
    "\n",
    "    return hist    # (U, T)\n"
   ],
   "id": "9bc49db2aee62849",
   "outputs": [],
   "execution_count": 125
  },
  {
   "cell_type": "code",
   "id": "cd29a40c68841c9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T13:24:01.057232Z",
     "start_time": "2025-06-07T12:50:36.508711Z"
    },
    "id": "cd29a40c68841c9c",
    "outputId": "3e81fa41-b447-43c0-857c-784c1a746a5d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "    # ------------------ 训练 ------------------\n",
    "model = GRU4RecBPR(n_items=N_ITEMS,\n",
    "                 embedding_dim=EMBEDDING_DIM, hidden_size=HIDDEN_SIZE,\n",
    "                 num_layers=1, pad_idx=PAD_IDX)\n",
    "model = model.to(device)\n",
    "train_model(model=model,epochs=45, train_df=train_df,batch_size=1024,lr=LR,test_df=test_df,device=device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Epoch 01/45] avg InBatch Softmax Loss = 7.4819, time = 1.55s\n",
      "[Epoch 02/45] avg InBatch Softmax Loss = 6.9264, time = 1.43s\n",
      "[Epoch 03/45] avg InBatch Softmax Loss = 6.8970, time = 1.41s\n",
      "[Epoch 04/45] avg InBatch Softmax Loss = 6.8366, time = 1.37s\n",
      "[Epoch 05/45] avg InBatch Softmax Loss = 6.7824, time = 1.38s\n",
      "[Epoch 06/45] avg InBatch Softmax Loss = 6.7431, time = 1.38s\n",
      "[Epoch 07/45] avg InBatch Softmax Loss = 6.7083, time = 1.42s\n",
      "[Epoch 08/45] avg InBatch Softmax Loss = 6.6738, time = 1.37s\n",
      "[Epoch 09/45] avg InBatch Softmax Loss = 6.6372, time = 1.34s\n",
      "[Epoch 10/45] avg InBatch Softmax Loss = 6.5935, time = 1.39s\n",
      "[Epoch 11/45] avg InBatch Softmax Loss = 6.5444, time = 1.42s\n",
      "[Epoch 12/45] avg InBatch Softmax Loss = 6.4904, time = 1.43s\n",
      "[Epoch 13/45] avg InBatch Softmax Loss = 6.4329, time = 1.37s\n",
      "[Epoch 14/45] avg InBatch Softmax Loss = 6.3722, time = 1.37s\n",
      "[Epoch 15/45] avg InBatch Softmax Loss = 6.3088, time = 1.37s\n",
      "[Epoch 16/45] avg InBatch Softmax Loss = 6.2418, time = 1.41s\n",
      "[Epoch 17/45] avg InBatch Softmax Loss = 6.1721, time = 1.45s\n",
      "[Epoch 18/45] avg InBatch Softmax Loss = 6.0990, time = 1.37s\n",
      "[Epoch 19/45] avg InBatch Softmax Loss = 6.0223, time = 1.38s\n",
      "[Epoch 20/45] avg InBatch Softmax Loss = 5.9439, time = 1.41s\n",
      "[Epoch 21/45] avg InBatch Softmax Loss = 5.8640, time = 1.41s\n",
      "[Epoch 22/45] avg InBatch Softmax Loss = 5.7840, time = 1.36s\n",
      "[Epoch 23/45] avg InBatch Softmax Loss = 5.7040, time = 1.37s\n",
      "[Epoch 24/45] avg InBatch Softmax Loss = 5.6246, time = 1.36s\n",
      "[Epoch 25/45] avg InBatch Softmax Loss = 5.5474, time = 1.38s\n",
      "[Epoch 26/45] avg InBatch Softmax Loss = 5.4717, time = 1.36s\n",
      "[Epoch 27/45] avg InBatch Softmax Loss = 5.3989, time = 1.37s\n",
      "[Epoch 28/45] avg InBatch Softmax Loss = 5.3281, time = 1.38s\n",
      "[Epoch 29/45] avg InBatch Softmax Loss = 5.2588, time = 1.48s\n",
      "[Epoch 30/45] avg InBatch Softmax Loss = 5.1936, time = 1.42s\n",
      "[Epoch 31/45] avg InBatch Softmax Loss = 5.1295, time = 1.37s\n",
      "[Epoch 32/45] avg InBatch Softmax Loss = 5.0676, time = 1.37s\n",
      "[Epoch 33/45] avg InBatch Softmax Loss = 5.0083, time = 1.38s\n",
      "[Epoch 34/45] avg InBatch Softmax Loss = 4.9503, time = 1.39s\n",
      "[Epoch 35/45] avg InBatch Softmax Loss = 4.8954, time = 1.37s\n",
      "[Epoch 36/45] avg InBatch Softmax Loss = 4.8417, time = 1.37s\n",
      "[Epoch 37/45] avg InBatch Softmax Loss = 4.7897, time = 1.37s\n",
      "[Epoch 38/45] avg InBatch Softmax Loss = 4.7393, time = 1.44s\n",
      "[Epoch 39/45] avg InBatch Softmax Loss = 4.6905, time = 1.40s\n",
      "[Epoch 40/45] avg InBatch Softmax Loss = 4.6453, time = 1.33s\n",
      "[Epoch 41/45] avg InBatch Softmax Loss = 4.5987, time = 1.35s\n",
      "[Epoch 42/45] avg InBatch Softmax Loss = 4.5553, time = 1.35s\n",
      "[Epoch 43/45] avg InBatch Softmax Loss = 4.5120, time = 1.46s\n",
      "[Epoch 44/45] avg InBatch Softmax Loss = 4.4711, time = 1.58s\n",
      "[Epoch 45/45] avg InBatch Softmax Loss = 4.4313, time = 1.55s\n"
     ]
    }
   ],
   "execution_count": 126
  },
  {
   "metadata": {
    "id": "9db4fac4ac9a88f1",
    "outputId": "b7cdaf77-e8c0-401f-94eb-5c79a4f4e62f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved user/item embeddings and FAISS index.\n"
     ]
    }
   ],
   "execution_count": 127,
   "source": [
    "model.save_embeddings(num_users=num_users,num_items=num_items,device=device,save_dir=save_dir)"
   ],
   "id": "9db4fac4ac9a88f1"
  },
  {
   "metadata": {
    "id": "236d0215db2d03ac"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 128,
   "source": [
    "test_loader = customdataset.build_test_loader(test_df, num_items ,user_col = user_id, item_col = item_id, batch_size=1024, num_workers=num_workers)\n",
    "item_pool = list(range(num_items))\n",
    "faiss_index = faiss.read_index(f\"{save_dir}/item_index.faiss\")\n",
    "hist_tensors = build_hist_matrix(train_df, max_len=MAX_SEQ_LEN, pad_idx=PAD_IDX,num_users=num_users).to(device)"
   ],
   "id": "236d0215db2d03ac"
  },
  {
   "metadata": {
    "id": "abe3041361cf8ecd",
    "outputId": "4c84ed09-2e2e-48b8-a244-100141df870d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Random HR@10 = 0.0006, nDCG@10 = 0.0003\n",
      "Popular HR@10 = 0.0029, nDCG@10 = 0.0014\n",
      "Model   HR@10 = 0.0403, nDCG@10 = 0.0225\n"
     ]
    }
   ],
   "execution_count": 129,
   "source": [
    "hr_r, ndcg_r = evaluate.evaluate_random(test_loader, item_pool ,top_k=top_k)\n",
    "print(f\"Random HR@{top_k} = {hr_r:.4f}, nDCG@{top_k} = {ndcg_r:.4f}\")\n",
    "hr_p, ndcg_p = evaluate.evaluate_popular(test_loader, train_df,top_k=top_k)\n",
    "print(f\"Popular HR@{top_k} = {hr_p:.4f}, nDCG@{top_k} = {ndcg_p:.4f}\")\n",
    "hr_m, ndcg_m = evaluate.evaluate_seq_model(test_loader, model, faiss_index, device,top_k=top_k,hist_tensors=hist_tensors)\n",
    "print(f\"Model   HR@{top_k} = {hr_m:.4f}, nDCG@{top_k} = {ndcg_m:.4f}\")"
   ],
   "id": "abe3041361cf8ecd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DP1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

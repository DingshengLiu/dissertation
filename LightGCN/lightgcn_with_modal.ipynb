{
  "cells": [
    {
      "metadata": {
        "id": "a8786635ccc5e581",
        "outputId": "e1c6b150-544c-4c98-927d-d4f8a485a832",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "skip:/content/tool exists\n",
            "skip:/content/MicroLens-50k_pairs.csv exists\n",
            "skip:/content/cover_emb128.lmdb exists\n",
            "skip:/content/title_emb1024.lmdb exists\n"
          ]
        }
      ],
      "execution_count": 125,
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "def copy_from_drive(src_path, dst_path):\n",
        "\n",
        "    if os.path.exists(dst_path):\n",
        "        print(f\"skip:{dst_path} exists\")\n",
        "        return\n",
        "\n",
        "    if os.path.isdir(src_path):\n",
        "        shutil.copytree(src_path, dst_path)\n",
        "    elif os.path.isfile(src_path):\n",
        "        shutil.copy(src_path, dst_path)\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "copy_from_drive('/content/drive/MyDrive/tool', '/content/tool')\n",
        "copy_from_drive('/content/drive/MyDrive/MicroLens-50k_pairs.csv','/content/MicroLens-50k_pairs.csv')\n",
        "copy_from_drive('/content/drive/MyDrive/cover_emb128.lmdb','/content/cover_emb128.lmdb')\n",
        "copy_from_drive('/content/drive/MyDrive/title_emb1024.lmdb','/content/title_emb1024.lmdb')"
      ],
      "id": "a8786635ccc5e581"
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "id": "b828e981",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T20:26:13.343878Z",
          "start_time": "2025-04-30T20:26:13.338767Z"
        },
        "id": "b828e981",
        "outputId": "576842eb-90c1-4ebd-d4c5-d65446552841",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.12/dist-packages (1.7.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu\n",
        "!pip install lmdb\n",
        "from tool import preprocess\n",
        "from tool import customdataset\n",
        "from tool import evaluate\n",
        "import faiss\n",
        "from datetime import datetime\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import csv\n",
        "from matplotlib import pyplot as plt\n"
      ]
    },
    {
      "metadata": {
        "id": "bad1b340170c222f",
        "outputId": "b2504a20-9045-4c80-9efa-a79f5ae66612",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function tool.preprocess.set_seed.<locals>.seed_worker(worker_id)>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>tool.preprocess.set_seed.&lt;locals&gt;.seed_worker</b><br/>def seed_worker(worker_id)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/tool/preprocess.py</a>&lt;no docstring&gt;</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 18);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ],
      "execution_count": 127,
      "source": [
        "preprocess.set_seed(42)"
      ],
      "id": "bad1b340170c222f"
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "id": "e99a31348e0a553",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T20:26:13.391974Z",
          "start_time": "2025-04-30T20:26:13.387554Z"
        },
        "id": "e99a31348e0a553"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "id": "76a2087d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T20:26:13.572661Z",
          "start_time": "2025-04-30T20:26:13.416069Z"
        },
        "id": "76a2087d"
      },
      "outputs": [],
      "source": [
        "# dataset_pd = pd.read_csv('D:\\\\VideoRecSystem\\\\MicroLens\\\\DataSet\\\\MicroLens-50k_pairs.csv')\n",
        "path = 'MicroLens-50k_pairs.csv'\n",
        "cover_lmdb_path = 'cover_emb128.lmdb'\n",
        "title_lmdb_path = 'title_emb1024.lmdb'\n",
        "record_path = './records'\n",
        "user = 'user'\n",
        "item = 'item'\n",
        "user_id = 'user_id'\n",
        "item_id = 'item_id'\n",
        "timestamp = 'timestamp'\n",
        "save_dir = './embeddings'\n",
        "PROJECT_NAME = 'LightGCN'\n",
        "# ---------- 超参数 ----------\n",
        "N_LAYERS = 2\n",
        "EMBEDDING_DIM = 64\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 1024\n",
        "LR = 1E-3\n",
        "MODAL = {'COVER':{\"LMDB_DIM\":128, \"HIDDEN_SIZE\":[EMBEDDING_DIM],\"DROPOUT\":0.2} , 'TITLE':{\"LMDB_DIM\":1024,\"HIDDEN_SIZE\":[EMBEDDING_DIM],\"DROPOUT\":0.2}\n",
        "         ,'COVER-TITLE': {\"LMDB_DIM\":128+1024, \"HIDDEN_SIZE\":[EMBEDDING_DIM],\"DROPOUT\":0.2}}\n",
        "FUSION_MODE = \"late\"\n",
        "CURRENT_MODAL = \"COVER-TITLE\"\n",
        "MODAL_CONFIG = MODAL[CURRENT_MODAL]\n",
        "MODAL_HIDDEN_SIZE = MODAL_CONFIG.get('HIDDEN_SIZE')\n",
        "LMDB_DIM = MODAL_CONFIG.get('LMDB_DIM')\n",
        "MODAL_DROPOUT = MODAL_CONFIG.get('DROPOUT')\n",
        "DROPOUT = 0.2\n",
        "L2_NORM = False\n",
        "TOP_K= 10\n",
        "PATIENCE = 5\n",
        "MONITOR = 'hr'\n",
        "NUM_WORKERS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "id": "943fc6c5",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T20:26:13.615265Z",
          "start_time": "2025-04-30T20:26:13.605752Z"
        },
        "id": "943fc6c5",
        "outputId": "92050a02-7f2a-4164-b060-068cb0353264",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset base information：\n",
            "- number of users：50000\n",
            "- number of items：19220\n",
            "- number of rows：359708\n"
          ]
        }
      ],
      "source": [
        "dataset_pd,num_users,num_items = preprocess.openAndSort(path,user_id=user,item_id=item,timestamp='timestamp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "id": "69160caa",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T20:26:13.713174Z",
          "start_time": "2025-04-30T20:26:13.702986Z"
        },
        "id": "69160caa",
        "outputId": "d4832f78-ef36-44e4-f12a-ae95d8e29a83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 259708\n",
            "Val_df size: 49156\n",
            "Test_df size: 47774\n",
            "Train_all_df size: 308864\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_df, val_df, test_df, train_all_df = preprocess.split_with_val(dataset_pd,user, item, timestamp)\n",
        "print(f\"Train size: {len(train_df)}\")\n",
        "print(f\"Val_df size: {len(val_df)}\")\n",
        "print(f\"Test_df size: {len(test_df)}\")\n",
        "print(f\"Train_all_df size: {len(train_all_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "id": "6d42c375",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T20:26:13.848465Z",
          "start_time": "2025-04-30T20:26:13.789646Z"
        },
        "id": "6d42c375"
      },
      "outputs": [],
      "source": [
        "# maintain a map from new id to old id, new id for constructing matrix\n",
        "user2id = {u: i for i, u in enumerate(dataset_pd[user].unique())}\n",
        "item2id = {i: j for j, i in enumerate(dataset_pd[item].unique())}\n",
        "\n",
        "# apply to train_df and test_df\n",
        "train_df[user_id] = train_df[user].map(user2id)\n",
        "train_df[item_id] = train_df[item].map(item2id)\n",
        "val_df[user_id] = val_df[user].map(user2id)\n",
        "val_df[item_id] = val_df[item].map(item2id)\n",
        "test_df[user_id] = test_df[user].map(user2id)\n",
        "test_df[item_id] = test_df[item].map(item2id)\n",
        "train_all_df[user_id] = train_all_df[user].map(user2id)\n",
        "train_all_df[item_id] = train_all_df[item].map(item2id)\n",
        "\n",
        "# 1. 构建 item_id 到 item 的映射（来自 train_df）\n",
        "item_id_to_item = {v: k for k, v in item2id.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "id": "a28cf3d3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T20:26:15.725506Z",
          "start_time": "2025-04-30T20:26:15.717Z"
        },
        "id": "a28cf3d3"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_adj_matrix(df, num_users, num_items ,user_id, item_id):\n",
        "    rows = df[user_id].values\n",
        "    cols = df[item_id].values\n",
        "    data = np.ones(len(df))\n",
        "    # set interaction of user-item as 1, other as 0\n",
        "    R = sp.coo_matrix((data, (rows, cols)), shape=(num_users, num_items))\n",
        "\n",
        "    # construct symetric matrix A\n",
        "    upper = sp.hstack([sp.csr_matrix((num_users, num_users)), R])\n",
        "    lower = sp.hstack([R.T, sp.csr_matrix((num_items, num_items))])\n",
        "    A = sp.vstack([upper, lower])\n",
        "\n",
        "    # normalization A → Ĥ = D^{-1/2} A D^{-1/2}\n",
        "    rowsum = np.array(A.sum(1)).flatten()\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5)\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    D_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    A_norm = D_inv_sqrt @ A @ D_inv_sqrt\n",
        "\n",
        "    # transform to torch.sparse\n",
        "    A_norm = A_norm.tocoo()\n",
        "    indices = torch.LongTensor([A_norm.row, A_norm.col])\n",
        "    values = torch.FloatTensor(A_norm.data)\n",
        "    return torch.sparse_coo_tensor(indices, values, A_norm.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "id": "a7aba75c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T20:26:16.197535Z",
          "start_time": "2025-04-30T20:26:16.187729Z"
        },
        "id": "a7aba75c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class LightGCN(nn.Module):\n",
        "    def __init__(self, num_users, num_items , embedding_dim, n_layers, adjacency,\n",
        "                 lmdb_dim=LMDB_DIM, modal_hidden_size=MODAL_HIDDEN_SIZE, modal_dropout=MODAL_DROPOUT,\n",
        "                 fusion_mode=FUSION_MODE):  # 'base' | 'early' | 'late'\n",
        "        super(LightGCN, self).__init__()\n",
        "        assert fusion_mode in {'base', 'early', 'late'}\n",
        "        self.fusion_mode   = fusion_mode\n",
        "        self.user_emb      = None\n",
        "        self.item_emb      = None\n",
        "        self.num_users     = num_users\n",
        "        self.num_items     = num_items\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.n_layers      = n_layers\n",
        "        self.adjacency     = adjacency  # torch.sparse_coo_tensor\n",
        "\n",
        "        # ----- ID embeddings -----\n",
        "        self.embedding_user = nn.Embedding(num_users, embedding_dim)\n",
        "        self.embedding_item = nn.Embedding(num_items, embedding_dim)\n",
        "        nn.init.xavier_uniform_(self.embedding_user.weight)\n",
        "        nn.init.xavier_uniform_(self.embedding_item.weight)\n",
        "\n",
        "        # modal 向量（冻结）\n",
        "        modal_emb_tensor = None\n",
        "        if FUSION_MODE!='base':\n",
        "            if CURRENT_MODAL=='COVER':\n",
        "                modal_emb_tensor = preprocess.load_tensor_from_lmdb(\n",
        "                    cover_lmdb_path, num_items, item_id_to_item, lmdb_dim\n",
        "                )\n",
        "            if CURRENT_MODAL=='TITLE':\n",
        "                modal_emb_tensor = preprocess.load_tensor_from_lmdb(\n",
        "                    title_lmdb_path, num_items, item_id_to_item, lmdb_dim\n",
        "                )\n",
        "            if CURRENT_MODAL=='COVER-TITLE':\n",
        "                cover_emb_tensor = preprocess.load_tensor_from_lmdb(\n",
        "                    cover_lmdb_path, num_items, item_id_to_item, 128\n",
        "                )\n",
        "                title_emb_tensor = preprocess.load_tensor_from_lmdb(\n",
        "                    title_lmdb_path, num_items, item_id_to_item, 1024\n",
        "                )\n",
        "                modal_emb_tensor = torch.cat([cover_emb_tensor, title_emb_tensor], dim=-1)\n",
        "\n",
        "            self.register_buffer('frozen_extra_emb', modal_emb_tensor)\n",
        "\n",
        "\n",
        "        # ----- 前融合投影：[item_id_emb; modal] -> emb_dim -----\n",
        "        self.mlp_item_modal = self.build_mlp(embedding_dim + lmdb_dim, modal_hidden_size, modal_dropout)\n",
        "\n",
        "        # ----- 后融合用 α（全局标量）-----\n",
        "        # sigmoid(0)=0.5；如需更稳可改为 1.0 使初期更偏向 ID\n",
        "        self.alpha_param = nn.Parameter(torch.tensor(0.0)) if fusion_mode == 'late' else None\n",
        "\n",
        "    def build_mlp(self, input_dim, hidden_sizes, dropout):\n",
        "        layers = []\n",
        "        for h in hidden_sizes:\n",
        "            layers += [nn.Linear(input_dim, h), nn.BatchNorm1d(h), nn.Tanh(), nn.Dropout(dropout)]\n",
        "            input_dim = h\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    # ============ LightGCN 传播（给定初始 user/item 表示） ============\n",
        "    def _propagate(self, user_init, item_init):\n",
        "        \"\"\"\n",
        "        user_init: (U, D), item_init: (I, D)\n",
        "        返回经 n_layers LightGCN 平均聚合后的 (user_emb, item_emb)\n",
        "        \"\"\"\n",
        "        all_embeddings = torch.cat([user_init, item_init], dim=0)   # (U+I, D)\n",
        "        embs = [all_embeddings]\n",
        "        for _ in range(self.n_layers):\n",
        "            all_embeddings = torch.sparse.mm(self.adjacency, all_embeddings)\n",
        "            embs.append(all_embeddings)\n",
        "        final = torch.stack(embs, dim=1).mean(dim=1)                # (U+I, D)\n",
        "        user_embedding, item_embedding = torch.split(final, [self.num_users, self.num_items])\n",
        "        return user_embedding, item_embedding\n",
        "\n",
        "    def _item_init_id_only(self):\n",
        "        return self.embedding_item.weight                             # (I, D)\n",
        "\n",
        "    def _item_init_early(self):\n",
        "        # [id_emb; modal] -> emb_dim\n",
        "        modal = self.frozen_extra_emb.to(self.embedding_item.weight.device)  # (I, C)\n",
        "        i_cat = torch.cat([self.embedding_item.weight, modal], dim=-1)       # (I, D+C)\n",
        "        i_emb = self.mlp_item_modal(i_cat)                                   # (I, D)\n",
        "        return i_emb\n",
        "\n",
        "    # ============ 前向：生成/缓存用户与物品图表示 ============\n",
        "    def forward(self):\n",
        "        device = self.embedding_item.weight.device\n",
        "        user_init = self.embedding_user.weight                              # (U, D)\n",
        "\n",
        "        if self.fusion_mode == 'base':\n",
        "            # 纯 ID：一次传播\n",
        "            item_init = self._item_init_id_only()\n",
        "            user_embedding, item_embedding = self._propagate(user_init, item_init)\n",
        "\n",
        "        elif self.fusion_mode == 'early':\n",
        "            # 前融合：先做模态映射，再作为图的初始 item 表示\n",
        "            item_init = self._item_init_early()\n",
        "            user_embedding, item_embedding = self._propagate(user_init, item_init)\n",
        "\n",
        "        else:  # 'late'\n",
        "            # 后融合：两条路径分别图传播，最后在图传播结果处做 α 加权（仅对 item）\n",
        "            item_init_id = self._item_init_id_only()\n",
        "            item_init_mm = self._item_init_early()\n",
        "\n",
        "            # 两次传播（共享同一 user_init）\n",
        "            user_id_emb,  item_id_emb  = self._propagate(user_init, item_init_id)\n",
        "            user_mm_emb,  item_mm_emb  = self._propagate(user_init, item_init_mm)\n",
        "\n",
        "            alpha = torch.sigmoid(self.alpha_param).to(device)  # 标量\n",
        "            item_embedding = alpha * item_id_emb + (1.0 - alpha) * item_mm_emb\n",
        "            # 用户侧：为最小改动与稳定性，采用 ID 路径的 user 表示（也可两路再平均/融合）\n",
        "            user_embedding = user_id_emb\n",
        "\n",
        "        # 缓存（评测/导出用）\n",
        "        self.user_emb = user_embedding.detach()\n",
        "        self.item_emb = item_embedding.detach()\n",
        "        return user_embedding, item_embedding\n",
        "\n",
        "    # ============ Getter（评测/召回） ============\n",
        "    def get_users_embedding(self, user_ids, l2_norm=False):\n",
        "        u_vec = self.user_emb[user_ids]\n",
        "        if l2_norm:\n",
        "            u_vec = F.normalize(u_vec, p=2, dim=1)\n",
        "        return u_vec\n",
        "\n",
        "    def get_items_embedding(self, item_ids, l2_norm=False):\n",
        "        i_vec = self.item_emb[item_ids]\n",
        "        if l2_norm:\n",
        "            i_vec = F.normalize(i_vec, p=2, dim=1)\n",
        "        return i_vec\n",
        "\n",
        "    # ============ 导出 ============\n",
        "    def save_embeddings(self, num_users, num_items, device, save_dir='./embeddings', l2_norm=L2_NORM):\n",
        "        import os, faiss\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        self.eval().to(device)\n",
        "\n",
        "        user_ids = torch.arange(num_users, dtype=torch.long, device=device)\n",
        "        item_ids = torch.arange(num_items, dtype=torch.long, device=device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            user_embeds = self.get_users_embedding(user_ids, l2_norm=l2_norm)\n",
        "            item_embeds = self.get_items_embedding(item_ids, l2_norm=l2_norm)\n",
        "\n",
        "        user_embeds = user_embeds.cpu().numpy().astype(np.float32)\n",
        "        item_embeds = item_embeds.cpu().numpy().astype(np.float32)\n",
        "\n",
        "        np.save(f\"{save_dir}/user_embeddings.npy\", user_embeds)\n",
        "        np.save(f\"{save_dir}/item_embeddings.npy\", item_embeds)\n",
        "\n",
        "        dim = item_embeds.shape[1]\n",
        "        index = faiss.IndexFlatIP(dim)\n",
        "        index.add(item_embeds)\n",
        "        faiss.write_index(index, f\"{save_dir}/item_index.faiss\")\n",
        "        print(\"Saved user/item embeddings and FAISS index.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "07e268c2",
      "metadata": {
        "id": "07e268c2",
        "ExecuteTime": {
          "end_time": "2025-08-06T15:52:33.410720Z",
          "start_time": "2025-08-06T15:52:01.496763Z"
        }
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def train_model(model,\n",
        "                train_df,\n",
        "                val_df,\n",
        "                top_k,\n",
        "                epochs,\n",
        "                batch_size,\n",
        "                lr,\n",
        "                val_mode,\n",
        "                device=None,\n",
        "                patience=PATIENCE,         # 早停容忍\n",
        "                monitor=MONITOR,       # \"hr\" 或 \"ndcg\"\n",
        "                record_path = record_path\n",
        "                ):\n",
        "    \"\"\"\n",
        "    训练 LightGCN (或其它 BPR 模型) 的通用函数\n",
        "    ------------------------------------------------\n",
        "    • train_df      : pandas DataFrame，含 user_id / item_id\n",
        "    • num_items     : 物品总数\n",
        "    • device        : torch.device；默认为 'cuda' (若可用) 否则 'cpu'\n",
        "    • max_grad_norm : 梯度裁剪阈值；避免梯度爆炸，可选\n",
        "    \"\"\"\n",
        "    # -------- 设备 ----------\n",
        "    if device is None:\n",
        "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    model = model.to(device)\n",
        "    if hasattr(model, \"adjacency\"):               # adjacency 可能是稀疏张量\n",
        "        model.adjacency = model.adjacency.to(device)\n",
        "\n",
        "    # -------- 优化器 ----------\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    train_loader = customdataset.build_train_loader_inbatch(train_df, batch_size=batch_size,user_col=user_id, item_col=item_id)\n",
        "    val_loader = customdataset.build_test_loader(val_df, num_items ,user_col = user_id, item_col = item_id, batch_size=1024, num_workers=NUM_WORKERS)\n",
        "\n",
        "    # 训练过程记录\n",
        "    hist = {\n",
        "        \"epoch\": [],\n",
        "        \"loss\": [],\n",
        "        f\"hr@{top_k}\": [],\n",
        "        f\"ndcg@{top_k}\": [],\n",
        "        \"alpha\": [],\n",
        "        \"beta\": [],\n",
        "    }\n",
        "\n",
        "    # 早停配置\n",
        "    best_metric = -math.inf\n",
        "    best_epoch  = -1\n",
        "    patience_cnt = 0\n",
        "    monitor_key = f\"{monitor}@{top_k}\"\n",
        "\n",
        "    print(f\"[EarlyStopping] monitor={monitor_key} , patience={patience}\")\n",
        "\n",
        "    # -------- 训练循环 ----------\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        dt_start = datetime.now()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            user_ids, pos_item_ids = batch\n",
        "            user_ids = user_ids.to(device).long()\n",
        "            pos_item_ids = pos_item_ids.to(device).long()\n",
        "\n",
        "\n",
        "            # 1. 前向传播（返回 user / item 向量）\n",
        "            user_emb, item_emb = model()\n",
        "            u_vec = user_emb[user_ids]\n",
        "            i_vec = item_emb[pos_item_ids]\n",
        "\n",
        "            # 2. 得分矩阵：每个 user 对所有正 item 的打分\n",
        "            logits = torch.matmul(u_vec, i_vec.T)  # shape: (B, B)\n",
        "\n",
        "            # 3. 构造标签：每个 user 的正确 item 在对角线（即位置 i）\n",
        "            labels = torch.arange(logits.size(0), device=device)  # [0, 1, ..., B-1]\n",
        "\n",
        "            # 4. Cross Entropy Loss\n",
        "            loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "            # 5. 反向传播\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        # 日志\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        dt_end = datetime.now()\n",
        "        dt = (dt_end - dt_start).total_seconds()\n",
        "        model.save_embeddings(num_users=num_users,num_items=num_items,device=device,save_dir=save_dir)\n",
        "        faiss_index = faiss.read_index(f\"{save_dir}/item_index.faiss\")\n",
        "        model.eval()\n",
        "        hr_m, ndcg_m = evaluate.evaluate_model(val_loader, model, faiss_index, device, top_k=top_k)\n",
        "\n",
        "        # gates（若存在）\n",
        "        alpha_val = float(torch.sigmoid(model.alpha_param).item()) if hasattr(model, \"alpha_param\") and model.alpha_param is not None else float(\"nan\")\n",
        "        beta_val  = float(torch.sigmoid(model.beta_param).item())  if hasattr(model, \"beta_param\") and model.beta_param is not None else float(\"nan\")\n",
        "\n",
        "        print(f\"[Epoch {epoch:02d}/{epochs}] avg InBatch Softmax Loss = {avg_loss:.4f}, \"\n",
        "              f\"HR@{top_k} = {hr_m:.4f}, NDCG@{top_k} = {ndcg_m:.4f}, \"\n",
        "              f\"alpha={alpha_val if not math.isnan(alpha_val) else 'NA'}, \"\n",
        "              f\"beta={beta_val if not math.isnan(beta_val) else 'NA'}, \"\n",
        "              f\"time = {dt:.2f}s\")\n",
        "\n",
        "        # —— 记录历史 ——\n",
        "        hist[\"epoch\"].append(epoch)\n",
        "        hist[\"loss\"].append(avg_loss)\n",
        "        hist[f\"hr@{top_k}\"].append(hr_m)\n",
        "        hist[f\"ndcg@{top_k}\"].append(ndcg_m)\n",
        "        hist[\"alpha\"].append(alpha_val)\n",
        "        hist[\"beta\"].append(beta_val)\n",
        "\n",
        "        # —— 早停判断（最大化 monitor 指标）——\n",
        "        if val_mode:\n",
        "          current_metric = hr_m if monitor == \"hr\" else ndcg_m\n",
        "          if current_metric > best_metric:\n",
        "              best_metric = current_metric\n",
        "              best_epoch = epoch\n",
        "              patience_cnt = 0\n",
        "              print(f\"current best {monitor_key}={best_metric:.4f} @ epoch {epoch}.\")\n",
        "                          # ==== 保存最佳 hr / ndcg / epoch ====\n",
        "              best_info_path = os.path.join(record_path,\n",
        "                                            \"validation mode\" if val_mode else \"train mode\",\n",
        "                                            \"best_result.txt\")\n",
        "              os.makedirs(os.path.dirname(best_info_path), exist_ok=True)\n",
        "              with open(best_info_path, \"w\") as f:\n",
        "                  f.write(f\"epoch: {epoch}\\n\")\n",
        "                  f.write(f\"HR@{top_k}: {hr_m:.4f}\\n\")\n",
        "                  f.write(f\"NDCG@{top_k}: {ndcg_m:.4f}\\n\")\n",
        "              print(f\"Best result info saved to {best_info_path}\")\n",
        "          else:\n",
        "              patience_cnt += 1\n",
        "              if patience_cnt >= patience:\n",
        "                  print(\"Early stopping triggered.\")\n",
        "                  break\n",
        "\n",
        "\n",
        "    # —— 导出历史 CSV ——\n",
        "    csv_path = os.path.join(record_path,\"validation mode\" if val_mode else \"train mode\",\"training_history.csv\")\n",
        "    os.makedirs(os.path.dirname(csv_path), exist_ok=True)  # 确保目录存在\n",
        "    with open(csv_path, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"epoch\", \"loss\", f\"hr@{top_k}\", f\"ndcg@{top_k}\", \"alpha\", \"beta\", \"time_sec\"])\n",
        "        for i in range(len(hist[\"epoch\"])):\n",
        "            writer.writerow([\n",
        "                hist[\"epoch\"][i],\n",
        "                hist[\"loss\"][i],\n",
        "                hist[f\"hr@{top_k}\"][i],\n",
        "                hist[f\"ndcg@{top_k}\"][i],\n",
        "                hist[\"alpha\"][i],\n",
        "                hist[\"beta\"][i],\n",
        "            ])\n",
        "    # —— 绘图：Loss ——\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(hist[\"epoch\"], hist[\"loss\"])\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"In-Batch CE Loss\"); plt.title(\"Training Loss\")\n",
        "    plt.grid(True, linestyle=\"--\", alpha=0.4); plt.tight_layout()\n",
        "    plt.xticks(range(1, max(hist[\"epoch\"]) + 1, 1))\n",
        "    fig1_path = os.path.join(record_path,\"validation mode\" if val_mode else \"train mode\",\"curve_loss.png\")\n",
        "    os.makedirs(os.path.dirname(fig1_path), exist_ok=True)  # 确保目录存在\n",
        "\n",
        "    plt.savefig(fig1_path, dpi=150); plt.close()\n",
        "    print(f\"Saved {fig1_path}\")\n",
        "\n",
        "    # —— 绘图：HR/NDCG ——\n",
        "    plt.figure()\n",
        "    plt.plot(hist[\"epoch\"], hist[f\"hr@{top_k}\"], label=f\"HR@{top_k}\")\n",
        "    plt.plot(hist[\"epoch\"], hist[f\"ndcg@{top_k}\"], label=f\"NDCG@{top_k}\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Metric\"); plt.title(\"Validation Metrics\")\n",
        "    plt.legend(); plt.grid(True, linestyle=\"--\", alpha=0.4); plt.tight_layout()\n",
        "    plt.xticks(range(1, max(hist[\"epoch\"]) + 1, 1))\n",
        "    fig2_path = os.path.join(record_path,\"validation mode\" if val_mode else \"train mode\",\"curve_metrics.png\")\n",
        "    os.makedirs(os.path.dirname(fig2_path), exist_ok=True)  # 确保目录存在\n",
        "    plt.savefig(fig2_path, dpi=150); plt.close()\n",
        "    print(f\"Saved {fig2_path}\")\n",
        "\n",
        "    # —— 绘图：alpha/beta（如存在） ——\n",
        "    if not all(math.isnan(v) for v in hist[\"alpha\"]) or not all(math.isnan(v) for v in hist[\"beta\"]):\n",
        "        plt.figure()\n",
        "        if not all(math.isnan(v) for v in hist[\"alpha\"]):\n",
        "            plt.plot(hist[\"epoch\"], hist[\"alpha\"], label=\"alpha (item late)\")\n",
        "        if not all(math.isnan(v) for v in hist[\"beta\"]):\n",
        "            plt.plot(hist[\"epoch\"], hist[\"beta\"],  label=\"beta (user late)\")\n",
        "        plt.xlabel(\"Epoch\"); plt.ylabel(\"Gate (sigmoid)\"); plt.title(\"Late Fusion Gates\")\n",
        "        plt.ylim(0, 1); plt.legend(); plt.grid(True, linestyle=\"--\", alpha=0.4); plt.tight_layout()\n",
        "        plt.xticks(range(1, max(hist[\"epoch\"]) + 1, 1))\n",
        "        fig3_path = os.path.join(record_path,\"validation mode\" if val_mode else \"train mode\",\"curve_alpha_beta.png\")\n",
        "        os.makedirs(os.path.dirname(fig3_path), exist_ok=True)  # 确保目录存在\n",
        "        plt.savefig(fig3_path, dpi=150); plt.close()\n",
        "        print(f\"Saved {fig3_path}\")\n",
        "\n",
        "    print(f\"Best {monitor_key}={best_metric:.4f} at epoch {best_epoch}\")\n",
        "    return best_epoch"
      ],
      "outputs": [],
      "execution_count": 135
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "id": "cd29a40c68841c9c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T20:34:51.542117Z",
          "start_time": "2025-04-30T20:26:16.696795Z"
        },
        "id": "cd29a40c68841c9c",
        "outputId": "3d255d8d-1eb9-4bd3-a2a0-39afa94ac7b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2216191345.py:15: RuntimeWarning: divide by zero encountered in power\n",
            "  d_inv_sqrt = np.power(rowsum, -0.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EarlyStopping] monitor=hr@10 , patience=5\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 01/50] avg InBatch Softmax Loss = 6.5407, HR@10 = 0.0167, NDCG@10 = 0.0081, alpha=0.4142559766769409, beta=NA, time = 4.79s\n",
            "current best hr@10=0.0167 @ epoch 1.\n",
            "Best result info saved to ./records/validation mode/best_result.txt\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 02/50] avg InBatch Softmax Loss = 6.0039, HR@10 = 0.0233, NDCG@10 = 0.0114, alpha=0.3485683798789978, beta=NA, time = 4.81s\n",
            "current best hr@10=0.0233 @ epoch 2.\n",
            "Best result info saved to ./records/validation mode/best_result.txt\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 03/50] avg InBatch Softmax Loss = 5.6822, HR@10 = 0.0236, NDCG@10 = 0.0109, alpha=0.2952204942703247, beta=NA, time = 4.82s\n",
            "current best hr@10=0.0236 @ epoch 3.\n",
            "Best result info saved to ./records/validation mode/best_result.txt\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 04/50] avg InBatch Softmax Loss = 5.4347, HR@10 = 0.0280, NDCG@10 = 0.0128, alpha=0.25084981322288513, beta=NA, time = 4.83s\n",
            "current best hr@10=0.0280 @ epoch 4.\n",
            "Best result info saved to ./records/validation mode/best_result.txt\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 05/50] avg InBatch Softmax Loss = 5.2242, HR@10 = 0.0321, NDCG@10 = 0.0148, alpha=0.2133825570344925, beta=NA, time = 5.00s\n",
            "current best hr@10=0.0321 @ epoch 5.\n",
            "Best result info saved to ./records/validation mode/best_result.txt\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 06/50] avg InBatch Softmax Loss = 5.0364, HR@10 = 0.0318, NDCG@10 = 0.0149, alpha=0.18189653754234314, beta=NA, time = 4.78s\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 07/50] avg InBatch Softmax Loss = 4.8659, HR@10 = 0.0346, NDCG@10 = 0.0160, alpha=0.1554616391658783, beta=NA, time = 4.76s\n",
            "current best hr@10=0.0346 @ epoch 7.\n",
            "Best result info saved to ./records/validation mode/best_result.txt\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 08/50] avg InBatch Softmax Loss = 4.7052, HR@10 = 0.0335, NDCG@10 = 0.0154, alpha=0.13310907781124115, beta=NA, time = 4.78s\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 09/50] avg InBatch Softmax Loss = 4.5509, HR@10 = 0.0358, NDCG@10 = 0.0162, alpha=0.11420733481645584, beta=NA, time = 4.77s\n",
            "current best hr@10=0.0358 @ epoch 9.\n",
            "Best result info saved to ./records/validation mode/best_result.txt\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 10/50] avg InBatch Softmax Loss = 4.4023, HR@10 = 0.0346, NDCG@10 = 0.0155, alpha=0.09818843752145767, beta=NA, time = 4.76s\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 11/50] avg InBatch Softmax Loss = 4.2576, HR@10 = 0.0359, NDCG@10 = 0.0161, alpha=0.08460912853479385, beta=NA, time = 4.77s\n",
            "current best hr@10=0.0359 @ epoch 11.\n",
            "Best result info saved to ./records/validation mode/best_result.txt\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 12/50] avg InBatch Softmax Loss = 4.1151, HR@10 = 0.0370, NDCG@10 = 0.0164, alpha=0.07306260615587234, beta=NA, time = 4.78s\n",
            "current best hr@10=0.0370 @ epoch 12.\n",
            "Best result info saved to ./records/validation mode/best_result.txt\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 13/50] avg InBatch Softmax Loss = 3.9757, HR@10 = 0.0371, NDCG@10 = 0.0161, alpha=0.06326580792665482, beta=NA, time = 4.79s\n",
            "current best hr@10=0.0371 @ epoch 13.\n",
            "Best result info saved to ./records/validation mode/best_result.txt\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 14/50] avg InBatch Softmax Loss = 3.8411, HR@10 = 0.0374, NDCG@10 = 0.0162, alpha=0.05495738238096237, beta=NA, time = 4.77s\n",
            "current best hr@10=0.0374 @ epoch 14.\n",
            "Best result info saved to ./records/validation mode/best_result.txt\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 15/50] avg InBatch Softmax Loss = 3.7076, HR@10 = 0.0377, NDCG@10 = 0.0162, alpha=0.04786919057369232, beta=NA, time = 4.79s\n",
            "current best hr@10=0.0377 @ epoch 15.\n",
            "Best result info saved to ./records/validation mode/best_result.txt\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 16/50] avg InBatch Softmax Loss = 3.5782, HR@10 = 0.0374, NDCG@10 = 0.0157, alpha=0.04183592647314072, beta=NA, time = 4.97s\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 17/50] avg InBatch Softmax Loss = 3.4514, HR@10 = 0.0366, NDCG@10 = 0.0153, alpha=0.036682236939668655, beta=NA, time = 4.77s\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 18/50] avg InBatch Softmax Loss = 3.3277, HR@10 = 0.0389, NDCG@10 = 0.0161, alpha=0.0322645828127861, beta=NA, time = 4.78s\n",
            "current best hr@10=0.0389 @ epoch 18.\n",
            "Best result info saved to ./records/validation mode/best_result.txt\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 19/50] avg InBatch Softmax Loss = 3.2097, HR@10 = 0.0365, NDCG@10 = 0.0150, alpha=0.028478529304265976, beta=NA, time = 4.77s\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 20/50] avg InBatch Softmax Loss = 3.0938, HR@10 = 0.0368, NDCG@10 = 0.0147, alpha=0.025219568982720375, beta=NA, time = 4.77s\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 21/50] avg InBatch Softmax Loss = 2.9831, HR@10 = 0.0370, NDCG@10 = 0.0148, alpha=0.02240793965756893, beta=NA, time = 4.78s\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 22/50] avg InBatch Softmax Loss = 2.8764, HR@10 = 0.0371, NDCG@10 = 0.0146, alpha=0.019976481795310974, beta=NA, time = 4.76s\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 23/50] avg InBatch Softmax Loss = 2.7727, HR@10 = 0.0358, NDCG@10 = 0.0141, alpha=0.017857041209936142, beta=NA, time = 4.77s\n",
            "Early stopping triggered.\n",
            "Saved ./records/validation mode/curve_loss.png\n",
            "Saved ./records/validation mode/curve_metrics.png\n",
            "Saved ./records/validation mode/curve_alpha_beta.png\n",
            "Best hr@10=0.0389 at epoch 18\n",
            "[EarlyStopping] monitor=hr@10 , patience=5\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 01/18] avg InBatch Softmax Loss = 6.4949, HR@10 = 0.0145, NDCG@10 = 0.0070, alpha=0.4005919098854065, beta=NA, time = 6.07s\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 02/18] avg InBatch Softmax Loss = 5.9514, HR@10 = 0.0213, NDCG@10 = 0.0099, alpha=0.32838672399520874, beta=NA, time = 6.26s\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 03/18] avg InBatch Softmax Loss = 5.6389, HR@10 = 0.0216, NDCG@10 = 0.0100, alpha=0.27177178859710693, beta=NA, time = 6.08s\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 04/18] avg InBatch Softmax Loss = 5.4033, HR@10 = 0.0279, NDCG@10 = 0.0126, alpha=0.2258322685956955, beta=NA, time = 6.07s\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 05/18] avg InBatch Softmax Loss = 5.2047, HR@10 = 0.0264, NDCG@10 = 0.0122, alpha=0.18815161287784576, beta=NA, time = 6.06s\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 06/18] avg InBatch Softmax Loss = 5.0333, HR@10 = 0.0290, NDCG@10 = 0.0131, alpha=0.15741781890392303, beta=NA, time = 6.07s\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 07/18] avg InBatch Softmax Loss = 4.8791, HR@10 = 0.0301, NDCG@10 = 0.0136, alpha=0.1322191208600998, beta=NA, time = 6.06s\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 08/18] avg InBatch Softmax Loss = 4.7345, HR@10 = 0.0333, NDCG@10 = 0.0150, alpha=0.11130551993846893, beta=NA, time = 6.04s\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 09/18] avg InBatch Softmax Loss = 4.5975, HR@10 = 0.0331, NDCG@10 = 0.0146, alpha=0.09396292269229889, beta=NA, time = 6.06s\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 10/18] avg InBatch Softmax Loss = 4.4654, HR@10 = 0.0322, NDCG@10 = 0.0142, alpha=0.07947251945734024, beta=NA, time = 6.06s\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 11/18] avg InBatch Softmax Loss = 4.3383, HR@10 = 0.0309, NDCG@10 = 0.0135, alpha=0.06738278269767761, beta=NA, time = 6.05s\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 12/18] avg InBatch Softmax Loss = 4.2151, HR@10 = 0.0301, NDCG@10 = 0.0134, alpha=0.05729919299483299, beta=NA, time = 6.08s\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 13/18] avg InBatch Softmax Loss = 4.0937, HR@10 = 0.0323, NDCG@10 = 0.0140, alpha=0.04883808270096779, beta=NA, time = 6.26s\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 14/18] avg InBatch Softmax Loss = 3.9763, HR@10 = 0.0329, NDCG@10 = 0.0145, alpha=0.04175708815455437, beta=NA, time = 6.26s\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 15/18] avg InBatch Softmax Loss = 3.8600, HR@10 = 0.0328, NDCG@10 = 0.0141, alpha=0.03579656779766083, beta=NA, time = 6.08s\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 16/18] avg InBatch Softmax Loss = 3.7467, HR@10 = 0.0326, NDCG@10 = 0.0139, alpha=0.030778247863054276, beta=NA, time = 6.06s\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 17/18] avg InBatch Softmax Loss = 3.6352, HR@10 = 0.0329, NDCG@10 = 0.0140, alpha=0.026530329138040543, beta=NA, time = 6.09s\n",
            "Saved user/item embeddings and FAISS index.\n",
            "[Epoch 18/18] avg InBatch Softmax Loss = 3.5272, HR@10 = 0.0320, NDCG@10 = 0.0132, alpha=0.02294335514307022, beta=NA, time = 6.05s\n",
            "Saved ./records/train mode/curve_loss.png\n",
            "Saved ./records/train mode/curve_metrics.png\n",
            "Saved ./records/train mode/curve_alpha_beta.png\n",
            "Best hr@10=-inf at epoch -1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ],
      "source": [
        "adj_torch = build_adj_matrix(train_df, num_users, num_items ,user_id, item_id)\n",
        "model = LightGCN(num_users,num_items,embedding_dim=EMBEDDING_DIM,n_layers=N_LAYERS,adjacency=adj_torch)\n",
        "model.to(device)\n",
        "selected_epoch = train_model(model=model,epochs=EPOCHS, train_df=train_df,val_df=val_df,batch_size=BATCH_SIZE,top_k=TOP_K,lr=LR,val_mode=True)\n",
        "adj_torch = build_adj_matrix(train_all_df, num_users, num_items ,user_id, item_id)\n",
        "model = LightGCN(num_users,num_items,embedding_dim=EMBEDDING_DIM,n_layers=N_LAYERS,adjacency=adj_torch)\n",
        "model.to(device)\n",
        "train_model(model=model,epochs=selected_epoch, train_df=train_all_df,val_df=test_df,batch_size=BATCH_SIZE,top_k=TOP_K,lr=LR,val_mode=False)"
      ]
    },
    {
      "metadata": {
        "id": "cc858c515448b5b8",
        "outputId": "e82495ae-3de6-4ba3-ec89-22d4ebc9e81d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved user/item embeddings and FAISS index.\n"
          ]
        }
      ],
      "execution_count": 137,
      "source": [
        "model.save_embeddings(num_users=num_users,num_items=num_items,device=device,save_dir=save_dir)"
      ],
      "id": "cc858c515448b5b8"
    },
    {
      "metadata": {
        "id": "2c5b952e28007084"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 138,
      "source": [
        "test_loader = customdataset.build_test_loader(test_df, num_items ,user_col = user_id, item_col = item_id, batch_size=1024, num_workers=NUM_WORKERS)\n",
        "item_pool = list(range(num_items))\n",
        "faiss_index = faiss.read_index(f\"{save_dir}/item_index.faiss\")"
      ],
      "id": "2c5b952e28007084"
    },
    {
      "metadata": {
        "id": "ae68d6ba3c864c65",
        "outputId": "15232a70-afde-4fdf-efa8-e466797e7511",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random HR@10 = 0.0004, NDCG@10 = 0.0002\n",
            "Popular HR@10 = 0.0030, NDCG@10 = 0.0014\n",
            "Model   HR@10 = 0.0320, NDCG@10 = 0.0132\n"
          ]
        }
      ],
      "execution_count": 139,
      "source": [
        "hr_r, ndcg_r = evaluate.evaluate_random(test_loader, item_pool ,top_k=TOP_K)\n",
        "print(f\"Random HR@{TOP_K} = {hr_r:.4f}, NDCG@{TOP_K} = {ndcg_r:.4f}\")\n",
        "hr_p, ndcg_p = evaluate.evaluate_popular(test_loader, train_all_df,top_k=TOP_K)\n",
        "print(f\"Popular HR@{TOP_K} = {hr_p:.4f}, NDCG@{TOP_K} = {ndcg_p:.4f}\")\n",
        "hr_m, ndcg_m = evaluate.evaluate_model(test_loader, model, faiss_index, device,top_k=TOP_K)\n",
        "print(f\"Model   HR@{TOP_K} = {hr_m:.4f}, NDCG@{TOP_K} = {ndcg_m:.4f}\")\n"
      ],
      "id": "ae68d6ba3c864c65"
    },
    {
      "metadata": {
        "id": "a7dc14aa90839c4a",
        "outputId": "4901ed23-591b-40e2-ef21-6196bf862223",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "execution_count": 140,
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 挂载 Google Drive\n",
        "drive.mount('/content/drive')\n",
        "# 目标路径\n",
        "target_dir = None\n",
        "if(FUSION_MODE==\"base\"):\n",
        "    target_dir = f\"/content/drive/MyDrive/REC/{PROJECT_NAME}/{FUSION_MODE}/\"\n",
        "else:\n",
        "    target_dir = f\"/content/drive/MyDrive/REC/{PROJECT_NAME}/{FUSION_MODE}/{CURRENT_MODAL}\"\n",
        "# 创建目标路径（包含上层目录）\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "# 复制 records 到目标路径\n",
        "!cp -r /content/records \"{target_dir}\"\n",
        "!rm -rf /content/records"
      ],
      "id": "a7dc14aa90839c4a"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "name": "python3",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
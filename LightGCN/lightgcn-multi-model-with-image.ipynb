{
 "cells": [
  {
   "cell_type": "code",
   "id": "b828e981",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T15:39:20.322366Z",
     "start_time": "2025-06-30T15:39:00.122880Z"
    }
   },
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from networkx.readwrite.json_graph import adjacency"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "e99a31348e0a553",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T15:39:20.476089Z",
     "start_time": "2025-06-30T15:39:20.338Z"
    }
   },
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "76a2087d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T15:39:21.784198Z",
     "start_time": "2025-06-30T15:39:21.609035Z"
    }
   },
   "source": [
    "dataset_pd = pd.read_csv('D:\\\\VideoRecSystem\\\\MicroLens\\\\DataSet\\\\MicroLens-50k_pairs.csv')\n",
    "# dataset_pd = pd.read_csv('MicroLens-50k_pairs.csv')"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "943fc6c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T15:39:21.831903Z",
     "start_time": "2025-06-30T15:39:21.811813Z"
    }
   },
   "source": [
    "dataset_pd.head(10)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    user   item      timestamp\n",
       "0  36121   9580  1583378629552\n",
       "1  26572   9580  1583436719018\n",
       "2  37550   9580  1584412681021\n",
       "3  14601   9580  1584848802432\n",
       "4  15061   9580  1585388171106\n",
       "5   6364   9580  1585390736041\n",
       "6   3542   9580  1585404918503\n",
       "7  21038   9580  1590144594477\n",
       "8  12538  14631  1634867362929\n",
       "9  47592  14631  1634872254913"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36121</td>\n",
       "      <td>9580</td>\n",
       "      <td>1583378629552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26572</td>\n",
       "      <td>9580</td>\n",
       "      <td>1583436719018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37550</td>\n",
       "      <td>9580</td>\n",
       "      <td>1584412681021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14601</td>\n",
       "      <td>9580</td>\n",
       "      <td>1584848802432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15061</td>\n",
       "      <td>9580</td>\n",
       "      <td>1585388171106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6364</td>\n",
       "      <td>9580</td>\n",
       "      <td>1585390736041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3542</td>\n",
       "      <td>9580</td>\n",
       "      <td>1585404918503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>21038</td>\n",
       "      <td>9580</td>\n",
       "      <td>1590144594477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12538</td>\n",
       "      <td>14631</td>\n",
       "      <td>1634867362929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>47592</td>\n",
       "      <td>14631</td>\n",
       "      <td>1634872254913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "69160caa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T15:39:21.886384Z",
     "start_time": "2025-06-30T15:39:21.872398Z"
    }
   },
   "source": [
    "dataset_pd.count"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.count of          user   item      timestamp\n",
       "0       36121   9580  1583378629552\n",
       "1       26572   9580  1583436719018\n",
       "2       37550   9580  1584412681021\n",
       "3       14601   9580  1584848802432\n",
       "4       15061   9580  1585388171106\n",
       "...       ...    ...            ...\n",
       "359703  48702   1363  1662984066647\n",
       "359704  27203   7291  1662984082974\n",
       "359705  29261  19649  1662984103874\n",
       "359706  28341  19188  1662984123833\n",
       "359707  38967   7254  1662984132429\n",
       "\n",
       "[359708 rows x 3 columns]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "6d42c375",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T15:39:22.005873Z",
     "start_time": "2025-06-30T15:39:21.939793Z"
    }
   },
   "source": [
    "user_counts = dataset_pd['user'].value_counts()\n",
    "item_counts = dataset_pd['item'].value_counts()\n",
    "valid_users = user_counts[user_counts > 3].index\n",
    "valid_items = item_counts[item_counts > 3].index\n",
    "filtered_df = dataset_pd[dataset_pd['user'].isin(valid_users) & dataset_pd['item'].isin(valid_items)]\n",
    "filtered_df.count"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.count of          user   item      timestamp\n",
       "0       36121   9580  1583378629552\n",
       "1       26572   9580  1583436719018\n",
       "2       37550   9580  1584412681021\n",
       "3       14601   9580  1584848802432\n",
       "4       15061   9580  1585388171106\n",
       "...       ...    ...            ...\n",
       "359703  48702   1363  1662984066647\n",
       "359704  27203   7291  1662984082974\n",
       "359705  29261  19649  1662984103874\n",
       "359706  28341  19188  1662984123833\n",
       "359707  38967   7254  1662984132429\n",
       "\n",
       "[352649 rows x 3 columns]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "46decf50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T15:39:22.071576Z",
     "start_time": "2025-06-30T15:39:22.064412Z"
    }
   },
   "source": [
    "dataset_pd = filtered_df.count"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "8b8ae0c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T15:39:22.333178Z",
     "start_time": "2025-06-30T15:39:22.151440Z"
    }
   },
   "source": [
    "# order by user,timestamp \n",
    "filtered_df = filtered_df.sort_values(by=[\"user\", \"timestamp\"])\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "2d2d4de256fd88a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T15:39:22.357690Z",
     "start_time": "2025-06-30T15:39:22.349147Z"
    }
   },
   "source": [
    "def split(df, user_col='user', item_col='item', time_col='timestamp'):\n",
    "\n",
    "    df = df.sort_values(by=[user_col, time_col])  # 按用户时间排序\n",
    "\n",
    "    # 获取每个用户的最后一条记录作为 test\n",
    "    test_df = df.groupby(user_col).tail(1)\n",
    "    train_df = df.drop(index=test_df.index)\n",
    "\n",
    "    # 过滤 test 中那些 user/item 不在 train 中的\n",
    "    train_users = set(train_df[user_col])\n",
    "    train_items = set(train_df[item_col])\n",
    "\n",
    "    test_df = test_df[\n",
    "        test_df[user_col].isin(train_users) &\n",
    "        test_df[item_col].isin(train_items)\n",
    "    ]\n",
    "\n",
    "    return train_df.reset_index(drop=True), test_df.reset_index(drop=True)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "3db1146761e45165",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T15:39:22.900979Z",
     "start_time": "2025-06-30T15:39:22.387654Z"
    }
   },
   "source": [
    "\n",
    "train_df, test_df = split(filtered_df,user_col='user', item_col='item', time_col='timestamp')\n",
    "\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Test size: {len(test_df)}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 302650\n",
      "Test size: 49681\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "ce079817",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T15:39:24.104923Z",
     "start_time": "2025-06-30T15:39:22.942991Z"
    }
   },
   "source": [
    "# main tain a map from new id to old id, new id for constructing matrix\n",
    "user2id = {u: i for i, u in enumerate(filtered_df['user'].unique())}\n",
    "item2id = {i: j for j, i in enumerate(filtered_df['item'].unique())}\n",
    "\n",
    "# apply to train_df and test_df\n",
    "train_df['user_id'] = train_df['user'].map(user2id)\n",
    "train_df['item_id'] = train_df['item'].map(item2id)\n",
    "test_df['user_id'] = test_df['user'].map(user2id)\n",
    "test_df['item_id'] = test_df['item'].map(item2id)\n",
    "\n",
    "num_users = len(user2id)\n",
    "num_items = len(item2id)\n"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "a28cf3d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T15:39:24.122039Z",
     "start_time": "2025-06-30T15:39:24.113460Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "def build_adj_matrix(df, num_users, num_items):\n",
    "    rows = df['user_id'].values\n",
    "    cols = df['item_id'].values\n",
    "    data = np.ones(len(df))\n",
    "    # set interaction of user-item as 1, other as 0\n",
    "    R = sp.coo_matrix((data, (rows, cols)), shape=(num_users, num_items))\n",
    "\n",
    "    # construct symetric matrix A\n",
    "    upper = sp.hstack([sp.csr_matrix((num_users, num_users)), R])\n",
    "    lower = sp.hstack([R.T, sp.csr_matrix((num_items, num_items))])\n",
    "    A = sp.vstack([upper, lower])\n",
    "\n",
    "    # normalization A → Ĥ = D^{-1/2} A D^{-1/2}\n",
    "    rowsum = np.array(A.sum(1)).flatten()\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5)\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    D_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    A_norm = D_inv_sqrt @ A @ D_inv_sqrt\n",
    "\n",
    "    # transform to torch.sparse\n",
    "    A_norm = A_norm.tocoo()\n",
    "    indices = torch.LongTensor([A_norm.row, A_norm.col])\n",
    "    values = torch.FloatTensor(A_norm.data)\n",
    "    return torch.sparse_coo_tensor(indices, values, A_norm.shape)\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "3d9cee55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T15:39:24.618359Z",
     "start_time": "2025-06-30T15:39:24.161398Z"
    }
   },
   "source": [
    "adj_torch = build_adj_matrix(train_df, num_users, num_items)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Liuds\\AppData\\Local\\Temp\\ipykernel_19920\\4111138054.py:19: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv_sqrt = np.power(rowsum, -0.5)\n",
      "C:\\Users\\Liuds\\AppData\\Local\\Temp\\ipykernel_19920\\4111138054.py:26: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  indices = torch.LongTensor([A_norm.row, A_norm.col])\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "79f25f01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T15:43:50.071169Z",
     "start_time": "2025-06-30T15:43:50.044072Z"
    }
   },
   "source": [
    "\n",
    "# ====== Load 128‑d cover embeddings from LMDB and align to item2id ======\n",
    "import lmdb, numpy as np, torch\n",
    "\n",
    "lmdb_path = r\"D:\\VideoRecSystem\\MicroLens\\cover_emb128.lmdb\"   # ← 修改为实际路径（可 Pathlib）\n",
    "\n",
    "def build_image_matrix(lmdb_path: str, item2id: dict, dtype=np.float32):\n",
    "    env = lmdb.open(lmdb_path, subdir=False, readonly=True, lock=False, readahead=False)\n",
    "    img_dim = 128\n",
    "    num_items = len(item2id)\n",
    "    mat = np.zeros((num_items, img_dim), dtype=dtype)\n",
    "    with env.begin() as txn:\n",
    "        for orig_id, new_id in item2id.items():\n",
    "            key = str(orig_id).encode()\n",
    "            val = txn.get(key)\n",
    "            if val is None:\n",
    "                continue          # 若缺失则保持 0\n",
    "            vec = np.frombuffer(val, dtype=dtype)\n",
    "            if vec.size != img_dim:\n",
    "                raise ValueError(f\"Item {orig_id} vec dim {vec.size} != 128\")\n",
    "            mat[new_id] = vec\n",
    "    env.close()\n",
    "    return torch.from_numpy(mat)\n",
    "\n",
    "image_tensor = build_image_matrix(lmdb_path, item2id)          # shape = (num_items,128)\n"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "a7aba75c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T15:43:50.105296Z",
     "start_time": "2025-06-30T15:43:50.091236Z"
    }
   },
   "source": [
    "\n",
    "# ====== LightGCN WITH Image Embedding ======\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LightGCN(nn.Module):\n",
    "    \"\"\"LightGCN that concatenates id embedding with pre‑computed 128‑d image vectors.\"\"\"\n",
    "    def __init__(self, num_users, num_items, embedding_dim, n_layers, adjacency,\n",
    "                 image_tensor=None, freeze_image=True):\n",
    "        super().__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.id_dim     = embedding_dim\n",
    "        self.img_dim    = 0 if image_tensor is None else image_tensor.size(1)\n",
    "        self.embedding_dim = self.id_dim + self.img_dim\n",
    "        self.n_layers   = n_layers\n",
    "        self.adjacency  = adjacency  # torch.sparse\n",
    "\n",
    "        # --- user embedding (id part + optional img‑pad) ---\n",
    "        self.embedding_user = nn.Embedding(num_users, self.embedding_dim)\n",
    "        nn.init.xavier_uniform_(self.embedding_user.weight)\n",
    "\n",
    "        # --- item id embedding ---\n",
    "        self.embedding_item_id = nn.Embedding(num_items, self.id_dim)\n",
    "        nn.init.xavier_uniform_(self.embedding_item_id.weight)\n",
    "\n",
    "        # --- item image embedding ---\n",
    "        if image_tensor is not None:\n",
    "            self.embedding_item_img = nn.Embedding.from_pretrained(\n",
    "                image_tensor, freeze=freeze_image\n",
    "            )\n",
    "        else:\n",
    "            self.embedding_item_img = None\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    def propagate(self, all_emb, adj_mat):\n",
    "        emb_list = [all_emb]\n",
    "        for _ in range(self.n_layers):\n",
    "            all_emb = torch.sparse.mm(adj_mat, all_emb)\n",
    "            emb_list.append(all_emb)\n",
    "        return torch.stack(emb_list, dim=1).mean(dim=1)\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    def forward(self):\n",
    "        # ----- build initial concatenated vector -----\n",
    "        if self.embedding_item_img is not None:\n",
    "            item_emb = torch.cat([self.embedding_item_id.weight,\n",
    "                                  self.embedding_item_img.weight], dim=1)\n",
    "        else:\n",
    "            zeros = torch.zeros(self.num_items, self.img_dim, device=self.embedding_item_id.weight.device)\n",
    "            item_emb = torch.cat([self.embedding_item_id.weight, zeros], dim=1)\n",
    "\n",
    "        all_emb_0 = torch.cat([self.embedding_user.weight, item_emb], dim=0)\n",
    "        all_emb_L = self.propagate(all_emb_0, self.adjacency)\n",
    "\n",
    "        user_emb_final, item_emb_final = torch.split(\n",
    "            all_emb_L, [self.num_users, self.num_items]\n",
    "        )\n",
    "        return user_emb_final, item_emb_final\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    def get_embedding(self):\n",
    "        return self.forward()\n",
    "\n",
    "    def predict(self, user_indices, item_indices):\n",
    "        u_e, i_e = self.forward()\n",
    "        return (u_e[user_indices] * i_e[item_indices]).sum(dim=1)\n"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "f21704ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T15:43:50.175296Z",
     "start_time": "2025-06-30T15:43:50.168474Z"
    }
   },
   "source": [
    "def bpr_loss(user_emb, pos_item_emb, neg_item_emb):\n",
    "    pos_scores = (user_emb * pos_item_emb).sum(dim=1)\n",
    "    neg_scores = (user_emb * neg_item_emb).sum(dim=1)\n",
    "    loss = -torch.log(torch.sigmoid(pos_scores - neg_scores)).mean()\n",
    "    return loss"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "503d8f70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T15:43:50.191092Z",
     "start_time": "2025-06-30T15:43:50.183775Z"
    }
   },
   "source": [
    "# Negative sampling function\n",
    "def sample_batch(train_df, num_items, batch_size):\n",
    "    users = train_df['user_id'].unique()\n",
    "    batch_users = np.random.choice(users, size=batch_size)\n",
    "    user_pos_dict = train_df.groupby('user_id')['item_id'].apply(set).to_dict()\n",
    "\n",
    "    user_ids, pos_ids, neg_ids = [], [], []\n",
    "    for u in batch_users:\n",
    "        pos_items = list(user_pos_dict[u])\n",
    "        pos = random.choice(pos_items)\n",
    "        while True:\n",
    "            neg = random.randint(0, num_items - 1)\n",
    "            if neg not in user_pos_dict[u]:\n",
    "                break\n",
    "        user_ids.append(u)\n",
    "        pos_ids.append(pos)\n",
    "        neg_ids.append(neg)\n",
    "\n",
    "    return torch.LongTensor(user_ids), torch.LongTensor(pos_ids), torch.LongTensor(neg_ids)"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "07e268c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T15:43:50.235227Z",
     "start_time": "2025-06-30T15:43:50.223663Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# --------- 采样函数保持不变 (示意) ------------\n",
    "# def sample_batch(train_df, num_items, batch_size): ...\n",
    "\n",
    "# --------- BPR 损失保持不变 -------------------\n",
    "# def bpr_loss(user_vec, pos_vec, neg_vec): ...\n",
    "\n",
    "def train_model(model,\n",
    "                train_df,\n",
    "                num_items,\n",
    "                epochs=10,\n",
    "                batch_size=1024,\n",
    "                lr=1e-3,\n",
    "                print_every=1,\n",
    "                max_grad_norm=None,          # =None 时不裁剪\n",
    "                device=None):\n",
    "    \"\"\"\n",
    "    训练 LightGCN (或其它 BPR 模型) 的通用函数\n",
    "    ------------------------------------------------\n",
    "    • train_df      : pandas DataFrame，含 user_id / item_id\n",
    "    • num_items     : 物品总数\n",
    "    • device        : torch.device；默认为 'cuda' (若可用) 否则 'cpu'\n",
    "    • max_grad_norm : 梯度裁剪阈值；避免梯度爆炸，可选\n",
    "    \"\"\"\n",
    "    # -------- 设备 ----------\n",
    "    if device is None:\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model = model.to(device)\n",
    "    if hasattr(model, \"adjacency\"):               # adjacency 可能是稀疏张量\n",
    "        model.adjacency = model.adjacency.to(device)\n",
    "\n",
    "    # -------- 优化器 ----------\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # -------- 训练循环 ----------\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "\n",
    "        # ==== 采样一个 mini-batch ====\n",
    "        user_ids, pos_ids, neg_ids = sample_batch(train_df, num_items, batch_size)\n",
    "\n",
    "        # ==== Tensor 化并搬设备 ====\n",
    "        user_ids = torch.LongTensor(user_ids).to(device)\n",
    "        pos_ids  = torch.LongTensor(pos_ids).to(device)\n",
    "        neg_ids  = torch.LongTensor(neg_ids).to(device)\n",
    "\n",
    "        # ==== 前向、计算损失 ====\n",
    "        user_emb, item_emb = model.get_embedding()          # already on device\n",
    "        loss = bpr_loss(user_emb[user_ids],\n",
    "                        item_emb[pos_ids],\n",
    "                        item_emb[neg_ids])\n",
    "\n",
    "        # ==== 反向 ====\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # （可选）梯度裁剪\n",
    "        if max_grad_norm is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # ==== 打印 ====\n",
    "        if epoch % print_every == 0:\n",
    "            print(f\"[Epoch {epoch}/{epochs}]  BPR Loss = {loss.item():.4f}\")\n",
    "\n",
    "    # -------- 训练完返回最终 loss (方便日志) --------\n",
    "    return float(loss.item())\n"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "10c90823",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T15:43:50.284843Z",
     "start_time": "2025-06-30T15:43:50.271803Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_ranking(\n",
    "        test_df,              # DataFrame, 必含 user_id / item_id\n",
    "        train_df,             # DataFrame, 用来构建用户→已交互物品集合\n",
    "        score_fn,             # callable(users_tensor, items_tensor) → np.array\n",
    "        num_items,            # 物品总数\n",
    "        k=10,                 # Hit@K / NDCG@K\n",
    "        num_neg=100,          # 每个正样本采多少负样本\n",
    "        user_col='user_id',\n",
    "        item_col='item_id',\n",
    "        seed=42\n",
    "    ):\n",
    "    \"\"\"\n",
    "    不依赖具体模型，只要提供 score_fn 就能评估。\n",
    "    score_fn: 接收 (user_tensor, item_tensor) 并返回同长度的 Numpy 分数向量。\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # 用户历史，用于采负样本 & 过滤\n",
    "    train_user_dict = (\n",
    "        train_df.groupby(user_col)[item_col].apply(set).to_dict()\n",
    "    )\n",
    "\n",
    "    hits, ndcgs = [], []\n",
    "\n",
    "    for _, row in test_df.iterrows():\n",
    "        u = int(row[user_col])\n",
    "        pos_item = int(row[item_col])\n",
    "\n",
    "        # ---------- 负采样 ----------\n",
    "        neg_items = set()\n",
    "        while len(neg_items) < num_neg:\n",
    "            neg = random.randint(0, num_items - 1)\n",
    "            if neg not in train_user_dict.get(u, set()) and neg != pos_item:\n",
    "                neg_items.add(neg)\n",
    "\n",
    "        item_candidates = list(neg_items) + [pos_item]\n",
    "\n",
    "        # ---------- 评分 ----------\n",
    "        users_t  = torch.LongTensor([u] * len(item_candidates))\n",
    "        items_t  = torch.LongTensor(item_candidates)\n",
    "        scores   = score_fn(users_t, items_t)        # ← 只依赖 score_fn\n",
    "        rank_idx = np.argsort(scores)[::-1]          # 降序\n",
    "        ranked_items = [item_candidates[i] for i in rank_idx]\n",
    "\n",
    "        # ---------- 指标 ----------\n",
    "        if pos_item in ranked_items[:k]:\n",
    "            hits.append(1)\n",
    "            rank_pos = ranked_items.index(pos_item)\n",
    "            ndcgs.append(1 / np.log2(rank_pos + 2))\n",
    "        else:\n",
    "            hits.append(0)\n",
    "            ndcgs.append(0)\n",
    "\n",
    "    hit_rate = float(np.mean(hits))\n",
    "    ndcg     = float(np.mean(ndcgs))\n",
    "    return hit_rate, ndcg"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "b28f36ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T15:43:50.330432Z",
     "start_time": "2025-06-30T15:43:50.321725Z"
    }
   },
   "source": [
    "# Top-K recommendation for a user\n",
    "def recommend_top_k(model, user_id, train_df, k=10):\n",
    "    model.eval()\n",
    "    user_emb, item_emb = model.get_embedding()\n",
    "    seen_items = set(train_df[train_df['user_id'] == user_id]['item_id'])\n",
    "    all_items = torch.arange(model.num_items)\n",
    "    scores = model.predict(torch.LongTensor([user_id] * model.num_items), all_items).detach().numpy()\n",
    "\n",
    "    ranked_items = np.argsort(scores)[::-1]\n",
    "    recommended = [i for i in ranked_items if i not in seen_items][:k]\n",
    "    return recommended"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "72feadb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T15:43:50.401146Z",
     "start_time": "2025-06-30T15:43:50.394818Z"
    }
   },
   "source": [
    "# Save and load model\n",
    "\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "cd29a40c68841c9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T15:53:17.433220Z",
     "start_time": "2025-06-30T15:43:50.411531Z"
    }
   },
   "source": [
    "\n",
    "# ===== Instantiate LightGCN with image vectors =====\n",
    "model = LightGCN(num_users, num_items,\n",
    "                 embedding_dim=128,       # id embedding dim\n",
    "                 n_layers=2,\n",
    "                 adjacency=adj_torch,\n",
    "                 image_tensor=image_tensor,\n",
    "                 freeze_image=True)\n",
    "model.to(device)\n",
    "train_model(model=model, train_df=train_df, epochs=300,\n",
    "            batch_size=1024, lr=0.005, num_items=model.num_items)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/300]  BPR Loss = 0.6930\n",
      "[Epoch 2/300]  BPR Loss = 0.6928\n",
      "[Epoch 3/300]  BPR Loss = 0.6925\n",
      "[Epoch 4/300]  BPR Loss = 0.6921\n",
      "[Epoch 5/300]  BPR Loss = 0.6915\n",
      "[Epoch 6/300]  BPR Loss = 0.6907\n",
      "[Epoch 7/300]  BPR Loss = 0.6904\n",
      "[Epoch 8/300]  BPR Loss = 0.6891\n",
      "[Epoch 9/300]  BPR Loss = 0.6874\n",
      "[Epoch 10/300]  BPR Loss = 0.6864\n",
      "[Epoch 11/300]  BPR Loss = 0.6850\n",
      "[Epoch 12/300]  BPR Loss = 0.6835\n",
      "[Epoch 13/300]  BPR Loss = 0.6812\n",
      "[Epoch 14/300]  BPR Loss = 0.6778\n",
      "[Epoch 15/300]  BPR Loss = 0.6775\n",
      "[Epoch 16/300]  BPR Loss = 0.6744\n",
      "[Epoch 17/300]  BPR Loss = 0.6746\n",
      "[Epoch 18/300]  BPR Loss = 0.6695\n",
      "[Epoch 19/300]  BPR Loss = 0.6643\n",
      "[Epoch 20/300]  BPR Loss = 0.6608\n",
      "[Epoch 21/300]  BPR Loss = 0.6640\n",
      "[Epoch 22/300]  BPR Loss = 0.6590\n",
      "[Epoch 23/300]  BPR Loss = 0.6557\n",
      "[Epoch 24/300]  BPR Loss = 0.6549\n",
      "[Epoch 25/300]  BPR Loss = 0.6470\n",
      "[Epoch 26/300]  BPR Loss = 0.6460\n",
      "[Epoch 27/300]  BPR Loss = 0.6462\n",
      "[Epoch 28/300]  BPR Loss = 0.6412\n",
      "[Epoch 29/300]  BPR Loss = 0.6305\n",
      "[Epoch 30/300]  BPR Loss = 0.6356\n",
      "[Epoch 31/300]  BPR Loss = 0.6280\n",
      "[Epoch 32/300]  BPR Loss = 0.6215\n",
      "[Epoch 33/300]  BPR Loss = 0.6179\n",
      "[Epoch 34/300]  BPR Loss = 0.6126\n",
      "[Epoch 35/300]  BPR Loss = 0.6079\n",
      "[Epoch 36/300]  BPR Loss = 0.6083\n",
      "[Epoch 37/300]  BPR Loss = 0.5992\n",
      "[Epoch 38/300]  BPR Loss = 0.5992\n",
      "[Epoch 39/300]  BPR Loss = 0.5942\n",
      "[Epoch 40/300]  BPR Loss = 0.5827\n",
      "[Epoch 41/300]  BPR Loss = 0.5788\n",
      "[Epoch 42/300]  BPR Loss = 0.5812\n",
      "[Epoch 43/300]  BPR Loss = 0.5756\n",
      "[Epoch 44/300]  BPR Loss = 0.5627\n",
      "[Epoch 45/300]  BPR Loss = 0.5626\n",
      "[Epoch 46/300]  BPR Loss = 0.5602\n",
      "[Epoch 47/300]  BPR Loss = 0.5478\n",
      "[Epoch 48/300]  BPR Loss = 0.5451\n",
      "[Epoch 49/300]  BPR Loss = 0.5260\n",
      "[Epoch 50/300]  BPR Loss = 0.5375\n",
      "[Epoch 51/300]  BPR Loss = 0.5214\n",
      "[Epoch 52/300]  BPR Loss = 0.5177\n",
      "[Epoch 53/300]  BPR Loss = 0.5136\n",
      "[Epoch 54/300]  BPR Loss = 0.5107\n",
      "[Epoch 55/300]  BPR Loss = 0.5120\n",
      "[Epoch 56/300]  BPR Loss = 0.4915\n",
      "[Epoch 57/300]  BPR Loss = 0.4948\n",
      "[Epoch 58/300]  BPR Loss = 0.4943\n",
      "[Epoch 59/300]  BPR Loss = 0.4726\n",
      "[Epoch 60/300]  BPR Loss = 0.4779\n",
      "[Epoch 61/300]  BPR Loss = 0.4750\n",
      "[Epoch 62/300]  BPR Loss = 0.4671\n",
      "[Epoch 63/300]  BPR Loss = 0.4644\n",
      "[Epoch 64/300]  BPR Loss = 0.4461\n",
      "[Epoch 65/300]  BPR Loss = 0.4405\n",
      "[Epoch 66/300]  BPR Loss = 0.4494\n",
      "[Epoch 67/300]  BPR Loss = 0.4392\n",
      "[Epoch 68/300]  BPR Loss = 0.4238\n",
      "[Epoch 69/300]  BPR Loss = 0.4105\n",
      "[Epoch 70/300]  BPR Loss = 0.4079\n",
      "[Epoch 71/300]  BPR Loss = 0.4207\n",
      "[Epoch 72/300]  BPR Loss = 0.4152\n",
      "[Epoch 73/300]  BPR Loss = 0.4049\n",
      "[Epoch 74/300]  BPR Loss = 0.4018\n",
      "[Epoch 75/300]  BPR Loss = 0.3956\n",
      "[Epoch 76/300]  BPR Loss = 0.3896\n",
      "[Epoch 77/300]  BPR Loss = 0.3852\n",
      "[Epoch 78/300]  BPR Loss = 0.3651\n",
      "[Epoch 79/300]  BPR Loss = 0.3744\n",
      "[Epoch 80/300]  BPR Loss = 0.3741\n",
      "[Epoch 81/300]  BPR Loss = 0.3789\n",
      "[Epoch 82/300]  BPR Loss = 0.3722\n",
      "[Epoch 83/300]  BPR Loss = 0.3568\n",
      "[Epoch 84/300]  BPR Loss = 0.3583\n",
      "[Epoch 85/300]  BPR Loss = 0.3583\n",
      "[Epoch 86/300]  BPR Loss = 0.3388\n",
      "[Epoch 87/300]  BPR Loss = 0.3602\n",
      "[Epoch 88/300]  BPR Loss = 0.3407\n",
      "[Epoch 89/300]  BPR Loss = 0.3488\n",
      "[Epoch 90/300]  BPR Loss = 0.3359\n",
      "[Epoch 91/300]  BPR Loss = 0.3330\n",
      "[Epoch 92/300]  BPR Loss = 0.3217\n",
      "[Epoch 93/300]  BPR Loss = 0.3326\n",
      "[Epoch 94/300]  BPR Loss = 0.3243\n",
      "[Epoch 95/300]  BPR Loss = 0.3115\n",
      "[Epoch 96/300]  BPR Loss = 0.3189\n",
      "[Epoch 97/300]  BPR Loss = 0.3138\n",
      "[Epoch 98/300]  BPR Loss = 0.3149\n",
      "[Epoch 99/300]  BPR Loss = 0.3204\n",
      "[Epoch 100/300]  BPR Loss = 0.2995\n",
      "[Epoch 101/300]  BPR Loss = 0.3137\n",
      "[Epoch 102/300]  BPR Loss = 0.3090\n",
      "[Epoch 103/300]  BPR Loss = 0.2998\n",
      "[Epoch 104/300]  BPR Loss = 0.2997\n",
      "[Epoch 105/300]  BPR Loss = 0.2938\n",
      "[Epoch 106/300]  BPR Loss = 0.3025\n",
      "[Epoch 107/300]  BPR Loss = 0.2856\n",
      "[Epoch 108/300]  BPR Loss = 0.2819\n",
      "[Epoch 109/300]  BPR Loss = 0.2880\n",
      "[Epoch 110/300]  BPR Loss = 0.2882\n",
      "[Epoch 111/300]  BPR Loss = 0.2809\n",
      "[Epoch 112/300]  BPR Loss = 0.2743\n",
      "[Epoch 113/300]  BPR Loss = 0.2775\n",
      "[Epoch 114/300]  BPR Loss = 0.2808\n",
      "[Epoch 115/300]  BPR Loss = 0.2713\n",
      "[Epoch 116/300]  BPR Loss = 0.2762\n",
      "[Epoch 117/300]  BPR Loss = 0.2706\n",
      "[Epoch 118/300]  BPR Loss = 0.2768\n",
      "[Epoch 119/300]  BPR Loss = 0.2745\n",
      "[Epoch 120/300]  BPR Loss = 0.2596\n",
      "[Epoch 121/300]  BPR Loss = 0.2769\n",
      "[Epoch 122/300]  BPR Loss = 0.2606\n",
      "[Epoch 123/300]  BPR Loss = 0.2658\n",
      "[Epoch 124/300]  BPR Loss = 0.2528\n",
      "[Epoch 125/300]  BPR Loss = 0.2533\n",
      "[Epoch 126/300]  BPR Loss = 0.2617\n",
      "[Epoch 127/300]  BPR Loss = 0.2481\n",
      "[Epoch 128/300]  BPR Loss = 0.2609\n",
      "[Epoch 129/300]  BPR Loss = 0.2385\n",
      "[Epoch 130/300]  BPR Loss = 0.2506\n",
      "[Epoch 131/300]  BPR Loss = 0.2271\n",
      "[Epoch 132/300]  BPR Loss = 0.2396\n",
      "[Epoch 133/300]  BPR Loss = 0.2407\n",
      "[Epoch 134/300]  BPR Loss = 0.2375\n",
      "[Epoch 135/300]  BPR Loss = 0.2419\n",
      "[Epoch 136/300]  BPR Loss = 0.2406\n",
      "[Epoch 137/300]  BPR Loss = 0.2146\n",
      "[Epoch 138/300]  BPR Loss = 0.2516\n",
      "[Epoch 139/300]  BPR Loss = 0.2236\n",
      "[Epoch 140/300]  BPR Loss = 0.2292\n",
      "[Epoch 141/300]  BPR Loss = 0.2177\n",
      "[Epoch 142/300]  BPR Loss = 0.2182\n",
      "[Epoch 143/300]  BPR Loss = 0.2435\n",
      "[Epoch 144/300]  BPR Loss = 0.2326\n",
      "[Epoch 145/300]  BPR Loss = 0.2100\n",
      "[Epoch 146/300]  BPR Loss = 0.2138\n",
      "[Epoch 147/300]  BPR Loss = 0.2317\n",
      "[Epoch 148/300]  BPR Loss = 0.2139\n",
      "[Epoch 149/300]  BPR Loss = 0.2277\n",
      "[Epoch 150/300]  BPR Loss = 0.2218\n",
      "[Epoch 151/300]  BPR Loss = 0.2026\n",
      "[Epoch 152/300]  BPR Loss = 0.2246\n",
      "[Epoch 153/300]  BPR Loss = 0.2218\n",
      "[Epoch 154/300]  BPR Loss = 0.2220\n",
      "[Epoch 155/300]  BPR Loss = 0.2073\n",
      "[Epoch 156/300]  BPR Loss = 0.2190\n",
      "[Epoch 157/300]  BPR Loss = 0.1848\n",
      "[Epoch 158/300]  BPR Loss = 0.2126\n",
      "[Epoch 159/300]  BPR Loss = 0.2080\n",
      "[Epoch 160/300]  BPR Loss = 0.2021\n",
      "[Epoch 161/300]  BPR Loss = 0.2000\n",
      "[Epoch 162/300]  BPR Loss = 0.1899\n",
      "[Epoch 163/300]  BPR Loss = 0.1970\n",
      "[Epoch 164/300]  BPR Loss = 0.1915\n",
      "[Epoch 165/300]  BPR Loss = 0.1958\n",
      "[Epoch 166/300]  BPR Loss = 0.1888\n",
      "[Epoch 167/300]  BPR Loss = 0.2143\n",
      "[Epoch 168/300]  BPR Loss = 0.1934\n",
      "[Epoch 169/300]  BPR Loss = 0.1968\n",
      "[Epoch 170/300]  BPR Loss = 0.1917\n",
      "[Epoch 171/300]  BPR Loss = 0.1956\n",
      "[Epoch 172/300]  BPR Loss = 0.2041\n",
      "[Epoch 173/300]  BPR Loss = 0.1957\n",
      "[Epoch 174/300]  BPR Loss = 0.1954\n",
      "[Epoch 175/300]  BPR Loss = 0.1772\n",
      "[Epoch 176/300]  BPR Loss = 0.2040\n",
      "[Epoch 177/300]  BPR Loss = 0.1759\n",
      "[Epoch 178/300]  BPR Loss = 0.1822\n",
      "[Epoch 179/300]  BPR Loss = 0.2050\n",
      "[Epoch 180/300]  BPR Loss = 0.1910\n",
      "[Epoch 181/300]  BPR Loss = 0.1753\n",
      "[Epoch 182/300]  BPR Loss = 0.1808\n",
      "[Epoch 183/300]  BPR Loss = 0.1893\n",
      "[Epoch 184/300]  BPR Loss = 0.1780\n",
      "[Epoch 185/300]  BPR Loss = 0.1817\n",
      "[Epoch 186/300]  BPR Loss = 0.1665\n",
      "[Epoch 187/300]  BPR Loss = 0.1888\n",
      "[Epoch 188/300]  BPR Loss = 0.1735\n",
      "[Epoch 189/300]  BPR Loss = 0.1810\n",
      "[Epoch 190/300]  BPR Loss = 0.1803\n",
      "[Epoch 191/300]  BPR Loss = 0.1655\n",
      "[Epoch 192/300]  BPR Loss = 0.1616\n",
      "[Epoch 193/300]  BPR Loss = 0.1734\n",
      "[Epoch 194/300]  BPR Loss = 0.1616\n",
      "[Epoch 195/300]  BPR Loss = 0.1580\n",
      "[Epoch 196/300]  BPR Loss = 0.1688\n",
      "[Epoch 197/300]  BPR Loss = 0.1724\n",
      "[Epoch 198/300]  BPR Loss = 0.1818\n",
      "[Epoch 199/300]  BPR Loss = 0.1807\n",
      "[Epoch 200/300]  BPR Loss = 0.1625\n",
      "[Epoch 201/300]  BPR Loss = 0.1754\n",
      "[Epoch 202/300]  BPR Loss = 0.1721\n",
      "[Epoch 203/300]  BPR Loss = 0.1741\n",
      "[Epoch 204/300]  BPR Loss = 0.1719\n",
      "[Epoch 205/300]  BPR Loss = 0.1582\n",
      "[Epoch 206/300]  BPR Loss = 0.1648\n",
      "[Epoch 207/300]  BPR Loss = 0.1609\n",
      "[Epoch 208/300]  BPR Loss = 0.1662\n",
      "[Epoch 209/300]  BPR Loss = 0.1632\n",
      "[Epoch 210/300]  BPR Loss = 0.1678\n",
      "[Epoch 211/300]  BPR Loss = 0.1563\n",
      "[Epoch 212/300]  BPR Loss = 0.1580\n",
      "[Epoch 213/300]  BPR Loss = 0.1677\n",
      "[Epoch 214/300]  BPR Loss = 0.1606\n",
      "[Epoch 215/300]  BPR Loss = 0.1782\n",
      "[Epoch 216/300]  BPR Loss = 0.1768\n",
      "[Epoch 217/300]  BPR Loss = 0.1616\n",
      "[Epoch 218/300]  BPR Loss = 0.1524\n",
      "[Epoch 219/300]  BPR Loss = 0.1555\n",
      "[Epoch 220/300]  BPR Loss = 0.1607\n",
      "[Epoch 221/300]  BPR Loss = 0.1501\n",
      "[Epoch 222/300]  BPR Loss = 0.1344\n",
      "[Epoch 223/300]  BPR Loss = 0.1414\n",
      "[Epoch 224/300]  BPR Loss = 0.1649\n",
      "[Epoch 225/300]  BPR Loss = 0.1553\n",
      "[Epoch 226/300]  BPR Loss = 0.1516\n",
      "[Epoch 227/300]  BPR Loss = 0.1450\n",
      "[Epoch 228/300]  BPR Loss = 0.1533\n",
      "[Epoch 229/300]  BPR Loss = 0.1401\n",
      "[Epoch 230/300]  BPR Loss = 0.1291\n",
      "[Epoch 231/300]  BPR Loss = 0.1511\n",
      "[Epoch 232/300]  BPR Loss = 0.1436\n",
      "[Epoch 233/300]  BPR Loss = 0.1452\n",
      "[Epoch 234/300]  BPR Loss = 0.1551\n",
      "[Epoch 235/300]  BPR Loss = 0.1447\n",
      "[Epoch 236/300]  BPR Loss = 0.1464\n",
      "[Epoch 237/300]  BPR Loss = 0.1417\n",
      "[Epoch 238/300]  BPR Loss = 0.1434\n",
      "[Epoch 239/300]  BPR Loss = 0.1380\n",
      "[Epoch 240/300]  BPR Loss = 0.1436\n",
      "[Epoch 241/300]  BPR Loss = 0.1262\n",
      "[Epoch 242/300]  BPR Loss = 0.1332\n",
      "[Epoch 243/300]  BPR Loss = 0.1438\n",
      "[Epoch 244/300]  BPR Loss = 0.1362\n",
      "[Epoch 245/300]  BPR Loss = 0.1296\n",
      "[Epoch 246/300]  BPR Loss = 0.1279\n",
      "[Epoch 247/300]  BPR Loss = 0.1341\n",
      "[Epoch 248/300]  BPR Loss = 0.1514\n",
      "[Epoch 249/300]  BPR Loss = 0.1373\n",
      "[Epoch 250/300]  BPR Loss = 0.1204\n",
      "[Epoch 251/300]  BPR Loss = 0.1409\n",
      "[Epoch 252/300]  BPR Loss = 0.1590\n",
      "[Epoch 253/300]  BPR Loss = 0.1304\n",
      "[Epoch 254/300]  BPR Loss = 0.1417\n",
      "[Epoch 255/300]  BPR Loss = 0.1324\n",
      "[Epoch 256/300]  BPR Loss = 0.1401\n",
      "[Epoch 257/300]  BPR Loss = 0.1204\n",
      "[Epoch 258/300]  BPR Loss = 0.1371\n",
      "[Epoch 259/300]  BPR Loss = 0.1233\n",
      "[Epoch 260/300]  BPR Loss = 0.1304\n",
      "[Epoch 261/300]  BPR Loss = 0.1362\n",
      "[Epoch 262/300]  BPR Loss = 0.1414\n",
      "[Epoch 263/300]  BPR Loss = 0.1127\n",
      "[Epoch 264/300]  BPR Loss = 0.1246\n",
      "[Epoch 265/300]  BPR Loss = 0.1441\n",
      "[Epoch 266/300]  BPR Loss = 0.1473\n",
      "[Epoch 267/300]  BPR Loss = 0.1243\n",
      "[Epoch 268/300]  BPR Loss = 0.1254\n",
      "[Epoch 269/300]  BPR Loss = 0.1342\n",
      "[Epoch 270/300]  BPR Loss = 0.1357\n",
      "[Epoch 271/300]  BPR Loss = 0.1343\n",
      "[Epoch 272/300]  BPR Loss = 0.1178\n",
      "[Epoch 273/300]  BPR Loss = 0.1368\n",
      "[Epoch 274/300]  BPR Loss = 0.1195\n",
      "[Epoch 275/300]  BPR Loss = 0.1383\n",
      "[Epoch 276/300]  BPR Loss = 0.1253\n",
      "[Epoch 277/300]  BPR Loss = 0.1361\n",
      "[Epoch 278/300]  BPR Loss = 0.1113\n",
      "[Epoch 279/300]  BPR Loss = 0.1169\n",
      "[Epoch 280/300]  BPR Loss = 0.1271\n",
      "[Epoch 281/300]  BPR Loss = 0.1231\n",
      "[Epoch 282/300]  BPR Loss = 0.1146\n",
      "[Epoch 283/300]  BPR Loss = 0.1393\n",
      "[Epoch 284/300]  BPR Loss = 0.1281\n",
      "[Epoch 285/300]  BPR Loss = 0.1153\n",
      "[Epoch 286/300]  BPR Loss = 0.1144\n",
      "[Epoch 287/300]  BPR Loss = 0.1177\n",
      "[Epoch 288/300]  BPR Loss = 0.1258\n",
      "[Epoch 289/300]  BPR Loss = 0.1261\n",
      "[Epoch 290/300]  BPR Loss = 0.1258\n",
      "[Epoch 291/300]  BPR Loss = 0.1099\n",
      "[Epoch 292/300]  BPR Loss = 0.1159\n",
      "[Epoch 293/300]  BPR Loss = 0.1078\n",
      "[Epoch 294/300]  BPR Loss = 0.1117\n",
      "[Epoch 295/300]  BPR Loss = 0.1066\n",
      "[Epoch 296/300]  BPR Loss = 0.1103\n",
      "[Epoch 297/300]  BPR Loss = 0.1190\n",
      "[Epoch 298/300]  BPR Loss = 0.0948\n",
      "[Epoch 299/300]  BPR Loss = 0.1224\n",
      "[Epoch 300/300]  BPR Loss = 0.1027\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10271526128053665"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "2967b016",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T15:53:17.581628Z",
     "start_time": "2025-06-30T15:53:17.573731Z"
    }
   },
   "source": [
    "def random_score_fn(users_t, items_t):\n",
    "    # 随机给每个 items_t 一个分数；users_t 不使用，但必须接收\n",
    "    return np.random.rand(len(items_t))"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "662cea8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T15:53:17.675095Z",
     "start_time": "2025-06-30T15:53:17.666689Z"
    }
   },
   "source": [
    "def make_popularity_score_fn(train_df, item_col='item_id'):\n",
    "    item_cnt = Counter(train_df[item_col])\n",
    "    default_score = min(item_cnt.values()) - 1  # 给没出现过的物品一个更低分\n",
    "    def _score_fn(users_t, items_t):\n",
    "        return np.array([item_cnt.get(int(i), default_score) for i in items_t])\n",
    "    return _score_fn"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "3942ea9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T15:53:17.724789Z",
     "start_time": "2025-06-30T15:53:17.715266Z"
    }
   },
   "source": [
    "def make_lightgcn_score_fn(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        user_emb, item_emb = model.get_embedding()\n",
    "        user_emb = user_emb.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        item_emb = item_emb.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def score_fn(users_t, items_t):\n",
    "        u_vec = user_emb[users_t]\n",
    "        i_vec = item_emb[items_t]\n",
    "        return torch.sum(u_vec * i_vec, dim=1).cpu().numpy()\n",
    "    return score_fn\n"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "bbd4f9d7af8b5c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T15:55:01.664625Z",
     "start_time": "2025-06-30T15:53:17.818248Z"
    }
   },
   "source": [
    "pop_score_fn  = make_popularity_score_fn(train_df)\n",
    "# ---------------- baseline：Popular ----------------\n",
    "hit_pop, ndcg_pop = evaluate_ranking(\n",
    "    test_df, train_df, pop_score_fn,\n",
    "    num_items=model.num_items, k=10\n",
    ")\n",
    "print(f\"Popular  Hit@10={hit_pop:.4f}  NDCG@10={ndcg_pop:.4f}\")\n",
    "\n",
    "# ---------------- baseline：Random -----------------\n",
    "hit_rand, ndcg_rand = evaluate_ranking(\n",
    "    test_df, train_df, random_score_fn,\n",
    "    num_items=model.num_items, k=10\n",
    ")\n",
    "print(f\"Random   Hit@10={hit_rand:.4f}  NDCG@10={ndcg_rand:.4f}\")\n",
    "\n",
    "# ---------------- LightGCN（或其他模型）------------\n",
    "score_fn_gcn = make_lightgcn_score_fn(model)\n",
    "hit_gcn, ndcg_gcn = evaluate_ranking(\n",
    "    test_df, train_df, score_fn_gcn,\n",
    "    num_items=model.num_items, k=10\n",
    ")\n",
    "print(f\"LightGCN Hit@10={hit_gcn:.4f}  NDCG@10={ndcg_gcn:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Popular  Hit@10=0.2166  NDCG@10=0.1081\n",
      "Random   Hit@10=0.0996  NDCG@10=0.0455\n",
      "LightGCN Hit@10=0.4949  NDCG@10=0.2983\n"
     ]
    }
   ],
   "execution_count": 27
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DP1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

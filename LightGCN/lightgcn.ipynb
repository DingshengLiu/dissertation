{
  "cells": [
    {
      "metadata": {
        "id": "a8786635ccc5e581",
        "outputId": "523b4d0c-49d0-41c1-f5d9-2c207425c78e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "execution_count": 1,
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "def copy_from_drive(src_path, dst_path):\n",
        "\n",
        "    if os.path.exists(dst_path):\n",
        "        print(f\"skip:{dst_path} exists\")\n",
        "        return\n",
        "\n",
        "    if os.path.isdir(src_path):\n",
        "        shutil.copytree(src_path, dst_path)\n",
        "    elif os.path.isfile(src_path):\n",
        "        shutil.copy(src_path, dst_path)\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "copy_from_drive('/content/drive/MyDrive/tool', '/content/tool')\n",
        "copy_from_drive('/content/drive/MyDrive/MicroLens-50k_pairs.csv','/content/MicroLens-50k_pairs.csv')"
      ],
      "id": "a8786635ccc5e581"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b828e981",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T20:26:13.343878Z",
          "start_time": "2025-04-30T20:26:13.338767Z"
        },
        "id": "b828e981",
        "outputId": "7b4cbc80-c10b-4db5-dfd3-6de2138329b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "from tool import evaluate\n",
        "from tool import preprocess\n",
        "from tool import customdataset\n",
        "!pip install faiss-cpu\n",
        "import faiss\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e99a31348e0a553",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T20:26:13.391974Z",
          "start_time": "2025-04-30T20:26:13.387554Z"
        },
        "id": "e99a31348e0a553"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "76a2087d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T20:26:13.572661Z",
          "start_time": "2025-04-30T20:26:13.416069Z"
        },
        "id": "76a2087d"
      },
      "outputs": [],
      "source": [
        "# dataset_pd = pd.read_csv('D:\\\\VideoRecSystem\\\\MicroLens\\\\DataSet\\\\MicroLens-50k_pairs.csv')\n",
        "path = 'MicroLens-50k_pairs.csv'\n",
        "user = 'user'\n",
        "item = 'item'\n",
        "user_id = 'user_id'\n",
        "item_id = 'item_id'\n",
        "timestamp = 'timestamp'\n",
        "save_dir = './embeddings'\n",
        "top_k = 10\n",
        "num_workers = 10\n",
        "k_neg = 10\n",
        "# path = pd.read_csv('MicroLens-50k_pairs.csv')\n",
        "# dataset_pd = pd.read_csv('MicroLens-50k_pairs.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "943fc6c5",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T20:26:13.615265Z",
          "start_time": "2025-04-30T20:26:13.605752Z"
        },
        "id": "943fc6c5",
        "outputId": "4275f8c5-5ffb-4313-add3-55b4c852dffe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset base information：\n",
            "- number of users：50000\n",
            "- number of items：19220\n",
            "- number of rows：359708\n"
          ]
        }
      ],
      "source": [
        "dataset_pd,num_users,num_items = preprocess.openAndSort(path,user_id=user,item_id=item,timestamp='timestamp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "69160caa",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T20:26:13.713174Z",
          "start_time": "2025-04-30T20:26:13.702986Z"
        },
        "id": "69160caa",
        "outputId": "5da9c08d-d382-4006-ee32-45680442e12f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 309708\n",
            "Test size: 49424\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_df, test_df = preprocess.split(dataset_pd,user, item, timestamp)\n",
        "print(f\"Train size: {len(train_df)}\")\n",
        "print(f\"Test size: {len(test_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6d42c375",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T20:26:13.848465Z",
          "start_time": "2025-04-30T20:26:13.789646Z"
        },
        "id": "6d42c375"
      },
      "outputs": [],
      "source": [
        "# maintain a map from new id to old id, new id for constructing matrix\n",
        "user2id = {u: i for i, u in enumerate(dataset_pd[user].unique())}\n",
        "item2id = {i: j for j, i in enumerate(dataset_pd[item].unique())}\n",
        "\n",
        "# apply to train_df and test_df\n",
        "train_df[user_id] = train_df[user].map(user2id)\n",
        "train_df[item_id] = train_df[item].map(item2id)\n",
        "test_df[user_id] = test_df[user].map(user2id)\n",
        "test_df[item_id] = test_df[item].map(item2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a28cf3d3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T20:26:15.725506Z",
          "start_time": "2025-04-30T20:26:15.717Z"
        },
        "id": "a28cf3d3"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_adj_matrix(df, num_users, num_items ,user_id, item_id):\n",
        "    rows = df[user_id].values\n",
        "    cols = df[item_id].values\n",
        "    data = np.ones(len(df))\n",
        "    # set interaction of user-item as 1, other as 0\n",
        "    R = sp.coo_matrix((data, (rows, cols)), shape=(num_users, num_items))\n",
        "\n",
        "    # construct symetric matrix A\n",
        "    upper = sp.hstack([sp.csr_matrix((num_users, num_users)), R])\n",
        "    lower = sp.hstack([R.T, sp.csr_matrix((num_items, num_items))])\n",
        "    A = sp.vstack([upper, lower])\n",
        "\n",
        "    # normalization A → Ĥ = D^{-1/2} A D^{-1/2}\n",
        "    rowsum = np.array(A.sum(1)).flatten()\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5)\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    D_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    A_norm = D_inv_sqrt @ A @ D_inv_sqrt\n",
        "\n",
        "    # transform to torch.sparse\n",
        "    A_norm = A_norm.tocoo()\n",
        "    indices = torch.LongTensor([A_norm.row, A_norm.col])\n",
        "    values = torch.FloatTensor(A_norm.data)\n",
        "    return torch.sparse_coo_tensor(indices, values, A_norm.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3d9cee55",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T20:26:16.173195Z",
          "start_time": "2025-04-30T20:26:15.789923Z"
        },
        "id": "3d9cee55",
        "outputId": "5ac088a8-3b2d-465d-e0ed-830347525f09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-8-2216191345.py:15: RuntimeWarning: divide by zero encountered in power\n",
            "  d_inv_sqrt = np.power(rowsum, -0.5)\n",
            "/tmp/ipython-input-8-2216191345.py:22: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  indices = torch.LongTensor([A_norm.row, A_norm.col])\n"
          ]
        }
      ],
      "source": [
        "adj_torch = build_adj_matrix(train_df, num_users, num_items ,user_id, item_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a7aba75c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T20:26:16.197535Z",
          "start_time": "2025-04-30T20:26:16.187729Z"
        },
        "id": "a7aba75c"
      },
      "outputs": [],
      "source": [
        "#  LightGCN implementation in PyTorch\n",
        "class LightGCN(nn.Module):\n",
        "    def __init__(self, num_users, num_items, embedding_dim, n_layers, adjacency):\n",
        "        super(LightGCN, self).__init__()\n",
        "        self.user_emb = None\n",
        "        self.item_emb = None\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.adjacency = adjacency  # torch.sparse format\n",
        "\n",
        "        self.embedding_user = nn.Embedding(num_users, embedding_dim)\n",
        "        self.embedding_item = nn.Embedding(num_items, embedding_dim)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.embedding_user.weight)\n",
        "        nn.init.xavier_uniform_(self.embedding_item.weight)\n",
        "\n",
        "\n",
        "    def forward(self):\n",
        "        all_embeddings = torch.cat([self.embedding_user.weight, self.embedding_item.weight], dim=0)\n",
        "        embeddings_list = [all_embeddings]\n",
        "\n",
        "        for _ in range(self.n_layers):\n",
        "            all_embeddings = torch.sparse.mm(self.adjacency, all_embeddings)\n",
        "            embeddings_list.append(all_embeddings)\n",
        "\n",
        "        final_embedding = torch.stack(embeddings_list, dim=1).mean(dim=1)\n",
        "        user_embedding, item_embedding = torch.split(final_embedding, [self.num_users, self.num_items])\n",
        "\n",
        "        self.user_emb = user_embedding.detach()\n",
        "        self.item_emb = item_embedding.detach()\n",
        "\n",
        "        return user_embedding, item_embedding\n",
        "\n",
        "\n",
        "    def get_users_embedding(self,user_ids,l2_norm=True):\n",
        "        u_vec = self.user_emb[user_ids]          # (B, emb_dim)\n",
        "        if l2_norm:\n",
        "            u_vec = F.normalize(u_vec, p=2, dim=1)\n",
        "        return u_vec\n",
        "\n",
        "    def get_items_embedding(self,item_ids,l2_norm=True):\n",
        "        i_vec = self.item_emb[item_ids]          # (B, emb_dim)\n",
        "        if l2_norm:\n",
        "            i_vec = F.normalize(i_vec, p=2, dim=1)\n",
        "        return i_vec\n",
        "\n",
        "    def save_embeddings(self, num_users, num_items, device, save_dir='./embeddings'):\n",
        "        import os\n",
        "        import faiss\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        self.eval()\n",
        "        self.to(device)\n",
        "\n",
        "        user_ids = torch.arange(num_users, dtype=torch.long, device=device)\n",
        "        item_ids = torch.arange(num_items, dtype=torch.long, device=device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            user_embeds = self.get_users_embedding(user_ids, l2_norm=True)\n",
        "            item_embeds = self.get_items_embedding(item_ids, l2_norm=True)\n",
        "\n",
        "        user_embeds = user_embeds.cpu().numpy().astype(np.float32)\n",
        "        item_embeds = item_embeds.cpu().numpy().astype(np.float32)\n",
        "\n",
        "        # 保存向量\n",
        "        np.save(f\"{save_dir}/user_embeddings.npy\", user_embeds)\n",
        "        np.save(f\"{save_dir}/item_embeddings.npy\", item_embeds)\n",
        "\n",
        "        # 构建 FAISS index（使用内积）\n",
        "        dim = item_embeds.shape[1]\n",
        "        index = faiss.IndexFlatIP(dim)\n",
        "        index.add(item_embeds)\n",
        "\n",
        "        faiss.write_index(index, f\"{save_dir}/item_index.faiss\")\n",
        "        print(\"Saved user/item embeddings and FAISS index.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "07e268c2",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T20:26:16.347084Z",
          "start_time": "2025-04-30T20:26:16.333836Z"
        },
        "id": "07e268c2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def train_model(model,\n",
        "                train_df,\n",
        "                num_items,\n",
        "                epochs=50,\n",
        "                batch_size=1024,\n",
        "                lr=1e-3,\n",
        "                device=None):\n",
        "    \"\"\"\n",
        "    训练 LightGCN (或其它 BPR 模型) 的通用函数\n",
        "    ------------------------------------------------\n",
        "    • train_df      : pandas DataFrame，含 user_id / item_id\n",
        "    • num_items     : 物品总数\n",
        "    • device        : torch.device；默认为 'cuda' (若可用) 否则 'cpu'\n",
        "    • max_grad_norm : 梯度裁剪阈值；避免梯度爆炸，可选\n",
        "    \"\"\"\n",
        "    # -------- 设备 ----------\n",
        "    if device is None:\n",
        "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    model = model.to(device)\n",
        "    if hasattr(model, \"adjacency\"):               # adjacency 可能是稀疏张量\n",
        "        model.adjacency = model.adjacency.to(device)\n",
        "\n",
        "    # -------- 优化器 ----------\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    train_loader = customdataset.build_train_loader_inbatch(train_df, batch_size=batch_size,user_col=user_id, item_col=item_id)\n",
        "\n",
        "    # -------- 训练循环 ----------\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        dt_start = datetime.now()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            user_ids, pos_item_ids = batch\n",
        "            user_ids = user_ids.to(device).long()\n",
        "            pos_item_ids = pos_item_ids.to(device).long()\n",
        "\n",
        "\n",
        "            # 1. 前向传播（返回 user / item 向量）\n",
        "            user_emb, item_emb = model()\n",
        "            u_vec = user_emb[user_ids]\n",
        "            i_vec = item_emb[pos_item_ids]\n",
        "\n",
        "            # 2. 得分矩阵：每个 user 对所有正 item 的打分\n",
        "            logits = torch.matmul(u_vec, i_vec.T)  # shape: (B, B)\n",
        "\n",
        "            # 3. 构造标签：每个 user 的正确 item 在对角线（即位置 i）\n",
        "            labels = torch.arange(logits.size(0), device=device)  # [0, 1, ..., B-1]\n",
        "\n",
        "            # 4. Cross Entropy Loss\n",
        "            loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "            # 5. 反向传播\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        # 日志\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        dt_end = datetime.now()\n",
        "        dt = dt_end - dt_start\n",
        "\n",
        "        print(f\"[Epoch {epoch:02d}/{epochs}] avg InBatch Softmax Loss = {avg_loss:.4f}, time = {dt.total_seconds():.2f}s\")\n",
        "\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "cd29a40c68841c9c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T20:34:51.542117Z",
          "start_time": "2025-04-30T20:26:16.696795Z"
        },
        "id": "cd29a40c68841c9c",
        "outputId": "4e1277e2-6288-409e-81be-80614d9b88a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 01/10] avg InBatch Softmax Loss = 6.5553, time = 3.02s\n",
            "[Epoch 02/10] avg InBatch Softmax Loss = 5.7569, time = 2.95s\n",
            "[Epoch 03/10] avg InBatch Softmax Loss = 5.3165, time = 2.96s\n",
            "[Epoch 04/10] avg InBatch Softmax Loss = 4.9698, time = 2.98s\n",
            "[Epoch 05/10] avg InBatch Softmax Loss = 4.6297, time = 2.97s\n",
            "[Epoch 06/10] avg InBatch Softmax Loss = 4.2736, time = 2.99s\n",
            "[Epoch 07/10] avg InBatch Softmax Loss = 3.8969, time = 2.95s\n",
            "[Epoch 08/10] avg InBatch Softmax Loss = 3.5051, time = 3.00s\n",
            "[Epoch 09/10] avg InBatch Softmax Loss = 3.1122, time = 2.96s\n",
            "[Epoch 10/10] avg InBatch Softmax Loss = 2.7353, time = 2.98s\n"
          ]
        }
      ],
      "source": [
        "model = LightGCN(num_users,num_items,embedding_dim=128,n_layers=2,adjacency=adj_torch)\n",
        "model.to(device)\n",
        "train_model(model=model,epochs=10, train_df=train_df,num_items=num_items,batch_size=1024)"
      ]
    },
    {
      "metadata": {
        "id": "cc858c515448b5b8",
        "outputId": "c736264f-de28-4d8c-cb84-5c678b20df0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved user/item embeddings and FAISS index.\n"
          ]
        }
      ],
      "execution_count": 27,
      "source": [
        "model.save_embeddings(num_users=num_users,num_items=num_items,device=device,save_dir=save_dir)"
      ],
      "id": "cc858c515448b5b8"
    },
    {
      "metadata": {
        "id": "2c5b952e28007084"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 28,
      "source": [
        "test_loader = customdataset.build_test_loader(test_df, num_items ,user_col = user_id, item_col = item_id, batch_size=1024, num_workers=num_workers)\n",
        "item_pool = list(range(num_items))\n",
        "faiss_index = faiss.read_index(f\"{save_dir}/item_index.faiss\")"
      ],
      "id": "2c5b952e28007084"
    },
    {
      "metadata": {
        "id": "ae68d6ba3c864c65",
        "outputId": "f1248ba8-8a4c-4003-9f8f-51576f2a3d77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random HR@10 = 0.0006, nDCG@10 = 0.0003\n",
            "Popular HR@10 = 0.0029, nDCG@10 = 0.0014\n",
            "Model   HR@10 = 0.0322, nDCG@10 = 0.0121\n"
          ]
        }
      ],
      "execution_count": 29,
      "source": [
        "hr_r, ndcg_r = evaluate.evaluate_random(test_loader, item_pool ,top_k=top_k)\n",
        "print(f\"Random HR@{top_k} = {hr_r:.4f}, nDCG@{top_k} = {ndcg_r:.4f}\")\n",
        "hr_p, ndcg_p = evaluate.evaluate_popular(test_loader, train_df,top_k=top_k)\n",
        "print(f\"Popular HR@{top_k} = {hr_p:.4f}, nDCG@{top_k} = {ndcg_p:.4f}\")\n",
        "hr_m, ndcg_m = evaluate.evaluate_model(test_loader, model, faiss_index, device,top_k=top_k)\n",
        "print(f\"Model   HR@{top_k} = {hr_m:.4f}, nDCG@{top_k} = {ndcg_m:.4f}\")\n"
      ],
      "id": "ae68d6ba3c864c65"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
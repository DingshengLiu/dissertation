{
 "cells": [
  {
   "cell_type": "code",
   "id": "b828e981",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:20.251324Z",
     "start_time": "2025-06-30T21:03:20.244326Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from networkx.readwrite.json_graph import adjacency\n",
    "import random, math, time, os\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "e99a31348e0a553",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:20.262854Z",
     "start_time": "2025-06-30T21:03:20.257852Z"
    }
   },
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "76a2087d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:20.451436Z",
     "start_time": "2025-06-30T21:03:20.297406Z"
    }
   },
   "source": [
    "dataset_pd = pd.read_csv('D:\\\\VideoRecSystem\\\\MicroLens\\\\DataSet\\\\MicroLens-50k_pairs.csv')\n",
    "# dataset_pd = pd.read_csv('MicroLens-50k_pairs.csv')"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "943fc6c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:20.498885Z",
     "start_time": "2025-06-30T21:03:20.487885Z"
    }
   },
   "source": [
    "dataset_pd.head(10)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    user   item      timestamp\n",
       "0  36121   9580  1583378629552\n",
       "1  26572   9580  1583436719018\n",
       "2  37550   9580  1584412681021\n",
       "3  14601   9580  1584848802432\n",
       "4  15061   9580  1585388171106\n",
       "5   6364   9580  1585390736041\n",
       "6   3542   9580  1585404918503\n",
       "7  21038   9580  1590144594477\n",
       "8  12538  14631  1634867362929\n",
       "9  47592  14631  1634872254913"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36121</td>\n",
       "      <td>9580</td>\n",
       "      <td>1583378629552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26572</td>\n",
       "      <td>9580</td>\n",
       "      <td>1583436719018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37550</td>\n",
       "      <td>9580</td>\n",
       "      <td>1584412681021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14601</td>\n",
       "      <td>9580</td>\n",
       "      <td>1584848802432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15061</td>\n",
       "      <td>9580</td>\n",
       "      <td>1585388171106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6364</td>\n",
       "      <td>9580</td>\n",
       "      <td>1585390736041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3542</td>\n",
       "      <td>9580</td>\n",
       "      <td>1585404918503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>21038</td>\n",
       "      <td>9580</td>\n",
       "      <td>1590144594477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12538</td>\n",
       "      <td>14631</td>\n",
       "      <td>1634867362929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>47592</td>\n",
       "      <td>14631</td>\n",
       "      <td>1634872254913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "69160caa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:20.619410Z",
     "start_time": "2025-06-30T21:03:20.607405Z"
    }
   },
   "source": [
    "dataset_pd.count"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.count of          user   item      timestamp\n",
       "0       36121   9580  1583378629552\n",
       "1       26572   9580  1583436719018\n",
       "2       37550   9580  1584412681021\n",
       "3       14601   9580  1584848802432\n",
       "4       15061   9580  1585388171106\n",
       "...       ...    ...            ...\n",
       "359703  48702   1363  1662984066647\n",
       "359704  27203   7291  1662984082974\n",
       "359705  29261  19649  1662984103874\n",
       "359706  28341  19188  1662984123833\n",
       "359707  38967   7254  1662984132429\n",
       "\n",
       "[359708 rows x 3 columns]>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "6d42c375",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:20.741689Z",
     "start_time": "2025-06-30T21:03:20.704856Z"
    }
   },
   "source": [
    "user_counts = dataset_pd['user'].value_counts()\n",
    "item_counts = dataset_pd['item'].value_counts()\n",
    "# valid_users = user_counts[user_counts > 3].index\n",
    "# valid_items = item_counts[item_counts > 3].index\n",
    "# filtered_df = dataset_pd[dataset_pd['user'].isin(valid_users) & dataset_pd['item'].isin(valid_items)]\n",
    "# filtered_df.count"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "8b8ae0c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:20.946749Z",
     "start_time": "2025-06-30T21:03:20.781697Z"
    }
   },
   "source": [
    "# order by user,timestamp \n",
    "filtered_df = dataset_pd.sort_values(by=[\"user\", \"timestamp\"])\n"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "id": "2d2d4de256fd88a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:20.999165Z",
     "start_time": "2025-06-30T21:03:20.990115Z"
    }
   },
   "source": [
    "def split(df, user_col='user', item_col='item', time_col='timestamp'):\n",
    "\n",
    "    df = df.sort_values(by=[user_col, time_col])  # 按用户时间排序\n",
    "\n",
    "    # 获取每个用户的最后一条记录作为 test\n",
    "    test_df = df.groupby(user_col).tail(1)\n",
    "    train_df = df.drop(index=test_df.index)\n",
    "\n",
    "    # 过滤 test 中那些 user/item 不在 train 中的\n",
    "    train_users = set(train_df[user_col])\n",
    "    train_items = set(train_df[item_col])\n",
    "\n",
    "    test_df = test_df[\n",
    "        test_df[user_col].isin(train_users) &\n",
    "        test_df[item_col].isin(train_items)\n",
    "    ]\n",
    "\n",
    "    return train_df.reset_index(drop=True), test_df.reset_index(drop=True)"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "3db1146761e45165",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:21.606722Z",
     "start_time": "2025-06-30T21:03:21.067786Z"
    }
   },
   "source": [
    "\n",
    "train_df, test_df = split(filtered_df,user_col='user', item_col='item', time_col='timestamp')\n",
    "\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Test size: {len(test_df)}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 309708\n",
      "Test size: 49424\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "id": "ce079817",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:22.568380Z",
     "start_time": "2025-06-30T21:03:21.612729Z"
    }
   },
   "source": [
    "# maintain a map from new id to old id, new id for constructing matrix\n",
    "user2id = {u: i for i, u in enumerate(filtered_df['user'].unique())}\n",
    "item2id = {i: j for j, i in enumerate(filtered_df['item'].unique())}\n",
    "\n",
    "# apply to train_df and test_df\n",
    "train_df['user_id'] = train_df['user'].map(user2id)\n",
    "train_df['item_id'] = train_df['item'].map(item2id)\n",
    "test_df['user_id'] = test_df['user'].map(user2id)\n",
    "test_df['item_id'] = test_df['item'].map(item2id)\n",
    "\n",
    "num_users = len(user2id)\n",
    "num_items = len(item2id)\n"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:22.645288Z",
     "start_time": "2025-06-30T21:03:22.610879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import lmdb\n",
    "\n",
    "def load_lmdb_embeddings(lmdb_path, num_items, emb_dim=128):\n",
    "    \"\"\"读取 LMDB 中的 128 维 cover 向量，key 从 1 开始计数\"\"\"\n",
    "    env = lmdb.open(lmdb_path, readonly=True, lock=False,subdir=False)\n",
    "    cover_embs = np.zeros((num_items + 1, emb_dim), dtype=np.float32)  # index 0 保留给 PAD\n",
    "    with env.begin() as txn:\n",
    "        for idx in range(1, num_items + 1):\n",
    "            val = txn.get(str(idx).encode('ascii'))\n",
    "            if val is not None:\n",
    "                cover_embs[idx] = np.frombuffer(val, dtype=np.float32)\n",
    "    env.close()\n",
    "    return torch.tensor(cover_embs)\n",
    "\n",
    "COVER_EMB_PATH = r\"D:/VideoRecSystem/MicroLens/cover_emb128.lmdb\"\n",
    "print(f\"Loading cover embeddings from {COVER_EMB_PATH} ...\")\n",
    "COVER_EMBS = load_lmdb_embeddings(COVER_EMB_PATH, num_items=num_items, emb_dim=128)\n",
    "print(\"COVER_EMBS shape:\", COVER_EMBS.shape)  # (num_items+1, 128)"
   ],
   "id": "b12c88400f9a63f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cover embeddings from D:/VideoRecSystem/MicroLens/cover_emb128.lmdb ...\n",
      "COVER_EMBS shape: torch.Size([19221, 128])\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:22.738347Z",
     "start_time": "2025-06-30T21:03:22.716828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------- 超参数 ----------\n",
    "MAX_SEQ_LEN   = 20          # 输入序列长度\n",
    "EMBEDDING_DIM = 64          # item & user 向量维度\n",
    "NUM_BLOCK     = 3           # 残差层堆叠次数（每次含 dilation=[1,2,4,8]）\n",
    "NEG_SAMPLE    = 5           # 训练时负采样个数\n",
    "BATCH_SIZE    = 512         # 1650Ti 8G 推荐 512；显存紧张用 256\n",
    "EPOCHS        = 10\n",
    "LR            = 1e-3\n",
    "SEED          = 42\n",
    "DROPOUT       = 0.1\n",
    "# ----------------------------\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# ---------- 常量 ----------\n",
    "PAD_IDX  = num_items              # padding 专用 id，不与真实 item 冲突\n",
    "N_ITEMS  = num_items + 1          # Embedding 行数（含 PAD）\n",
    "ALL_ITEM_IDS = np.arange(num_items, dtype=np.int64)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# ----------------------------\n",
    "\n",
    "# ======== Dataset (同前) ======== #\n",
    "class NextItNetBPRDataset(Dataset):\n",
    "    def __init__(self, df, max_len=MAX_SEQ_LEN, pad_idx=PAD_IDX, n_neg=NEG_SAMPLE):\n",
    "        self.max_len, self.pad_idx, self.n_neg = max_len, pad_idx, n_neg\n",
    "        self.inputs, self.targets = [], []\n",
    "        for _, user_hist in df.groupby('user_id'):\n",
    "            seq = user_hist['item_id'].tolist()\n",
    "            for i in range(1, len(seq)):\n",
    "                hist = seq[max(0, i - max_len): i]\n",
    "                hist = [pad_idx]*(max_len - len(hist)) + hist   # 左侧 pad\n",
    "                self.inputs.append(hist)\n",
    "                self.targets.append(seq[i])\n",
    "        self.inputs  = np.asarray(self.inputs,  dtype=np.int64)\n",
    "        self.targets = np.asarray(self.targets, dtype=np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def _neg_sample(self, pos):\n",
    "        negs = np.random.choice(ALL_ITEM_IDS, size=self.n_neg, replace=False)\n",
    "        while (negs == pos).any():\n",
    "            dup = negs == pos\n",
    "            negs[dup] = np.random.choice(ALL_ITEM_IDS, size=dup.sum(), replace=False)\n",
    "        return negs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        hist = torch.tensor(self.inputs[idx], dtype=torch.long)\n",
    "        pos  = torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "        negs = torch.tensor(self._neg_sample(self.targets[idx]), dtype=torch.long)\n",
    "        return hist, pos, negs"
   ],
   "id": "6e56658ac26b07e2",
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "id": "a7aba75c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:22.926426Z",
     "start_time": "2025-06-30T21:03:22.910207Z"
    }
   },
   "source": [
    "# ======== NextItNet 模型 ======== #\n",
    "class DilatedResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    一组 (dilated conv → ReLU → dilated conv) + residual\n",
    "    \"\"\"\n",
    "    def __init__(self, d, dilation, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(d, d, kernel_size=3, padding=dilation, dilation=dilation)\n",
    "        self.conv2 = nn.Conv1d(d, d, kernel_size=3, padding=1, dilation=1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layernorm1 = nn.LayerNorm(d)\n",
    "        self.layernorm2 = nn.LayerNorm(d)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T, D)  → conv 需要 (B, D, T)\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        x = self.layernorm1(x)\n",
    "        x = F.relu(self.conv1(x.transpose(1,2)).transpose(1,2))\n",
    "        x = self.dropout(x)\n",
    "        x = self.layernorm2(x)\n",
    "        x = F.relu(self.conv2(x.transpose(1,2)).transpose(1,2))\n",
    "        x = self.dropout(x)\n",
    "        return x + residual\n",
    "\n",
    "\n",
    "class NextItNet(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_items=N_ITEMS, dim=EMBEDDING_DIM,\n",
    "        cover_embs=None, pad_idx=PAD_IDX,\n",
    "        n_blocks=NUM_BLOCK, dropout=DROPOUT\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # ① ID embedding（可训练）\n",
    "        self.id_emb   = nn.Embedding(n_items, dim, padding_idx=pad_idx)\n",
    "        # ② cover embedding（冻结）\n",
    "        self.cover_emb = nn.Embedding.from_pretrained(cover_embs, freeze=True)\n",
    "\n",
    "        # ③ 将拼接后的 (dim+128) → dim 的 1×1 线性层\n",
    "        self.in_proj = nn.Linear(dim + 128, dim, bias=False)\n",
    "\n",
    "        # ④ Dilated-Residual blocks（保持原参数）\n",
    "        dilations = [1, 2, 4, 8]\n",
    "        blocks = []\n",
    "        for _ in range(n_blocks):\n",
    "            for d_in in dilations:\n",
    "                blocks.append(DilatedResidualBlock(dim, dilation=d_in, dropout=dropout))\n",
    "        self.net = nn.ModuleList(blocks)\n",
    "\n",
    "    def forward(self, seq):                      # seq: (B, T)\n",
    "        id_vec   = self.id_emb(seq)              # (B, T, D)\n",
    "        cover_vec= self.cover_emb(seq)           # (B, T, 128)\n",
    "        x = torch.cat([id_vec, cover_vec], -1)   # (B, T, D+128)\n",
    "        x = self.in_proj(x)                      # (B, T, D) —— 轻量压缩\n",
    "\n",
    "        for block in self.net:                   # Dilated CNN 堆叠\n",
    "            x = block(x)\n",
    "        h = x[:, -1, :]                          # (B, D)\n",
    "        return h\n",
    "\n",
    "    def score(self, h, items):\n",
    "        # item embedding 拼接后投影\n",
    "        item_id_vec = self.id_emb(items)         # (B, D)\n",
    "        item_cover_vec = self.cover_emb(items)   # (B, 128)\n",
    "        item_vec = torch.cat([item_id_vec, item_cover_vec], dim=-1)\n",
    "        item_vec = self.in_proj(item_vec)        # (B, D)\n",
    "\n",
    "        # === 加入 L2 规范化 ===\n",
    "        h = F.normalize(h, p=2, dim=-1)               # (B, D)\n",
    "        item_vec = F.normalize(item_vec, p=2, dim=-1) # (B, D)\n",
    "\n",
    "        return (h.unsqueeze(-2) * item_vec).sum(-1)   # cosine similarity\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "id": "f21704ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:23.012869Z",
     "start_time": "2025-06-30T21:03:23.007407Z"
    }
   },
   "source": [
    "# ======== BPR 损失 ======== #\n",
    "def bpr_loss(pos_s, neg_s):\n",
    "    return -torch.log(torch.sigmoid(pos_s.unsqueeze(-1) - neg_s) + 1e-8).mean()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "id": "07e268c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:23.115071Z",
     "start_time": "2025-06-30T21:03:23.101636Z"
    }
   },
   "source": [
    "# ======== 训练函数 ======== #\n",
    "def train_nextitnet_bpr(train_df, test_df=None):\n",
    "    ds = NextItNetBPRDataset(train_df)\n",
    "    loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                        num_workers=0, pin_memory=True)\n",
    "\n",
    "    model = NextItNet(cover_embs=COVER_EMBS).to(DEVICE)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        total_loss, t0 = 0.0, time.time()\n",
    "        for step, (hist, pos, neg) in enumerate(loader, 1):\n",
    "            hist, pos, neg = hist.to(DEVICE), pos.to(DEVICE), neg.to(DEVICE)\n",
    "            h = model(hist)\n",
    "            pos_s = model.score(h, pos)\n",
    "            neg_s = model.score(h, neg)\n",
    "            loss  = bpr_loss(pos_s, neg_s)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            total_loss += loss.item() * hist.size(0)\n",
    "            if step % 100 == 0 or step == 1:\n",
    "                print(f\"[Epoch {epoch}] step {step}/{len(loader)} | loss {loss.item():.4f}\", flush=True)\n",
    "\n",
    "        print(f\"Epoch {epoch} finished | avg loss {total_loss/len(ds):.4f} | time {time.time()-t0:.1f}s\", flush=True)\n",
    "\n",
    "    return model\n"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:23.223150Z",
     "start_time": "2025-06-30T21:03:23.214137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ======== 构建 (user→历史张量) ======== #\n",
    "def build_hist_tensors(df):\n",
    "    cache = {}\n",
    "    for uid, items in df.groupby('user_id')['item_id']:\n",
    "        seq = items.tolist()[-MAX_SEQ_LEN:]\n",
    "        seq = [PAD_IDX]*(MAX_SEQ_LEN-len(seq)) + seq\n",
    "        cache[uid] = torch.tensor(seq, dtype=torch.long).unsqueeze(0)\n",
    "    return cache"
   ],
   "id": "9bc49db2aee62849",
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "id": "10c90823",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:23.250020Z",
     "start_time": "2025-06-30T21:03:23.237188Z"
    }
   },
   "source": [
    "\n",
    "def evaluate_ranking(\n",
    "        test_df,              # DataFrame, 必含 user_id / item_id\n",
    "        train_df,             # DataFrame, 用来构建用户→已交互物品集合\n",
    "        score_fn,             # callable(users_tensor, items_tensor) → np.array\n",
    "        num_items,            # 物品总数\n",
    "        k=10,                 # Hit@K / NDCG@K\n",
    "        num_neg=100,          # 每个正样本采多少负样本\n",
    "        user_col='user_id',\n",
    "        item_col='item_id',\n",
    "        seed=42\n",
    "    ):\n",
    "    \"\"\"\n",
    "    不依赖具体模型，只要提供 score_fn 就能评估。\n",
    "    score_fn: 接收 (user_tensor, item_tensor) 并返回同长度的 Numpy 分数向量。\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # 用户历史，用于采负样本 & 过滤\n",
    "    train_user_dict = (\n",
    "        train_df.groupby(user_col)[item_col].apply(set).to_dict()\n",
    "    )\n",
    "\n",
    "    hits, ndcgs = [], []\n",
    "\n",
    "    for _, row in test_df.iterrows():\n",
    "        u = int(row[user_col])\n",
    "        pos_item = int(row[item_col])\n",
    "\n",
    "        # ---------- 负采样 ----------\n",
    "        neg_items = set()\n",
    "        while len(neg_items) < num_neg:\n",
    "            neg = random.randint(0, num_items - 1)\n",
    "            if neg not in train_user_dict.get(u, set()) and neg != pos_item:\n",
    "                neg_items.add(neg)\n",
    "\n",
    "        item_candidates = list(neg_items) + [pos_item]\n",
    "\n",
    "        # ---------- 评分 ----------\n",
    "        users_t  = torch.LongTensor([u] * len(item_candidates))\n",
    "        items_t  = torch.LongTensor(item_candidates)\n",
    "        scores   = score_fn(users_t, items_t)        # ← 只依赖 score_fn\n",
    "        rank_idx = np.argsort(scores)[::-1]          # 降序\n",
    "        ranked_items = [item_candidates[i] for i in rank_idx]\n",
    "\n",
    "        # ---------- 指标 ----------\n",
    "        if pos_item in ranked_items[:k]:\n",
    "            hits.append(1)\n",
    "            rank_pos = ranked_items.index(pos_item)\n",
    "            ndcgs.append(1 / np.log2(rank_pos + 2))\n",
    "        else:\n",
    "            hits.append(0)\n",
    "            ndcgs.append(0)\n",
    "\n",
    "    hit_rate = float(np.mean(hits))\n",
    "    ndcg     = float(np.mean(ndcgs))\n",
    "    return hit_rate, ndcg"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "id": "cd29a40c68841c9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:38:54.323273Z",
     "start_time": "2025-06-30T21:03:23.288552Z"
    }
   },
   "source": [
    "    # ------------------ 训练 ------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model  = train_nextitnet_bpr(train_df, test_df)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] step 1/508 | loss 0.6959\n",
      "[Epoch 1] step 100/508 | loss 0.6901\n",
      "[Epoch 1] step 200/508 | loss 0.6749\n",
      "[Epoch 1] step 300/508 | loss 0.6415\n",
      "[Epoch 1] step 400/508 | loss 0.6115\n",
      "[Epoch 1] step 500/508 | loss 0.5787\n",
      "Epoch 1 finished | avg loss 0.6541 | time 211.6s\n",
      "[Epoch 2] step 1/508 | loss 0.5917\n",
      "[Epoch 2] step 100/508 | loss 0.5716\n",
      "[Epoch 2] step 200/508 | loss 0.5491\n",
      "[Epoch 2] step 300/508 | loss 0.5347\n",
      "[Epoch 2] step 400/508 | loss 0.5346\n",
      "[Epoch 2] step 500/508 | loss 0.5471\n",
      "Epoch 2 finished | avg loss 0.5497 | time 213.7s\n",
      "[Epoch 3] step 1/508 | loss 0.5171\n",
      "[Epoch 3] step 100/508 | loss 0.4950\n",
      "[Epoch 3] step 200/508 | loss 0.5053\n",
      "[Epoch 3] step 300/508 | loss 0.5140\n",
      "[Epoch 3] step 400/508 | loss 0.5256\n",
      "[Epoch 3] step 500/508 | loss 0.4976\n",
      "Epoch 3 finished | avg loss 0.5172 | time 208.4s\n",
      "[Epoch 4] step 1/508 | loss 0.5034\n",
      "[Epoch 4] step 100/508 | loss 0.5153\n",
      "[Epoch 4] step 200/508 | loss 0.5231\n",
      "[Epoch 4] step 300/508 | loss 0.4897\n",
      "[Epoch 4] step 400/508 | loss 0.5205\n",
      "[Epoch 4] step 500/508 | loss 0.4809\n",
      "Epoch 4 finished | avg loss 0.5086 | time 217.0s\n",
      "[Epoch 5] step 1/508 | loss 0.5027\n",
      "[Epoch 5] step 100/508 | loss 0.4880\n",
      "[Epoch 5] step 200/508 | loss 0.5017\n",
      "[Epoch 5] step 300/508 | loss 0.5003\n",
      "[Epoch 5] step 400/508 | loss 0.5135\n",
      "[Epoch 5] step 500/508 | loss 0.5226\n",
      "Epoch 5 finished | avg loss 0.5047 | time 209.2s\n",
      "[Epoch 6] step 1/508 | loss 0.4927\n",
      "[Epoch 6] step 100/508 | loss 0.5154\n",
      "[Epoch 6] step 200/508 | loss 0.4940\n",
      "[Epoch 6] step 300/508 | loss 0.4917\n",
      "[Epoch 6] step 400/508 | loss 0.5080\n",
      "[Epoch 6] step 500/508 | loss 0.5038\n",
      "Epoch 6 finished | avg loss 0.5025 | time 216.1s\n",
      "[Epoch 7] step 1/508 | loss 0.5065\n",
      "[Epoch 7] step 100/508 | loss 0.5209\n",
      "[Epoch 7] step 200/508 | loss 0.4805\n",
      "[Epoch 7] step 300/508 | loss 0.5121\n",
      "[Epoch 7] step 400/508 | loss 0.4969\n",
      "[Epoch 7] step 500/508 | loss 0.5230\n",
      "Epoch 7 finished | avg loss 0.5008 | time 220.0s\n",
      "[Epoch 8] step 1/508 | loss 0.5103\n",
      "[Epoch 8] step 100/508 | loss 0.5014\n",
      "[Epoch 8] step 200/508 | loss 0.4975\n",
      "[Epoch 8] step 300/508 | loss 0.5170\n",
      "[Epoch 8] step 400/508 | loss 0.4971\n",
      "[Epoch 8] step 500/508 | loss 0.5008\n",
      "Epoch 8 finished | avg loss 0.5003 | time 219.5s\n",
      "[Epoch 9] step 1/508 | loss 0.4991\n",
      "[Epoch 9] step 100/508 | loss 0.4938\n",
      "[Epoch 9] step 200/508 | loss 0.5055\n",
      "[Epoch 9] step 300/508 | loss 0.4791\n",
      "[Epoch 9] step 400/508 | loss 0.5002\n",
      "[Epoch 9] step 500/508 | loss 0.4878\n",
      "Epoch 9 finished | avg loss 0.4990 | time 216.1s\n",
      "[Epoch 10] step 1/508 | loss 0.4926\n",
      "[Epoch 10] step 100/508 | loss 0.4977\n",
      "[Epoch 10] step 200/508 | loss 0.5042\n",
      "[Epoch 10] step 300/508 | loss 0.5080\n",
      "[Epoch 10] step 400/508 | loss 0.4795\n",
      "[Epoch 10] step 500/508 | loss 0.5096\n",
      "Epoch 10 finished | avg loss 0.4985 | time 194.9s\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "id": "900b523c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:38:54.399051Z",
     "start_time": "2025-06-30T21:38:54.392753Z"
    }
   },
   "source": [
    "def make_popularity_score_fn(train_df, item_col='item_id'):\n",
    "    item_cnt = Counter(train_df[item_col])\n",
    "    default_score = min(item_cnt.values()) - 1  # 给没出现过的物品一个更低分\n",
    "    def _score_fn(users_t, items_t):\n",
    "        return np.array([item_cnt.get(int(i), default_score) for i in items_t])\n",
    "    return _score_fn"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "id": "994e2156",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:38:54.476107Z",
     "start_time": "2025-06-30T21:38:54.469097Z"
    }
   },
   "source": [
    "def random_score_fn(users_t, items_t):\n",
    "    # 随机给每个 items_t 一个分数；users_t 不使用，但必须接收\n",
    "    return np.random.rand(len(items_t))"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "id": "4ef6ceb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:48:29.539870Z",
     "start_time": "2025-06-30T21:48:29.529874Z"
    }
   },
   "source": [
    "def make_score_fn(model, hist_cache):\n",
    "    \"\"\"\n",
    "    评估阶段：user_t 可能是长度 m 的重复用户 id 向量，\n",
    "    items_t 是长度 m 的候选 item 列表。\n",
    "    只取 user_t[0] 来生成单个用户向量，再与全部候选做点积。\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # —— 预生成 “ID+cover→投影→L2” 的 item 表 —— #\n",
    "    with torch.no_grad():\n",
    "        id_mat    = model.id_emb.weight.data          # (n_items+1, D)\n",
    "        cover_mat = model.cover_emb.weight.data       # (n_items+1, 128)\n",
    "        fused_mat = model.in_proj(\n",
    "            torch.cat([id_mat, cover_mat], dim=1)\n",
    "        )\n",
    "        fused_mat = F.normalize(fused_mat, p=2, dim=-1).to(device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def score_fn(user_t, item_t):\n",
    "        \"\"\"\n",
    "        user_t : 1-D ndarray / tensor (长度 m，全是同一个用户 id)\n",
    "        item_t : 1-D tensor  (长度 m，候选 item ids)\n",
    "        return : 1-D numpy  (长度 m，分数)\n",
    "        \"\"\"\n",
    "        # ---- 1. 取该用户的历史序列，算用户向量 ----\n",
    "        uid = int(user_t[0])                  # 只需一个\n",
    "        hist_seq = hist_cache[uid].to(device) # shape (1, T)\n",
    "        h = model(hist_seq)                   # (1, D)\n",
    "        h = F.normalize(h, p=2, dim=-1)       # (1, D)\n",
    "\n",
    "        # ---- 2. 取候选 item 融合后表征 ----\n",
    "        item_vec = fused_mat[item_t.to(device)]   # (m, D)  已归一化\n",
    "\n",
    "        # ---- 3. cosine 相似度 ----\n",
    "        scores = (h * item_vec).sum(-1)           # → (m,)\n",
    "        return scores.cpu().numpy()               # 1-D，长度 = m\n",
    "\n",
    "    return score_fn\n"
   ],
   "outputs": [],
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "id": "bbd4f9d7af8b5c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:57:50.475980Z",
     "start_time": "2025-06-30T21:48:34.509997Z"
    }
   },
   "source": [
    "\n",
    "# ----------------NextItNet （或其他模型）------------\n",
    "score_fn_nextItNet = make_score_fn(model,hist_cache=build_hist_tensors(train_df))\n",
    "hit_nextItNet, ndcg_nextItNet = evaluate_ranking(\n",
    "    test_df, train_df, score_fn_nextItNet,\n",
    "    num_items=num_items, k=10\n",
    ")\n",
    "print(f\"NextItNet   Hit@10={hit_nextItNet:.4f}  NDCG@10={ndcg_nextItNet:.4f}\")\n",
    "\n",
    "# ---------------- baseline：Popular ----------------\n",
    "pop_score_fn  = make_popularity_score_fn(train_df)\n",
    "hit_pop, ndcg_pop = evaluate_ranking(\n",
    "    test_df, train_df, pop_score_fn,\n",
    "    num_items=num_items, k=10\n",
    ")\n",
    "print(f\"Popular  Hit@10={hit_pop:.4f}  NDCG@10={ndcg_pop:.4f}\")\n",
    "\n",
    "# ---------------- baseline：Random -----------------\n",
    "hit_rand, ndcg_rand = evaluate_ranking(\n",
    "    test_df, train_df, random_score_fn,\n",
    "    num_items=num_items, k=10\n",
    ")\n",
    "print(f\"Random   Hit@10={hit_rand:.4f}  NDCG@10={ndcg_rand:.4f}\")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NextItNet   Hit@10=0.2905  NDCG@10=0.1443\n",
      "Popular  Hit@10=0.2528  NDCG@10=0.1262\n",
      "Random   Hit@10=0.0996  NDCG@10=0.0455\n"
     ]
    }
   ],
   "execution_count": 50
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DP1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

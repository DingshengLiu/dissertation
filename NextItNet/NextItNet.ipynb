{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from google.colab import drive\n",
    "import shutil\n",
    "import os\n",
    "def copy_from_drive(src_path, dst_path):\n",
    "\n",
    "    if os.path.exists(dst_path):\n",
    "        print(f\"skip:{dst_path} exists\")\n",
    "        return\n",
    "\n",
    "    if os.path.isdir(src_path):\n",
    "        shutil.copytree(src_path, dst_path)\n",
    "    elif os.path.isfile(src_path):\n",
    "        shutil.copy(src_path, dst_path)\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "copy_from_drive('/content/drive/MyDrive/tool', '/content/tool')\n",
    "copy_from_drive('/content/drive/MyDrive/MicroLens-50k_pairs.csv','/content/MicroLens-50k_pairs.csv')"
   ],
   "id": "a1f5288db4eae3cc"
  },
  {
   "cell_type": "code",
   "id": "b828e981",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T12:37:20.796421Z",
     "start_time": "2025-06-10T12:37:00.216482Z"
    }
   },
   "source": [
    "from collections import Counter\n",
    "\n",
    "import random, math, time, os\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from tool import preprocess\n",
    "from tool import customdataset\n",
    "from tool import evaluate\n",
    "!pip install faiss-cpu\n",
    "import faiss\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "e99a31348e0a553",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T12:37:20.938306Z",
     "start_time": "2025-06-10T12:37:20.811445Z"
    }
   },
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "76a2087d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T12:37:22.218781Z",
     "start_time": "2025-06-10T12:37:22.031787Z"
    }
   },
   "source": [
    "path = 'MicroLens-50k_pairs.csv'\n",
    "user = 'user'\n",
    "item = 'item'\n",
    "user_id = 'user_id'\n",
    "item_id = 'item_id'\n",
    "timestamp = 'timestamp'\n",
    "save_dir = './embeddings'\n",
    "top_k = 10\n",
    "num_workers = 10\n",
    "k_neg = 10\n",
    "# path = pd.read_csv('MicroLens-50k_pairs.csv')"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "6d42c375",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T12:37:22.403492Z",
     "start_time": "2025-06-10T12:37:22.362787Z"
    }
   },
   "source": "dataset_pd,num_users,num_items = preprocess.openAndSort(path,user_id=user,item_id=item,timestamp='timestamp')",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "8b8ae0c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T12:37:22.730043Z",
     "start_time": "2025-06-10T12:37:22.514325Z"
    }
   },
   "source": [
    "train_df, test_df = preprocess.split(dataset_pd,user, item, timestamp)\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Test size: {len(test_df)}\")\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "2d2d4de256fd88a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T12:37:22.921255Z",
     "start_time": "2025-06-10T12:37:22.910158Z"
    }
   },
   "source": [
    "# maintain a map from new id to old id, new id for constructing matrix\n",
    "user2id = {u: i for i, u in enumerate(dataset_pd[user].unique())}\n",
    "item2id = {i: j for j, i in enumerate(dataset_pd[item].unique())}\n",
    "\n",
    "# apply to train_df and test_df\n",
    "train_df[user_id] = train_df[user].map(user2id)\n",
    "train_df[item_id] = train_df[item].map(item2id)\n",
    "test_df[user_id] = test_df[user].map(user2id)\n",
    "test_df[item_id] = test_df[item].map(item2id)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T12:37:24.576697Z",
     "start_time": "2025-06-10T12:37:24.490687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------- 超参数 ----------\n",
    "MAX_SEQ_LEN   = 20          # 输入序列长度\n",
    "EMBEDDING_DIM = 64          # item & user 向量维度\n",
    "NUM_BLOCK     = 3           # 残差层堆叠次数（每次含 dilation=[1,2,4,8]）\n",
    "NEG_SAMPLE    = 5           # 训练时负采样个数\n",
    "BATCH_SIZE    = 1024         # 1650Ti 8G 推荐 512；显存紧张用 256\n",
    "EPOCHS        = 10\n",
    "LR            = 1e-3\n",
    "SEED          = 42\n",
    "DROPOUT       = 0.2\n",
    "# ----------------------------\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# ---------- 常量 ----------\n",
    "PAD_IDX  = num_items              # padding 专用 id，不与真实 item 冲突\n",
    "N_ITEMS  = num_items + 1          # Embedding 行数（含 PAD）\n",
    "\n",
    "\n"
   ],
   "id": "6e56658ac26b07e2",
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "a7aba75c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T12:37:24.595276Z",
     "start_time": "2025-06-10T12:37:24.583711Z"
    }
   },
   "source": [
    "# ======== NextItNet 模型 ======== #\n",
    "class DilatedResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    一组 (dilated conv → ReLU → dilated conv) + residual\n",
    "    \"\"\"\n",
    "    def __init__(self, d, dilation, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(d, d, kernel_size=3, padding=dilation, dilation=dilation)\n",
    "        self.conv2 = nn.Conv1d(d, d, kernel_size=3, padding=1, dilation=1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layernorm1 = nn.LayerNorm(d)\n",
    "        self.layernorm2 = nn.LayerNorm(d)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T, D)  → conv 需要 (B, D, T)\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        x = self.layernorm1(x)\n",
    "        x = F.relu(self.conv1(x.transpose(1,2)).transpose(1,2))\n",
    "        x = self.dropout(x)\n",
    "        x = self.layernorm2(x)\n",
    "        x = F.relu(self.conv2(x.transpose(1,2)).transpose(1,2))\n",
    "        x = self.dropout(x)\n",
    "        return x + residual\n",
    "\n",
    "\n",
    "class NextItNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Embedding → N × [dilated residual blocks] → 取最后位置 hidden → (B, D)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_items=N_ITEMS, embedding_dim=EMBEDDING_DIM, n_blocks=NUM_BLOCK, pad_idx=PAD_IDX):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_items, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        blocks = []\n",
    "        dilations = [1,2,4,8]  # 每轮堆叠 1-2-4-8\n",
    "        for _ in range(n_blocks):\n",
    "            for d in dilations:\n",
    "                blocks.append(DilatedResidualBlock(embedding_dim, dilation=d))\n",
    "        self.net = nn.ModuleList(blocks)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        \"\"\"\n",
    "        seq: (B, T) → 返回用户向量 (B, D)\n",
    "        \"\"\"\n",
    "        x = self.embedding(seq)              # (B, T, D)\n",
    "        for block in self.net:\n",
    "            x = block(x)\n",
    "        h = x[:, -1, :]                      # 最后一个时间步\n",
    "        return h\n",
    "\n",
    "    def get_items_embedding(self,item_ids,l2_norm=True):\n",
    "        i_vec = self.embedding(item_ids)          # (B, emb_dim)\n",
    "        if l2_norm:\n",
    "            i_vec = F.normalize(i_vec, p=2, dim=1)\n",
    "        return i_vec\n",
    "\n",
    "    def save_embeddings(self, num_users, num_items, device, save_dir='./embeddings'):\n",
    "        import os\n",
    "        import faiss\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        self.eval()\n",
    "        self.to(device)\n",
    "\n",
    "        item_ids = torch.arange(num_items, dtype=torch.long, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            item_embeds = self.get_items_embedding(item_ids, l2_norm=True)\n",
    "\n",
    "        item_embeds = item_embeds.cpu().numpy().astype(np.float32)\n",
    "\n",
    "        # 保存向量\n",
    "        np.save(f\"{save_dir}/item_embeddings.npy\", item_embeds)\n",
    "\n",
    "        # 构建 FAISS index（使用内积）\n",
    "        dim = item_embeds.shape[1]\n",
    "        index = faiss.IndexFlatIP(dim)\n",
    "        index.add(item_embeds)\n",
    "\n",
    "        faiss.write_index(index, f\"{save_dir}/item_index.faiss\")\n",
    "        print(\"Saved user/item embeddings and FAISS index.\")\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "07e268c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T12:37:24.714436Z",
     "start_time": "2025-06-10T12:37:24.701177Z"
    }
   },
   "source": [
    "# ======== 训练流程 ======== #\n",
    "def train_model(model, train_df, epochs,lr , batch_size, test_df=None, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "    train_loader  = customdataset.build_seq_loader(train_df, batch_size=batch_size,\n",
    "                         shuffle=True, num_workers=10,pad_idx=PAD_IDX,max_len=MAX_SEQ_LEN,user_id=user,item_id=item_id)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        dt_start = datetime.now()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            hist, pos = batch\n",
    "            hist, pos = hist.to(device), pos.to(device)\n",
    "\n",
    "            # 1. 前向传播（返回预测向量）\n",
    "            predict = model(hist)                      # (B, D)\n",
    "            i_vec = model.get_items_embedding(pos,l2_norm=True)\n",
    "\n",
    "            # 2. 得分矩阵：每个 user 对所有正 item 的打分\n",
    "            logits = torch.matmul(predict, i_vec.T)  # shape: (B, B)\n",
    "\n",
    "            # 3. 构造标签：每个 user 的正确 item 在对角线（即位置 i）\n",
    "            labels = torch.arange(logits.size(0), device=device)  # [0, 1, ..., B-1]\n",
    "\n",
    "            # 4. Cross Entropy Loss\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "            # 5. 反向传播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # 日志\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        dt_end = datetime.now()\n",
    "        dt = dt_end - dt_start\n",
    "\n",
    "        print(f\"[Epoch {epoch:02d}/{epochs}] avg InBatch Softmax Loss = {avg_loss:.4f}, time = {dt.total_seconds():.2f}s\")\n",
    "\n",
    "    return\n"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T12:37:24.763267Z",
     "start_time": "2025-06-10T12:37:24.756672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_hist_matrix(df,\n",
    "                      num_users,\n",
    "                      max_len=MAX_SEQ_LEN,\n",
    "                      pad_idx=PAD_IDX,\n",
    "                      user_col=user_id,\n",
    "                      item_col=item_id):\n",
    "    \"\"\"\n",
    "    返回形状为 (num_users, max_len) 的 LongTensor。\n",
    "    第 i 行是用户 i 的历史序列，左侧 PAD，右对齐。\n",
    "    不存在历史的用户整行都是 pad_idx。\n",
    "    \"\"\"\n",
    "    # 先全部填 PAD\n",
    "    hist = torch.full((num_users, max_len), pad_idx, dtype=torch.long)\n",
    "\n",
    "    # groupby 遍历每个用户已有交互\n",
    "    for uid, items in df.groupby(user_col)[item_col]:\n",
    "        seq = items.to_numpy()[-max_len:]             # 取最近 max_len 条\n",
    "        hist[uid, -len(seq):] = torch.as_tensor(seq, dtype=torch.long)\n",
    "\n",
    "    return hist    # (U, T)\n"
   ],
   "id": "9bc49db2aee62849",
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "cd29a40c68841c9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T13:12:06.514670Z",
     "start_time": "2025-06-10T12:37:24.864122Z"
    }
   },
   "source": [
    "model = NextItNet(n_items=N_ITEMS,\n",
    "                 embedding_dim=EMBEDDING_DIM,\n",
    "                  n_blocks=NUM_BLOCK,\n",
    "                 pad_idx=PAD_IDX)\n",
    "model = model.to(device)\n",
    "train_model(model=model,epochs=10, train_df=train_df,batch_size=1024,lr=LR,test_df=test_df,device=device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] step 1/508 | loss 6.8156\n",
      "[Epoch 1] step 100/508 | loss 4.1198\n",
      "[Epoch 1] step 200/508 | loss 3.6768\n",
      "[Epoch 1] step 300/508 | loss 3.3334\n",
      "[Epoch 1] step 400/508 | loss 3.3411\n",
      "[Epoch 1] step 500/508 | loss 2.8915\n",
      "Epoch 1 finished | avg loss 3.6849 | time 219.5s\n",
      "[Epoch 2] step 1/508 | loss 2.7976\n",
      "[Epoch 2] step 100/508 | loss 2.5741\n",
      "[Epoch 2] step 200/508 | loss 2.3416\n",
      "[Epoch 2] step 300/508 | loss 2.2341\n",
      "[Epoch 2] step 400/508 | loss 2.0840\n",
      "[Epoch 2] step 500/508 | loss 1.9960\n",
      "Epoch 2 finished | avg loss 2.3734 | time 215.2s\n",
      "[Epoch 3] step 1/508 | loss 1.7842\n",
      "[Epoch 3] step 100/508 | loss 1.6016\n",
      "[Epoch 3] step 200/508 | loss 1.5918\n",
      "[Epoch 3] step 300/508 | loss 1.4118\n",
      "[Epoch 3] step 400/508 | loss 1.4312\n",
      "[Epoch 3] step 500/508 | loss 1.4476\n",
      "Epoch 3 finished | avg loss 1.5983 | time 205.0s\n",
      "[Epoch 4] step 1/508 | loss 1.2418\n",
      "[Epoch 4] step 100/508 | loss 1.3542\n",
      "[Epoch 4] step 200/508 | loss 1.4371\n",
      "[Epoch 4] step 300/508 | loss 1.2469\n",
      "[Epoch 4] step 400/508 | loss 1.2142\n",
      "[Epoch 4] step 500/508 | loss 1.1310\n",
      "Epoch 4 finished | avg loss 1.2446 | time 202.3s\n",
      "[Epoch 5] step 1/508 | loss 1.0863\n",
      "[Epoch 5] step 100/508 | loss 1.0665\n",
      "[Epoch 5] step 200/508 | loss 1.0665\n",
      "[Epoch 5] step 300/508 | loss 1.0106\n",
      "[Epoch 5] step 400/508 | loss 1.1778\n",
      "[Epoch 5] step 500/508 | loss 1.1610\n",
      "Epoch 5 finished | avg loss 1.0873 | time 205.1s\n",
      "[Epoch 6] step 1/508 | loss 1.0645\n",
      "[Epoch 6] step 100/508 | loss 1.0832\n",
      "[Epoch 6] step 200/508 | loss 1.0356\n",
      "[Epoch 6] step 300/508 | loss 0.9967\n",
      "[Epoch 6] step 400/508 | loss 1.0204\n",
      "[Epoch 6] step 500/508 | loss 0.9716\n",
      "Epoch 6 finished | avg loss 1.0007 | time 216.2s\n",
      "[Epoch 7] step 1/508 | loss 0.9037\n",
      "[Epoch 7] step 100/508 | loss 0.9236\n",
      "[Epoch 7] step 200/508 | loss 0.8517\n",
      "[Epoch 7] step 300/508 | loss 0.8628\n",
      "[Epoch 7] step 400/508 | loss 0.9201\n",
      "[Epoch 7] step 500/508 | loss 0.8910\n",
      "Epoch 7 finished | avg loss 0.9300 | time 214.3s\n",
      "[Epoch 8] step 1/508 | loss 0.8793\n",
      "[Epoch 8] step 100/508 | loss 0.8192\n",
      "[Epoch 8] step 200/508 | loss 0.9541\n",
      "[Epoch 8] step 300/508 | loss 0.8781\n",
      "[Epoch 8] step 400/508 | loss 0.8090\n",
      "[Epoch 8] step 500/508 | loss 0.8485\n",
      "Epoch 8 finished | avg loss 0.8780 | time 198.3s\n",
      "[Epoch 9] step 1/508 | loss 0.8175\n",
      "[Epoch 9] step 100/508 | loss 0.7629\n",
      "[Epoch 9] step 200/508 | loss 0.7751\n",
      "[Epoch 9] step 300/508 | loss 0.7678\n",
      "[Epoch 9] step 400/508 | loss 0.8434\n",
      "[Epoch 9] step 500/508 | loss 0.8457\n",
      "Epoch 9 finished | avg loss 0.8429 | time 197.1s\n",
      "[Epoch 10] step 1/508 | loss 0.7303\n",
      "[Epoch 10] step 100/508 | loss 0.8389\n",
      "[Epoch 10] step 200/508 | loss 0.8133\n",
      "[Epoch 10] step 300/508 | loss 0.8310\n",
      "[Epoch 10] step 400/508 | loss 0.8211\n",
      "[Epoch 10] step 500/508 | loss 0.8220\n",
      "Epoch 10 finished | avg loss 0.8188 | time 199.3s\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "900b523c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T13:12:06.686635Z",
     "start_time": "2025-06-10T13:12:06.677115Z"
    }
   },
   "source": "model.save_embeddings(num_users=num_users,num_items=num_items,device=device,save_dir=save_dir)\n",
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "994e2156",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T13:12:06.701513Z",
     "start_time": "2025-06-10T13:12:06.696128Z"
    }
   },
   "source": [
    "test_loader = customdataset.build_test_loader(test_df, num_items ,user_col = user_id, item_col = item_id, batch_size=1024, num_workers=num_workers)\n",
    "item_pool = list(range(num_items))\n",
    "faiss_index = faiss.read_index(f\"{save_dir}/item_index.faiss\")\n",
    "hist_tensors = build_hist_matrix(train_df, max_len=MAX_SEQ_LEN, pad_idx=PAD_IDX,num_users=num_users).to(device)"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "4ef6ceb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T13:12:06.746547Z",
     "start_time": "2025-06-10T13:12:06.737305Z"
    }
   },
   "source": [
    "hr_r, ndcg_r = evaluate.evaluate_random(test_loader, item_pool ,top_k=top_k)\n",
    "print(f\"Random HR@{top_k} = {hr_r:.4f}, nDCG@{top_k} = {ndcg_r:.4f}\")\n",
    "hr_p, ndcg_p = evaluate.evaluate_popular(test_loader, train_df,top_k=top_k)\n",
    "print(f\"Popular HR@{top_k} = {hr_p:.4f}, nDCG@{top_k} = {ndcg_p:.4f}\")\n",
    "hr_m, ndcg_m = evaluate.evaluate_seq_model(test_loader, model, faiss_index, device,top_k=top_k,hist_tensors=hist_tensors)\n",
    "print(f\"Model   HR@{top_k} = {hr_m:.4f}, nDCG@{top_k} = {ndcg_m:.4f}\")"
   ],
   "outputs": [],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DP1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "metadata": {
        "id": "a1f5288db4eae3cc",
        "outputId": "570c86b6-7d61-4e4d-9b50-c3dc2b6032d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "execution_count": 1,
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "def copy_from_drive(src_path, dst_path):\n",
        "\n",
        "    if os.path.exists(dst_path):\n",
        "        print(f\"skip:{dst_path} exists\")\n",
        "        return\n",
        "\n",
        "    if os.path.isdir(src_path):\n",
        "        shutil.copytree(src_path, dst_path)\n",
        "    elif os.path.isfile(src_path):\n",
        "        shutil.copy(src_path, dst_path)\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "copy_from_drive('/content/drive/MyDrive/tool', '/content/tool')\n",
        "copy_from_drive('/content/drive/MyDrive/MicroLens-50k_pairs.csv','/content/MicroLens-50k_pairs.csv')"
      ],
      "id": "a1f5288db4eae3cc"
    },
    {
      "cell_type": "code",
      "id": "b828e981",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-06-10T12:37:20.796421Z",
          "start_time": "2025-06-10T12:37:00.216482Z"
        },
        "id": "b828e981",
        "outputId": "9d8178ec-bf0c-4b04-c4fc-ac78bd78810f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install faiss-cpu\n",
        "!pip install lmdb\n",
        "import random, os\n",
        "from tool import preprocess\n",
        "from tool import customdataset\n",
        "from tool import evaluate\n",
        "import faiss\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from datetime import datetime\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0.post1\n",
            "Collecting lmdb\n",
            "  Downloading lmdb-1.7.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Downloading lmdb-1.7.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.6/299.6 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lmdb\n",
            "Successfully installed lmdb-1.7.2\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "c2228929b816cb1e",
        "outputId": "320253b1-53a3-4a12-d1a2-ccca535bf77b"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function tool.preprocess.set_seed.<locals>.seed_worker(worker_id)>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>tool.preprocess.set_seed.&lt;locals&gt;.seed_worker</b><br/>def seed_worker(worker_id)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/tool/preprocess.py</a>&lt;no docstring&gt;</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 18);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "execution_count": 3,
      "source": [
        "preprocess.set_seed(42)"
      ],
      "id": "c2228929b816cb1e"
    },
    {
      "cell_type": "code",
      "id": "e99a31348e0a553",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-06-10T12:37:20.938306Z",
          "start_time": "2025-06-10T12:37:20.811445Z"
        },
        "id": "e99a31348e0a553"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "id": "76a2087d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-06-10T12:37:22.218781Z",
          "start_time": "2025-06-10T12:37:22.031787Z"
        },
        "id": "76a2087d"
      },
      "source": [
        "path = 'MicroLens-50k_pairs.csv'\n",
        "user = 'user'\n",
        "item = 'item'\n",
        "user_id = 'user_id'\n",
        "item_id = 'item_id'\n",
        "timestamp = 'timestamp'\n",
        "save_dir = './embeddings'\n",
        "top_k = 10\n",
        "num_workers = 10\n",
        "k_neg = 10\n",
        "# path = pd.read_csv('MicroLens-50k_pairs.csv')"
      ],
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "id": "6d42c375",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-06-10T12:37:22.403492Z",
          "start_time": "2025-06-10T12:37:22.362787Z"
        },
        "id": "6d42c375",
        "outputId": "49b1f10e-0685-4e86-dbf0-fe2cc029b3d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dataset_pd,num_users,num_items = preprocess.openAndSort(path,user_id=user,item_id=item,timestamp='timestamp')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset base information：\n",
            "- number of users：50000\n",
            "- number of items：19220\n",
            "- number of rows：359708\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "id": "8b8ae0c7",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-06-10T12:37:22.730043Z",
          "start_time": "2025-06-10T12:37:22.514325Z"
        },
        "id": "8b8ae0c7",
        "outputId": "9826b7ee-029d-40ef-c80c-048c33e93195",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_df, test_df = preprocess.split(dataset_pd,user, item, timestamp)\n",
        "print(f\"Train size: {len(train_df)}\")\n",
        "print(f\"Test size: {len(test_df)}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 309708\n",
            "Test size: 49424\n"
          ]
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "id": "2d2d4de256fd88a5",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-06-10T12:37:22.921255Z",
          "start_time": "2025-06-10T12:37:22.910158Z"
        },
        "id": "2d2d4de256fd88a5"
      },
      "source": [
        "# maintain a map from new id to old id, new id for constructing matrix\n",
        "user2id = {u: i for i, u in enumerate(dataset_pd[user].unique())}\n",
        "item2id = {i: j for j, i in enumerate(dataset_pd[item].unique())}\n",
        "\n",
        "# apply to train_df and test_df\n",
        "train_df[user_id] = train_df[user].map(user2id)\n",
        "train_df[item_id] = train_df[item].map(item2id)\n",
        "test_df[user_id] = test_df[user].map(user2id)\n",
        "test_df[item_id] = test_df[item].map(item2id)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 8
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-06-10T12:37:24.576697Z",
          "start_time": "2025-06-10T12:37:24.490687Z"
        },
        "id": "6e56658ac26b07e2"
      },
      "cell_type": "code",
      "source": [
        "# ---------- 超参数 ----------\n",
        "MAX_SEQ_LEN   = 10          # 输入序列长度\n",
        "EMBEDDING_DIM = 64          # item & user 向量维度\n",
        "NUM_BLOCK     = 3           # 残差层堆叠次数（每次含 dilation=[1,2,4,8]）\n",
        "NEG_SAMPLE    = 5           # 训练时负采样个数\n",
        "BATCH_SIZE    = 1024         # 1650Ti 8G 推荐 512；显存紧张用 256\n",
        "EPOCHS        = 10\n",
        "LR            = 1e-3\n",
        "SEED          = 42\n",
        "DROPOUT       = 0.2\n",
        "L2_NORM = False\n",
        "# ----------------------------\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# ---------- 常量 ----------\n",
        "PAD_IDX  = num_items              # padding 专用 id，不与真实 item 冲突\n",
        "N_ITEMS  = num_items + 1          # Embedding 行数（含 PAD）\n",
        "\n",
        "\n"
      ],
      "id": "6e56658ac26b07e2",
      "outputs": [],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "id": "a7aba75c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-06-10T12:37:24.595276Z",
          "start_time": "2025-06-10T12:37:24.583711Z"
        },
        "id": "a7aba75c"
      },
      "source": [
        "# ======== NextItNet 模型 ======== #\n",
        "class DepthwiseResidualBlock(nn.Module):\n",
        "    def __init__(self, d, dropout=DROPOUT):\n",
        "        super().__init__()\n",
        "        self.layernorm1 = nn.LayerNorm(d)\n",
        "        self.depthwise = nn.Conv1d(d, d, kernel_size=3, padding=1, groups=d)\n",
        "        self.pointwise = nn.Conv1d(d, d, kernel_size=1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layernorm2 = nn.LayerNorm(d)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.layernorm1(x)\n",
        "        x = self.depthwise(x.transpose(1, 2))\n",
        "        x = self.pointwise(x).transpose(1, 2)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layernorm2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x + residual\n",
        "\n",
        "class NextItNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Embedding → N × [dilated residual blocks] → 取最后位置 hidden → (B, D)\n",
        "    \"\"\"\n",
        "    def __init__(self, n_items=N_ITEMS, embedding_dim=EMBEDDING_DIM, n_blocks=NUM_BLOCK, pad_idx=PAD_IDX):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(n_items, embedding_dim, padding_idx=pad_idx)\n",
        "\n",
        "        blocks = []\n",
        "        dilations = [1,2,4,8]  # 每轮堆叠 1-2-4-8\n",
        "        for _ in range(n_blocks):\n",
        "            for d in dilations:\n",
        "                blocks.append(DepthwiseResidualBlock(embedding_dim))\n",
        "        self.net = nn.ModuleList(blocks)\n",
        "\n",
        "    def forward(self, seq):\n",
        "        \"\"\"\n",
        "        seq: (B, T) → 返回用户向量 (B, D)\n",
        "        \"\"\"\n",
        "        x = self.embedding(seq)              # (B, T, D)\n",
        "        for block in self.net:\n",
        "            x = block(x)\n",
        "        h = x[:, -1, :]                      # 最后一个时间步\n",
        "        if L2_NORM:\n",
        "            h = F.normalize(h, p=2, dim=1)\n",
        "        return h\n",
        "\n",
        "    def get_items_embedding(self,item_ids,l2_norm=False):\n",
        "        i_vec = self.embedding(item_ids)          # (B, emb_dim)\n",
        "        if l2_norm:\n",
        "            i_vec = F.normalize(i_vec, p=2, dim=1)\n",
        "        return i_vec\n",
        "\n",
        "    def save_embeddings(self, num_users, num_items, device, save_dir='./embeddings',l2_norm=L2_NORM):\n",
        "        import os\n",
        "        import faiss\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        self.eval()\n",
        "        self.to(device)\n",
        "\n",
        "        item_ids = torch.arange(num_items, dtype=torch.long, device=device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            item_embeds = self.get_items_embedding(item_ids, l2_norm=l2_norm)\n",
        "\n",
        "        item_embeds = item_embeds.cpu().numpy().astype(np.float32)\n",
        "\n",
        "        # 保存向量\n",
        "        np.save(f\"{save_dir}/item_embeddings.npy\", item_embeds)\n",
        "\n",
        "        # 构建 FAISS index（使用内积）\n",
        "        dim = item_embeds.shape[1]\n",
        "        index = faiss.IndexFlatIP(dim)\n",
        "        index.add(item_embeds)\n",
        "\n",
        "        faiss.write_index(index, f\"{save_dir}/item_index.faiss\")\n",
        "        print(\"Saved user/item embeddings and FAISS index.\")\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "id": "07e268c2",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-06-10T12:37:24.714436Z",
          "start_time": "2025-06-10T12:37:24.701177Z"
        },
        "id": "07e268c2"
      },
      "source": [
        "# ======== 训练流程 ======== #\n",
        "def train_model(model, train_df, epochs,lr , batch_size, test_df=None, device=None):\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "    train_loader  = customdataset.build_seq_loader(train_df, batch_size=batch_size,\n",
        "                         shuffle=True, num_workers=10,pad_idx=PAD_IDX,max_len=MAX_SEQ_LEN,user_id=user_id,item_id=item_id)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        dt_start = datetime.now()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            hist, pos = batch\n",
        "            hist, pos = hist.to(device), pos.to(device)\n",
        "\n",
        "            # 1. 前向传播（返回预测向量）\n",
        "            predict = model(hist)                      # (B, D)\n",
        "            i_vec = model.get_items_embedding(pos,l2_norm=L2_NORM)\n",
        "\n",
        "            # 2. 得分矩阵：每个 user 对所有正 item 的打分\n",
        "            logits = torch.matmul(predict, i_vec.T)  # shape: (B, B)\n",
        "\n",
        "            # 3. 构造标签：每个 user 的正确 item 在对角线（即位置 i）\n",
        "            labels = torch.arange(logits.size(0), device=device)  # [0, 1, ..., B-1]\n",
        "\n",
        "            # 4. Cross Entropy Loss\n",
        "            loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "            # 5. 反向传播\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        # 日志\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        dt_end = datetime.now()\n",
        "        dt = dt_end - dt_start\n",
        "\n",
        "        print(f\"[Epoch {epoch:02d}/{epochs}] avg InBatch Softmax Loss = {avg_loss:.4f}, time = {dt.total_seconds():.2f}s\")\n",
        "\n",
        "    return\n"
      ],
      "outputs": [],
      "execution_count": 11
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-06-10T12:37:24.763267Z",
          "start_time": "2025-06-10T12:37:24.756672Z"
        },
        "id": "9bc49db2aee62849"
      },
      "cell_type": "code",
      "source": [
        "def build_hist_matrix(df,\n",
        "                      num_users,\n",
        "                      max_len=MAX_SEQ_LEN,\n",
        "                      pad_idx=PAD_IDX,\n",
        "                      user_col=user_id,\n",
        "                      item_col=item_id):\n",
        "    \"\"\"\n",
        "    返回形状为 (num_users, max_len) 的 LongTensor。\n",
        "    第 i 行是用户 i 的历史序列，左侧 PAD，右对齐。\n",
        "    不存在历史的用户整行都是 pad_idx。\n",
        "    \"\"\"\n",
        "    # 先全部填 PAD\n",
        "    hist = torch.full((num_users, max_len), pad_idx, dtype=torch.long)\n",
        "\n",
        "    # groupby 遍历每个用户已有交互\n",
        "    for uid, items in df.groupby(user_col)[item_col]:\n",
        "        seq = items.to_numpy()[-max_len:]             # 取最近 max_len 条\n",
        "        hist[uid, -len(seq):] = torch.as_tensor(seq, dtype=torch.long)\n",
        "\n",
        "    return hist    # (U, T)\n"
      ],
      "id": "9bc49db2aee62849",
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "id": "cd29a40c68841c9c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-06-10T13:12:06.514670Z",
          "start_time": "2025-06-10T12:37:24.864122Z"
        },
        "id": "cd29a40c68841c9c",
        "outputId": "1880786d-7224-4011-8d55-ca7c2f4d2b8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = NextItNet(n_items=N_ITEMS,\n",
        "                 embedding_dim=EMBEDDING_DIM,\n",
        "                  n_blocks=NUM_BLOCK,\n",
        "                 pad_idx=PAD_IDX)\n",
        "model = model.to(device)\n",
        "train_model(model=model,epochs=100, train_df=train_df,batch_size=1024,lr=LR,test_df=test_df,device=device)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 01/100] avg InBatch Softmax Loss = 55.7652, time = 6.06s\n",
            "[Epoch 02/100] avg InBatch Softmax Loss = 22.8886, time = 4.31s\n",
            "[Epoch 03/100] avg InBatch Softmax Loss = 14.4805, time = 4.14s\n",
            "[Epoch 04/100] avg InBatch Softmax Loss = 11.6011, time = 4.42s\n",
            "[Epoch 05/100] avg InBatch Softmax Loss = 10.2345, time = 4.27s\n",
            "[Epoch 06/100] avg InBatch Softmax Loss = 9.5245, time = 4.30s\n",
            "[Epoch 07/100] avg InBatch Softmax Loss = 9.1170, time = 4.12s\n",
            "[Epoch 08/100] avg InBatch Softmax Loss = 8.8117, time = 4.26s\n",
            "[Epoch 09/100] avg InBatch Softmax Loss = 8.6118, time = 4.33s\n",
            "[Epoch 10/100] avg InBatch Softmax Loss = 8.4628, time = 4.07s\n",
            "[Epoch 11/100] avg InBatch Softmax Loss = 8.3292, time = 4.17s\n",
            "[Epoch 12/100] avg InBatch Softmax Loss = 8.2195, time = 4.32s\n",
            "[Epoch 13/100] avg InBatch Softmax Loss = 8.1231, time = 4.27s\n",
            "[Epoch 14/100] avg InBatch Softmax Loss = 8.0309, time = 4.18s\n",
            "[Epoch 15/100] avg InBatch Softmax Loss = 7.9477, time = 4.39s\n",
            "[Epoch 16/100] avg InBatch Softmax Loss = 7.8579, time = 4.26s\n",
            "[Epoch 17/100] avg InBatch Softmax Loss = 7.7732, time = 4.22s\n",
            "[Epoch 18/100] avg InBatch Softmax Loss = 7.7034, time = 4.11s\n",
            "[Epoch 19/100] avg InBatch Softmax Loss = 7.6280, time = 4.13s\n",
            "[Epoch 20/100] avg InBatch Softmax Loss = 7.5621, time = 4.23s\n",
            "[Epoch 21/100] avg InBatch Softmax Loss = 7.4986, time = 4.32s\n",
            "[Epoch 22/100] avg InBatch Softmax Loss = 7.4392, time = 4.22s\n",
            "[Epoch 23/100] avg InBatch Softmax Loss = 7.3869, time = 4.28s\n",
            "[Epoch 24/100] avg InBatch Softmax Loss = 7.3327, time = 4.36s\n",
            "[Epoch 25/100] avg InBatch Softmax Loss = 7.2772, time = 4.34s\n",
            "[Epoch 26/100] avg InBatch Softmax Loss = 7.2295, time = 4.26s\n",
            "[Epoch 27/100] avg InBatch Softmax Loss = 7.1799, time = 4.25s\n",
            "[Epoch 28/100] avg InBatch Softmax Loss = 7.1282, time = 4.08s\n",
            "[Epoch 29/100] avg InBatch Softmax Loss = 7.0785, time = 4.46s\n",
            "[Epoch 30/100] avg InBatch Softmax Loss = 7.0270, time = 4.12s\n",
            "[Epoch 31/100] avg InBatch Softmax Loss = 6.9766, time = 4.28s\n",
            "[Epoch 32/100] avg InBatch Softmax Loss = 6.9215, time = 4.24s\n",
            "[Epoch 33/100] avg InBatch Softmax Loss = 6.8735, time = 4.54s\n",
            "[Epoch 34/100] avg InBatch Softmax Loss = 6.8236, time = 4.20s\n",
            "[Epoch 35/100] avg InBatch Softmax Loss = 6.7730, time = 4.37s\n",
            "[Epoch 36/100] avg InBatch Softmax Loss = 6.7214, time = 4.35s\n",
            "[Epoch 37/100] avg InBatch Softmax Loss = 6.6744, time = 4.52s\n",
            "[Epoch 38/100] avg InBatch Softmax Loss = 6.6258, time = 4.27s\n",
            "[Epoch 39/100] avg InBatch Softmax Loss = 6.5787, time = 4.10s\n",
            "[Epoch 40/100] avg InBatch Softmax Loss = 6.5307, time = 4.11s\n",
            "[Epoch 41/100] avg InBatch Softmax Loss = 6.4899, time = 4.33s\n",
            "[Epoch 42/100] avg InBatch Softmax Loss = 6.4439, time = 4.25s\n",
            "[Epoch 43/100] avg InBatch Softmax Loss = 6.4000, time = 4.16s\n",
            "[Epoch 44/100] avg InBatch Softmax Loss = 6.3589, time = 4.37s\n",
            "[Epoch 45/100] avg InBatch Softmax Loss = 6.3167, time = 4.55s\n",
            "[Epoch 46/100] avg InBatch Softmax Loss = 6.2724, time = 4.50s\n",
            "[Epoch 47/100] avg InBatch Softmax Loss = 6.2304, time = 4.40s\n",
            "[Epoch 48/100] avg InBatch Softmax Loss = 6.1843, time = 4.29s\n",
            "[Epoch 49/100] avg InBatch Softmax Loss = 6.1402, time = 4.38s\n",
            "[Epoch 50/100] avg InBatch Softmax Loss = 6.0988, time = 4.48s\n",
            "[Epoch 51/100] avg InBatch Softmax Loss = 6.0516, time = 4.21s\n",
            "[Epoch 52/100] avg InBatch Softmax Loss = 6.0047, time = 4.34s\n",
            "[Epoch 53/100] avg InBatch Softmax Loss = 5.9573, time = 4.36s\n",
            "[Epoch 54/100] avg InBatch Softmax Loss = 5.9091, time = 4.29s\n",
            "[Epoch 55/100] avg InBatch Softmax Loss = 5.8631, time = 4.28s\n",
            "[Epoch 56/100] avg InBatch Softmax Loss = 5.8156, time = 4.36s\n",
            "[Epoch 57/100] avg InBatch Softmax Loss = 5.7675, time = 4.27s\n",
            "[Epoch 58/100] avg InBatch Softmax Loss = 5.7178, time = 4.37s\n",
            "[Epoch 59/100] avg InBatch Softmax Loss = 5.6731, time = 4.28s\n",
            "[Epoch 60/100] avg InBatch Softmax Loss = 5.6210, time = 4.19s\n",
            "[Epoch 61/100] avg InBatch Softmax Loss = 5.5744, time = 4.37s\n",
            "[Epoch 62/100] avg InBatch Softmax Loss = 5.5265, time = 4.26s\n",
            "[Epoch 63/100] avg InBatch Softmax Loss = 5.4782, time = 4.32s\n",
            "[Epoch 64/100] avg InBatch Softmax Loss = 5.4337, time = 4.36s\n",
            "[Epoch 65/100] avg InBatch Softmax Loss = 5.3863, time = 4.28s\n",
            "[Epoch 66/100] avg InBatch Softmax Loss = 5.3432, time = 4.48s\n",
            "[Epoch 67/100] avg InBatch Softmax Loss = 5.2999, time = 4.25s\n",
            "[Epoch 68/100] avg InBatch Softmax Loss = 5.2586, time = 4.39s\n",
            "[Epoch 69/100] avg InBatch Softmax Loss = 5.2147, time = 4.40s\n",
            "[Epoch 70/100] avg InBatch Softmax Loss = 5.1720, time = 4.44s\n",
            "[Epoch 71/100] avg InBatch Softmax Loss = 5.1317, time = 4.36s\n",
            "[Epoch 72/100] avg InBatch Softmax Loss = 5.0923, time = 4.30s\n",
            "[Epoch 73/100] avg InBatch Softmax Loss = 5.0535, time = 4.34s\n",
            "[Epoch 74/100] avg InBatch Softmax Loss = 5.0172, time = 4.56s\n",
            "[Epoch 75/100] avg InBatch Softmax Loss = 4.9780, time = 4.34s\n",
            "[Epoch 76/100] avg InBatch Softmax Loss = 4.9422, time = 4.39s\n",
            "[Epoch 77/100] avg InBatch Softmax Loss = 4.9039, time = 4.39s\n",
            "[Epoch 78/100] avg InBatch Softmax Loss = 4.8688, time = 4.41s\n",
            "[Epoch 79/100] avg InBatch Softmax Loss = 4.8346, time = 4.16s\n",
            "[Epoch 80/100] avg InBatch Softmax Loss = 4.8008, time = 4.07s\n",
            "[Epoch 81/100] avg InBatch Softmax Loss = 4.7639, time = 4.37s\n",
            "[Epoch 82/100] avg InBatch Softmax Loss = 4.7338, time = 4.40s\n",
            "[Epoch 83/100] avg InBatch Softmax Loss = 4.7062, time = 4.34s\n",
            "[Epoch 84/100] avg InBatch Softmax Loss = 4.6736, time = 4.20s\n",
            "[Epoch 85/100] avg InBatch Softmax Loss = 4.6395, time = 4.29s\n",
            "[Epoch 86/100] avg InBatch Softmax Loss = 4.6128, time = 4.47s\n",
            "[Epoch 87/100] avg InBatch Softmax Loss = 4.5844, time = 4.42s\n",
            "[Epoch 88/100] avg InBatch Softmax Loss = 4.5553, time = 4.20s\n",
            "[Epoch 89/100] avg InBatch Softmax Loss = 4.5292, time = 4.37s\n",
            "[Epoch 90/100] avg InBatch Softmax Loss = 4.5012, time = 4.49s\n",
            "[Epoch 91/100] avg InBatch Softmax Loss = 4.4775, time = 4.47s\n",
            "[Epoch 92/100] avg InBatch Softmax Loss = 4.4528, time = 4.27s\n",
            "[Epoch 93/100] avg InBatch Softmax Loss = 4.4294, time = 4.53s\n",
            "[Epoch 94/100] avg InBatch Softmax Loss = 4.4045, time = 4.32s\n",
            "[Epoch 95/100] avg InBatch Softmax Loss = 4.3851, time = 4.49s\n",
            "[Epoch 96/100] avg InBatch Softmax Loss = 4.3598, time = 4.24s\n",
            "[Epoch 97/100] avg InBatch Softmax Loss = 4.3388, time = 4.60s\n",
            "[Epoch 98/100] avg InBatch Softmax Loss = 4.3185, time = 4.53s\n",
            "[Epoch 99/100] avg InBatch Softmax Loss = 4.2984, time = 4.32s\n",
            "[Epoch 100/100] avg InBatch Softmax Loss = 4.2772, time = 4.10s\n"
          ]
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "id": "900b523c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-06-10T13:12:06.686635Z",
          "start_time": "2025-06-10T13:12:06.677115Z"
        },
        "id": "900b523c",
        "outputId": "b3ab91dc-c0c3-4062-cfcb-c2c10e12ae3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.save_embeddings(num_users=num_users,num_items=num_items,device=device,save_dir=save_dir)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved user/item embeddings and FAISS index.\n"
          ]
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "id": "994e2156",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-06-10T13:12:06.701513Z",
          "start_time": "2025-06-10T13:12:06.696128Z"
        },
        "id": "994e2156"
      },
      "source": [
        "test_loader = customdataset.build_test_loader(test_df, num_items ,user_col = user_id, item_col = item_id, batch_size=1024, num_workers=num_workers)\n",
        "item_pool = list(range(num_items))\n",
        "faiss_index = faiss.read_index(f\"{save_dir}/item_index.faiss\")\n",
        "hist_tensors = build_hist_matrix(train_df, max_len=MAX_SEQ_LEN, pad_idx=PAD_IDX,num_users=num_users).to(device)"
      ],
      "outputs": [],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "id": "4ef6ceb2",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-06-10T13:12:06.746547Z",
          "start_time": "2025-06-10T13:12:06.737305Z"
        },
        "id": "4ef6ceb2",
        "outputId": "ed1fd7f9-233b-48ed-bd86-e75f5d6cbcc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "hr_r, ndcg_r = evaluate.evaluate_random(test_loader, item_pool ,top_k=top_k)\n",
        "print(f\"Random HR@{top_k} = {hr_r:.4f}, nDCG@{top_k} = {ndcg_r:.4f}\")\n",
        "hr_p, ndcg_p = evaluate.evaluate_popular(test_loader, train_df,top_k=top_k)\n",
        "print(f\"Popular HR@{top_k} = {hr_p:.4f}, nDCG@{top_k} = {ndcg_p:.4f}\")\n",
        "hr_m, ndcg_m = evaluate.evaluate_seq_model(test_loader, model, faiss_index, device,top_k=top_k,hist_tensors=hist_tensors,l2_norm=L2_NORM)\n",
        "print(f\"Model   HR@{top_k} = {hr_m:.4f}, nDCG@{top_k} = {ndcg_m:.4f}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random HR@10 = 0.0003, nDCG@10 = 0.0001\n",
            "Popular HR@10 = 0.0029, nDCG@10 = 0.0014\n",
            "Model   HR@10 = 0.0495, nDCG@10 = 0.0238\n"
          ]
        }
      ],
      "execution_count": 16
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
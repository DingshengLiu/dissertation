{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from tkinter.constants import HIDDEN\n",
    "\n",
    "from google.colab import drive\n",
    "import shutil\n",
    "import os\n",
    "def copy_from_drive(src_path, dst_path):\n",
    "\n",
    "    if os.path.exists(dst_path):\n",
    "        print(f\"skip:{dst_path} exists\")\n",
    "        return\n",
    "\n",
    "    if os.path.isdir(src_path):\n",
    "        shutil.copytree(src_path, dst_path)\n",
    "    elif os.path.isfile(src_path):\n",
    "        shutil.copy(src_path, dst_path)\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "copy_from_drive('/content/drive/MyDrive/tool', '/content/tool')\n",
    "copy_from_drive('/content/drive/MyDrive/sbr_data_1M.csv','/content/sbr_data_1M.csv')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S3TTppBdUSal",
    "outputId": "fd793df7-858f-45ae-b6bf-3c806813993f"
   },
   "id": "S3TTppBdUSal",
   "execution_count": 113,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "skip:/content/tool exists\n",
      "skip:/content/MicroLens-50k_pairs.csv exists\n"
     ]
    }
   ]
  },
  {
   "metadata": {
    "id": "2cf7762f790b03a9",
    "outputId": "ef1c4274-4498-4d4b-c8f4-94ac346fadba",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: lmdb in /usr/local/lib/python3.11/dist-packages (1.7.3)\n"
     ]
    }
   ],
   "execution_count": 114,
   "source": [
    "!pip install faiss-cpu\n",
    "!pip install lmdb\n",
    "from tool import preprocess\n",
    "from tool import customdataset\n",
    "from tool import evaluate\n",
    "import faiss\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime"
   ],
   "id": "2cf7762f790b03a9"
  },
  {
   "metadata": {
    "id": "79b35d503cf0a9e2",
    "outputId": "45b13ace-1f68-4039-c749-9680839fe45f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function tool.preprocess.set_seed.<locals>.seed_worker(worker_id)>"
      ],
      "text/html": [
       "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
       "      pre.function-repr-contents {\n",
       "        overflow-x: auto;\n",
       "        padding: 8px 12px;\n",
       "        max-height: 500px;\n",
       "      }\n",
       "\n",
       "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
       "        cursor: pointer;\n",
       "        max-height: 100px;\n",
       "      }\n",
       "    </style>\n",
       "    <pre style=\"white-space: initial; background:\n",
       "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
       "         border-bottom: 1px solid var(--colab-border-color);\"><b>tool.preprocess.set_seed.&lt;locals&gt;.seed_worker</b><br/>def seed_worker(worker_id)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/tool/preprocess.py</a>&lt;no docstring&gt;</pre>\n",
       "      <script>\n",
       "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
       "        for (const element of document.querySelectorAll('.filepath')) {\n",
       "          element.style.display = 'block'\n",
       "          element.onclick = (event) => {\n",
       "            event.preventDefault();\n",
       "            event.stopPropagation();\n",
       "            google.colab.files.view(element.textContent, 18);\n",
       "          };\n",
       "        }\n",
       "      }\n",
       "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
       "        element.onclick = (event) => {\n",
       "          event.preventDefault();\n",
       "          event.stopPropagation();\n",
       "          element.classList.toggle('function-repr-contents-collapsed');\n",
       "        };\n",
       "      }\n",
       "      </script>\n",
       "      </div>"
      ]
     },
     "metadata": {},
     "execution_count": 115
    }
   ],
   "execution_count": 115,
   "source": [
    "preprocess.set_seed(42)"
   ],
   "id": "79b35d503cf0a9e2"
  },
  {
   "metadata": {
    "id": "d97293349e89d19f"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 116,
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "id": "d97293349e89d19f"
  },
  {
   "metadata": {
    "id": "7577a987b7a2c3c0"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 117,
   "source": [
    "path = 'sbr_data_1M.csv'\n",
    "user = 'user_id'\n",
    "item = 'item_id'\n",
    "user_id = 'user_nid'\n",
    "item_id = 'item_nid'\n",
    "timestamp = 'timestamp'\n",
    "save_dir = './embeddings'\n",
    "top_k = 10\n",
    "num_workers = 10\n",
    "k_neg = 10\n",
    "L2_NORM = False\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 1024\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_SIZE = [256, 128, 64]\n",
    "DROPOUT = 0.2\n",
    "# path = pd.read_csv('MicroLens-50k_pairs.csv')"
   ],
   "id": "7577a987b7a2c3c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T13:58:50.578506Z",
     "start_time": "2025-07-12T13:58:50.287146Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de02a54222d2235",
    "outputId": "b772a185-83b1-4931-813b-11099cd8f3db"
   },
   "cell_type": "code",
   "source": "dataset_pd,num_users,num_items = preprocess.openAndSort(path,user_id=user,item_id=item)",
   "id": "de02a54222d2235",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dataset base informationÔºö\n",
      "- number of usersÔºö50000\n",
      "- number of itemsÔºö19220\n",
      "- number of rowsÔºö359708\n"
     ]
    }
   ],
   "execution_count": 118
  },
  {
   "cell_type": "code",
   "id": "3db1146761e45165",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T13:58:50.861004Z",
     "start_time": "2025-07-12T13:58:50.639579Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3db1146761e45165",
    "outputId": "eed7d56c-f244-4146-8249-f909c690bccf"
   },
   "source": [
    "\n",
    "train_df, test_df = preprocess.split(dataset_pd,user, item, timestamp)\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Test size: {len(test_df)}\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train size: 309708\n",
      "Test size: 49424\n"
     ]
    }
   ],
   "execution_count": 119
  },
  {
   "cell_type": "code",
   "id": "ce079817",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T13:58:51.882961Z",
     "start_time": "2025-07-12T13:58:50.882539Z"
    },
    "id": "ce079817"
   },
   "source": [
    "# maintain a map from new id to old id, new id for constructing matrix\n",
    "user2id = {u: i for i, u in enumerate(dataset_pd[user].unique())}\n",
    "item2id = {i: j for j, i in enumerate(dataset_pd[item].unique())}\n",
    "\n",
    "# apply to train_df and test_df\n",
    "train_df[user_id] = train_df[user].map(user2id)\n",
    "train_df[item_id] = train_df[item].map(item2id)\n",
    "test_df[user_id] = test_df[user].map(user2id)\n",
    "test_df[item_id] = test_df[item].map(item2id)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 120
  },
  {
   "metadata": {
    "id": "9c4d28e9e285faa9"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 121,
   "source": [
    "# DSSM implementation in PyTorch\n",
    "\n",
    "\n",
    "class DSSM(nn.Module):\n",
    "    \"\"\"\n",
    "    ÂèåÂ°î DSSMÔºöÁî®Êà∑Â°î + Áâ©ÂìÅÂ°î\n",
    "    hidden_dims Â¶Ç [128, 64]ÔºåÊúÄÂêéËæìÂá∫Áª¥Â∫¶ = hidden_dims[-1]\n",
    "    \"\"\"\n",
    "    def __init__(self, num_users, num_items, emb_dim=EMBEDDING_DIM, mlp_hidden_size=HIDDEN_SIZE, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, emb_dim)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_dim)\n",
    "\n",
    "        # üî∏ ÂàùÂßãÂåñÔºàÂèØÈÄâ‰ΩÜÊé®ËçêÔºâ\n",
    "        nn.init.xavier_uniform_(self.user_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.item_emb.weight)\n",
    "                # ÊûÑÂª∫ MLP Â±Ç\n",
    "        self.mlp_user = self.build_mlp(emb_dim, mlp_hidden_size, dropout)\n",
    "        self.mlp_item = self.build_mlp(emb_dim, mlp_hidden_size, dropout)\n",
    "\n",
    "    def build_mlp(self, input_dim, hidden_sizes, dropout):\n",
    "          layers = []\n",
    "          for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(input_dim, h))\n",
    "            layers.append(nn.BatchNorm1d(h))\n",
    "            layers.append(nn.Tanh())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            input_dim = h\n",
    "          return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, user_id, item_id, l2_norm=L2_NORM):\n",
    "        \"\"\"\n",
    "        ËøîÂõû:\n",
    "          score: (B,) ÁÇπÁßØÂàÜÊï∞\n",
    "          u_vec, i_vec: (B, d) ‰∏§‰æßÂêëÈáè\n",
    "        \"\"\"\n",
    "        u = self.user_emb(user_id)          # (B, emb_dim)\n",
    "        i = self.item_emb(item_id)          # (B, emb_dim)\n",
    "\n",
    "        u_vec = self.mlp_user(u)            # (B, d)\n",
    "        i_vec = self.mlp_item(i)            # (B, d)\n",
    "\n",
    "        if l2_norm:\n",
    "            u_vec = F.normalize(u_vec, p=2, dim=1)\n",
    "            i_vec = F.normalize(i_vec, p=2, dim=1)\n",
    "\n",
    "        return u_vec, i_vec\n",
    "    def get_users_embedding(self,user_ids,l2_norm=L2_NORM):\n",
    "        u = self.user_emb(user_ids)          # (B, emb_dim)\n",
    "\n",
    "        u_vec = self.mlp_user(u)            # (B, d)\n",
    "\n",
    "        if l2_norm:\n",
    "            u_vec = F.normalize(u_vec, p=2, dim=1)\n",
    "        return u_vec\n",
    "    def get_items_embedding(self,item_ids,l2_norm=L2_NORM):\n",
    "        i = self.item_emb(item_ids)          # (B, emb_dim)\n",
    "\n",
    "        i_vec = self.mlp_item(i)            # (B, d)\n",
    "\n",
    "        if l2_norm:\n",
    "            i_vec = F.normalize(i_vec, p=2, dim=1)\n",
    "        return i_vec\n",
    "\n",
    "    def save_embeddings(self, num_users, num_items, device, save_dir='./embeddings',l2_norm = L2_NORM):\n",
    "        import os\n",
    "        import faiss\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        self.eval()\n",
    "        self.to(device)\n",
    "\n",
    "        user_ids = torch.arange(num_users, dtype=torch.long, device=device)\n",
    "        item_ids = torch.arange(num_items, dtype=torch.long, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            user_embeds = self.get_users_embedding(user_ids, l2_norm=l2_norm)\n",
    "            item_embeds = self.get_items_embedding(item_ids, l2_norm=l2_norm)\n",
    "\n",
    "        user_embeds = user_embeds.cpu().numpy().astype(np.float32)\n",
    "        item_embeds = item_embeds.cpu().numpy().astype(np.float32)\n",
    "\n",
    "        # ‰øùÂ≠òÂêëÈáè\n",
    "        np.save(f\"{save_dir}/user_embeddings.npy\", user_embeds)\n",
    "        np.save(f\"{save_dir}/item_embeddings.npy\", item_embeds)\n",
    "\n",
    "        # ÊûÑÂª∫ FAISS indexÔºà‰ΩøÁî®ÂÜÖÁßØÔºâ\n",
    "        dim = item_embeds.shape[1]\n",
    "        index = faiss.IndexFlatIP(dim)\n",
    "        index.add(item_embeds)\n",
    "\n",
    "        faiss.write_index(index, f\"{save_dir}/item_index.faiss\")\n",
    "        print(\"Saved user/item embeddings and FAISS index.\")\n",
    "\n"
   ],
   "id": "9c4d28e9e285faa9"
  },
  {
   "metadata": {
    "id": "f80a999c1ccde5e8"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 122,
   "source": [
    "\n",
    "def train_model(model,train_df,\n",
    "                epochs=10,\n",
    "                batch_size=64,\n",
    "                lr=1e-3,\n",
    "                device=None):\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # ‰Ω†ÈúÄË¶Å‰∏Ä‰∏™ data_loader ËøîÂõû (user_id, pos_item_id) ÂØπÔºåÊó†Ë¥üÊ†∑Êú¨\n",
    "    train_loader = customdataset.build_train_loader_inbatch(train_df, batch_size=batch_size,user_col=user_id, item_col=item_id)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        dt_start = datetime.now()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            user_ids, pos_item_ids = batch\n",
    "            user_ids = user_ids.to(device)\n",
    "            pos_item_ids = pos_item_ids.to(device)\n",
    "\n",
    "            # 1. ÂâçÂêë‰º†Êí≠ÔºàËøîÂõû user / item ÂêëÈáèÔºâ\n",
    "            u_vec, i_vec = model(user_ids, pos_item_ids, l2_norm=L2_NORM)\n",
    "\n",
    "            # 2. ÂæóÂàÜÁü©ÈòµÔºöÊØè‰∏™ user ÂØπÊâÄÊúâÊ≠£ item ÁöÑÊâìÂàÜ\n",
    "            logits = torch.matmul(u_vec, i_vec.T)  # shape: (B, B)\n",
    "\n",
    "            # 3. ÊûÑÈÄ†Ê†áÁ≠æÔºöÊØè‰∏™ user ÁöÑÊ≠£Á°Æ item Âú®ÂØπËßíÁ∫øÔºàÂç≥‰ΩçÁΩÆ iÔºâ\n",
    "            labels = torch.arange(logits.size(0), device=device)  # [0, 1, ..., B-1]\n",
    "\n",
    "            # 4. Cross Entropy Loss\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "            # 5. ÂèçÂêë‰º†Êí≠\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Êó•Âøó\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        dt_end = datetime.now()\n",
    "        dt = dt_end - dt_start\n",
    "\n",
    "        print(f\"[Epoch {epoch:02d}/{epochs}] avg InBatch Softmax Loss = {avg_loss:.4f}, time = {dt.total_seconds():.2f}s\")\n",
    "\n",
    "    return\n",
    "\n",
    "\n"
   ],
   "id": "f80a999c1ccde5e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T14:01:55.008421Z",
     "start_time": "2025-07-12T14:01:54.939903Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "848b63e397b62c61",
    "outputId": "76728e3a-ea8f-4d15-d629-02606b15e230"
   },
   "cell_type": "code",
   "source": [
    "model = DSSM(num_users,num_items)\n",
    "model.to(device)\n",
    "train_model(model=model,epochs=EPOCHS, train_df=train_df,batch_size=BATCH_SIZE)"
   ],
   "id": "848b63e397b62c61",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Epoch 01/50] avg InBatch Softmax Loss = 9.5322, time = 3.15s\n",
      "[Epoch 02/50] avg InBatch Softmax Loss = 7.9468, time = 3.16s\n",
      "[Epoch 03/50] avg InBatch Softmax Loss = 7.2972, time = 3.22s\n",
      "[Epoch 04/50] avg InBatch Softmax Loss = 7.0288, time = 3.06s\n",
      "[Epoch 05/50] avg InBatch Softmax Loss = 6.8261, time = 3.16s\n",
      "[Epoch 06/50] avg InBatch Softmax Loss = 6.5714, time = 3.09s\n",
      "[Epoch 07/50] avg InBatch Softmax Loss = 6.3170, time = 3.07s\n",
      "[Epoch 08/50] avg InBatch Softmax Loss = 6.0346, time = 3.09s\n",
      "[Epoch 09/50] avg InBatch Softmax Loss = 5.7439, time = 3.06s\n",
      "[Epoch 10/50] avg InBatch Softmax Loss = 5.4829, time = 3.10s\n",
      "[Epoch 11/50] avg InBatch Softmax Loss = 5.2834, time = 3.10s\n",
      "[Epoch 12/50] avg InBatch Softmax Loss = 5.1380, time = 3.03s\n",
      "[Epoch 13/50] avg InBatch Softmax Loss = 5.0249, time = 3.07s\n",
      "[Epoch 14/50] avg InBatch Softmax Loss = 4.9413, time = 3.10s\n",
      "[Epoch 15/50] avg InBatch Softmax Loss = 4.8745, time = 3.09s\n",
      "[Epoch 16/50] avg InBatch Softmax Loss = 4.8158, time = 3.03s\n",
      "[Epoch 17/50] avg InBatch Softmax Loss = 4.7643, time = 3.04s\n",
      "[Epoch 18/50] avg InBatch Softmax Loss = 4.7214, time = 3.09s\n",
      "[Epoch 19/50] avg InBatch Softmax Loss = 4.6808, time = 3.07s\n",
      "[Epoch 20/50] avg InBatch Softmax Loss = 4.6472, time = 3.03s\n",
      "[Epoch 21/50] avg InBatch Softmax Loss = 4.6163, time = 3.08s\n",
      "[Epoch 22/50] avg InBatch Softmax Loss = 4.5831, time = 3.06s\n",
      "[Epoch 23/50] avg InBatch Softmax Loss = 4.5558, time = 3.08s\n",
      "[Epoch 24/50] avg InBatch Softmax Loss = 4.5330, time = 3.13s\n",
      "[Epoch 25/50] avg InBatch Softmax Loss = 4.5053, time = 3.10s\n",
      "[Epoch 26/50] avg InBatch Softmax Loss = 4.4831, time = 3.09s\n",
      "[Epoch 27/50] avg InBatch Softmax Loss = 4.4632, time = 3.02s\n",
      "[Epoch 28/50] avg InBatch Softmax Loss = 4.4435, time = 3.05s\n",
      "[Epoch 29/50] avg InBatch Softmax Loss = 4.4207, time = 2.99s\n",
      "[Epoch 30/50] avg InBatch Softmax Loss = 4.4065, time = 3.04s\n",
      "[Epoch 31/50] avg InBatch Softmax Loss = 4.3881, time = 3.00s\n",
      "[Epoch 32/50] avg InBatch Softmax Loss = 4.3735, time = 3.02s\n",
      "[Epoch 33/50] avg InBatch Softmax Loss = 4.3599, time = 3.01s\n",
      "[Epoch 34/50] avg InBatch Softmax Loss = 4.3427, time = 3.06s\n",
      "[Epoch 35/50] avg InBatch Softmax Loss = 4.3323, time = 3.07s\n",
      "[Epoch 36/50] avg InBatch Softmax Loss = 4.3167, time = 3.01s\n",
      "[Epoch 37/50] avg InBatch Softmax Loss = 4.3044, time = 3.18s\n",
      "[Epoch 38/50] avg InBatch Softmax Loss = 4.2917, time = 3.08s\n",
      "[Epoch 39/50] avg InBatch Softmax Loss = 4.2818, time = 3.01s\n",
      "[Epoch 40/50] avg InBatch Softmax Loss = 4.2700, time = 3.00s\n",
      "[Epoch 41/50] avg InBatch Softmax Loss = 4.2607, time = 3.12s\n",
      "[Epoch 42/50] avg InBatch Softmax Loss = 4.2537, time = 3.05s\n",
      "[Epoch 43/50] avg InBatch Softmax Loss = 4.2413, time = 3.00s\n",
      "[Epoch 44/50] avg InBatch Softmax Loss = 4.2344, time = 3.05s\n",
      "[Epoch 45/50] avg InBatch Softmax Loss = 4.2257, time = 3.04s\n",
      "[Epoch 46/50] avg InBatch Softmax Loss = 4.2179, time = 3.08s\n",
      "[Epoch 47/50] avg InBatch Softmax Loss = 4.2082, time = 3.04s\n",
      "[Epoch 48/50] avg InBatch Softmax Loss = 4.2009, time = 3.02s\n",
      "[Epoch 49/50] avg InBatch Softmax Loss = 4.1985, time = 3.10s\n",
      "[Epoch 50/50] avg InBatch Softmax Loss = 4.1895, time = 3.09s\n"
     ]
    }
   ],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T14:02:03.509536Z",
     "start_time": "2025-07-12T14:01:55.208715Z"
    },
    "id": "d626cca926d2732f",
    "outputId": "8d06a378-4f82-4edb-963f-d1414e914c7f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "source": [
    "model.save_embeddings(num_users=num_users,num_items=num_items,device=device,save_dir=save_dir)"
   ],
   "id": "d626cca926d2732f",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved user/item embeddings and FAISS index.\n"
     ]
    }
   ],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T14:02:11.793103Z",
     "start_time": "2025-07-12T14:02:03.541546Z"
    },
    "id": "8be6dec3309abad7"
   },
   "cell_type": "code",
   "source": [
    "test_loader = customdataset.build_test_loader(test_df, num_items ,user_col = user_id, item_col = item_id, batch_size=1024, num_workers=num_workers)\n",
    "item_pool = list(range(num_items))\n",
    "faiss_index = faiss.read_index(f\"{save_dir}/item_index.faiss\")"
   ],
   "id": "8be6dec3309abad7",
   "outputs": [],
   "execution_count": 125
  },
  {
   "metadata": {
    "id": "79cc0d7cfa3e17c2",
    "outputId": "128004af-e219-4477-f109-f530e3748a7e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Random HR@10 = 0.0005, nDCG@10 = 0.0003\n",
      "Popular HR@10 = 0.0029, nDCG@10 = 0.0014\n",
      "Model   HR@10 = 0.0230, nDCG@10 = 0.0092\n"
     ]
    }
   ],
   "execution_count": 126,
   "source": [
    "hr_r, ndcg_r = evaluate.evaluate_random(test_loader, item_pool ,top_k=top_k)\n",
    "print(f\"Random HR@{top_k} = {hr_r:.4f}, NDCG@{top_k} = {ndcg_r:.4f}\")\n",
    "hr_p, ndcg_p = evaluate.evaluate_popular(test_loader, train_df,top_k=top_k)\n",
    "print(f\"Popular HR@{top_k} = {hr_p:.4f}, NDCG@{top_k} = {ndcg_p:.4f}\")\n",
    "hr_m, ndcg_m = evaluate.evaluate_model(test_loader, model, faiss_index, device,top_k=top_k)\n",
    "print(f\"Model   HR@{top_k} = {hr_m:.4f}, NDCG@{top_k} = {ndcg_m:.4f}\")\n"
   ],
   "id": "79cc0d7cfa3e17c2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "colab": {
   "provenance": [],
   "gpuType": "L4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

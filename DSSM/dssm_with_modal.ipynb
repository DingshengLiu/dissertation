{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from tkinter.constants import HIDDEN\n",
        "\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "\n",
        "def copy_from_drive(src_path, dst_path):\n",
        "\n",
        "    if os.path.exists(dst_path):\n",
        "        print(f\"skip:{dst_path} exists\")\n",
        "        return\n",
        "\n",
        "    if os.path.isdir(src_path):\n",
        "        shutil.copytree(src_path, dst_path)\n",
        "    elif os.path.isfile(src_path):\n",
        "        shutil.copy(src_path, dst_path)\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "copy_from_drive('/content/drive/MyDrive/tool', '/content/tool')\n",
        "copy_from_drive('/content/drive/MyDrive/MicroLens-50k_pairs.csv','/content/MicroLens-50k_pairs.csv')\n",
        "copy_from_drive('/content/drive/MyDrive/cover_emb128.lmdb','/content/cover_emb128.lmdb')\n",
        "copy_from_drive('/content/drive/MyDrive/title_emb1024.lmdb','/content/title_emb1024.lmdb')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3TTppBdUSal",
        "outputId": "0dc2144f-4db9-4c83-b3da-c5b66d2b389b"
      },
      "id": "S3TTppBdUSal",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "skip:/content/tool exists\n",
            "skip:/content/MicroLens-50k_pairs.csv exists\n",
            "skip:/content/cover_emb128.lmdb exists\n",
            "skip:/content/title_emb1024.lmdb exists\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "2cf7762f790b03a9",
        "outputId": "04242e84-5837-4bd2-f2dd-eea34e3e4605",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.1-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.13.1-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m117.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.13.1\n",
            "Collecting lmdb\n",
            "  Downloading lmdb-1.7.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (1.4 kB)\n",
            "Downloading lmdb-1.7.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (299 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.4/299.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lmdb\n",
            "Successfully installed lmdb-1.7.5\n"
          ]
        }
      ],
      "execution_count": 3,
      "source": [
        "!pip install faiss-cpu\n",
        "!pip install lmdb\n",
        "from tool import preprocess\n",
        "from tool import customdataset\n",
        "from tool import evaluate\n",
        "import faiss\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from datetime import datetime\n",
        "import math\n",
        "import csv\n",
        "from matplotlib import pyplot as plt"
      ],
      "id": "2cf7762f790b03a9"
    },
    {
      "metadata": {
        "id": "79b35d503cf0a9e2",
        "outputId": "a9632ea3-2a4c-4720-be96-cec35cc09b60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function tool.preprocess.set_seed.<locals>.seed_worker(worker_id)>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>tool.preprocess.set_seed.&lt;locals&gt;.seed_worker</b><br/>def seed_worker(worker_id)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/tool/preprocess.py</a>&lt;no docstring&gt;</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 18);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "execution_count": 4,
      "source": [
        "preprocess.set_seed(42)"
      ],
      "id": "79b35d503cf0a9e2"
    },
    {
      "metadata": {
        "id": "d97293349e89d19f"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 5,
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "id": "d97293349e89d19f"
    },
    {
      "metadata": {
        "id": "7577a987b7a2c3c0"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 6,
      "source": [
        "path = 'MicroLens-50k_pairs.csv'\n",
        "user = 'user'\n",
        "item = 'item'\n",
        "user_id = 'user_id'\n",
        "item_id = 'item_id'\n",
        "timestamp = 'timestamp'\n",
        "save_dir = './embeddings'\n",
        "record_path = './records'\n",
        "cover_lmdb_path = 'cover_emb128.lmdb'\n",
        "title_lmdb_path = 'title_emb1024.lmdb'\n",
        "PROJECT_NAME = 'DSSM'\n",
        "# ---------- 超参数 ----------\n",
        "LR = 1e-3\n",
        "TOP_K= 10\n",
        "PATIENCE = 5\n",
        "MONITOR = 'hr'\n",
        "NUM_WORKERS = 10\n",
        "L2_NORM = False\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 1024\n",
        "EMBEDDING_DIM = 256\n",
        "HIDDEN_SIZE = [256, 128, 64]\n",
        "DROPOUT = 0.2\n",
        "MODAL = {'COVER':{\"LMDB_DIM\":128, \"HIDDEN_SIZE\":[EMBEDDING_DIM],\"DROPOUT\":0.2} , 'TITLE':{\"LMDB_DIM\":1024,\"HIDDEN_SIZE\":[EMBEDDING_DIM],\"DROPOUT\":0.2}\n",
        "         ,'COVER-TITLE': {\"LMDB_DIM\":128+1024, \"HIDDEN_SIZE\":[EMBEDDING_DIM],\"DROPOUT\":0.2}}\n",
        "FUSION_MODE='base'\n",
        "CURRENT_MODAL = \"COVER-TITLE\"\n",
        "MODAL_CONFIG = MODAL[CURRENT_MODAL]\n",
        "MODAL_HIDDEN_SIZE = MODAL_CONFIG.get('HIDDEN_SIZE')\n",
        "LMDB_DIM = MODAL_CONFIG.get('LMDB_DIM')\n",
        "MODAL_DROPOUT = MODAL_CONFIG.get('DROPOUT')\n",
        "# path = pd.read_csv('MicroLens-50k_pairs.csv')"
      ],
      "id": "7577a987b7a2c3c0"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-07-12T13:58:50.578506Z",
          "start_time": "2025-07-12T13:58:50.287146Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de02a54222d2235",
        "outputId": "22fac0e5-1807-4579-91e3-d59cedcc4bfe"
      },
      "cell_type": "code",
      "source": [
        "dataset_pd,num_users,num_items = preprocess.openAndSort(path,user_id=user,item_id=item,timestamp='timestamp')"
      ],
      "id": "de02a54222d2235",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset base information：\n",
            "- number of users：50000\n",
            "- number of items：19220\n",
            "- number of rows：359708\n"
          ]
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "id": "3db1146761e45165",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-07-12T13:58:50.861004Z",
          "start_time": "2025-07-12T13:58:50.639579Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3db1146761e45165",
        "outputId": "f0b24952-585f-4d73-935d-3a1e6706d554"
      },
      "source": [
        "\n",
        "train_df, val_df, test_df, train_all_df = preprocess.split_with_val(dataset_pd,user, item, timestamp)\n",
        "print(f\"Train size: {len(train_df)}\")\n",
        "print(f\"Val_df size: {len(val_df)}\")\n",
        "print(f\"Test_df size: {len(test_df)}\")\n",
        "print(f\"Train_all_df size: {len(train_all_df)}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 259708\n",
            "Val_df size: 49156\n",
            "Test_df size: 47774\n",
            "Train_all_df size: 308864\n"
          ]
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "id": "ce079817",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-07-12T13:58:51.882961Z",
          "start_time": "2025-07-12T13:58:50.882539Z"
        },
        "id": "ce079817"
      },
      "source": [
        "# maintain a map from new id to old id, new id for constructing matrix\n",
        "user2id = {u: i for i, u in enumerate(dataset_pd[user].unique())}\n",
        "item2id = {i: j for j, i in enumerate(dataset_pd[item].unique())}\n",
        "\n",
        "# apply to train_df and test_df\n",
        "train_df[user_id] = train_df[user].map(user2id)\n",
        "train_df[item_id] = train_df[item].map(item2id)\n",
        "val_df[user_id] = val_df[user].map(user2id)\n",
        "val_df[item_id] = val_df[item].map(item2id)\n",
        "test_df[user_id] = test_df[user].map(user2id)\n",
        "test_df[item_id] = test_df[item].map(item2id)\n",
        "train_all_df[user_id] = train_all_df[user].map(user2id)\n",
        "train_all_df[item_id] = train_all_df[item].map(item2id)\n",
        "\n",
        "# 1. 构建 item_id 到 item 的映射（来自 train_df）\n",
        "item_id_to_item = {v: k for k, v in item2id.items()}"
      ],
      "outputs": [],
      "execution_count": 9
    },
    {
      "metadata": {
        "id": "9c4d28e9e285faa9"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 10,
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class DSSM(nn.Module):\n",
        "    \"\"\"\n",
        "    双塔 DSSM（支持三种融合）:\n",
        "      fusion_mode:\n",
        "        - 'base'  : 纯 ID\n",
        "        - 'early' : 前融合（你现有的： [item_id_emb; modal] -> mlp_item_modal -> emb_dim）\n",
        "        - 'late'  : 后融合（ID 路径与模态路径分别编码，再 alpha 加权）\n",
        "    \"\"\"\n",
        "    def __init__(self, num_users, num_items,\n",
        "                 emb_dim=EMBEDDING_DIM,\n",
        "                 mlp_hidden_size=HIDDEN_SIZE,\n",
        "                 dropout=DROPOUT,\n",
        "                 modal_hidden_size=MODAL_HIDDEN_SIZE,\n",
        "                 modal_dropout=MODAL_DROPOUT,\n",
        "                 lmdb_dim=LMDB_DIM,\n",
        "                 fusion_mode=FUSION_MODE):\n",
        "        super().__init__()\n",
        "        assert fusion_mode in {'base','early','late'}\n",
        "        self.fusion_mode = fusion_mode\n",
        "\n",
        "        self.user_emb = nn.Embedding(num_users, emb_dim)\n",
        "        self.item_emb = nn.Embedding(num_items, emb_dim)\n",
        "        nn.init.xavier_uniform_(self.user_emb.weight)\n",
        "        nn.init.xavier_uniform_(self.item_emb.weight)\n",
        "        # modal 向量（冻结）\n",
        "        modal_emb_tensor = None\n",
        "        if FUSION_MODE!='base':\n",
        "            if CURRENT_MODAL=='COVER':\n",
        "                modal_emb_tensor = preprocess.load_tensor_from_lmdb(\n",
        "                    cover_lmdb_path, num_items, item_id_to_item, lmdb_dim\n",
        "                )\n",
        "            if CURRENT_MODAL=='TITLE':\n",
        "                modal_emb_tensor = preprocess.load_tensor_from_lmdb(\n",
        "                    title_lmdb_path, num_items, item_id_to_item, lmdb_dim\n",
        "                )\n",
        "            if CURRENT_MODAL=='COVER-TITLE':\n",
        "                cover_emb_tensor = preprocess.load_tensor_from_lmdb(\n",
        "                    cover_lmdb_path, num_items, item_id_to_item, 128\n",
        "                )\n",
        "                title_emb_tensor = preprocess.load_tensor_from_lmdb(\n",
        "                    title_lmdb_path, num_items, item_id_to_item, 1024\n",
        "                )\n",
        "                modal_emb_tensor = torch.cat([cover_emb_tensor, title_emb_tensor], dim=-1)\n",
        "\n",
        "            self.register_buffer('frozen_extra_emb', modal_emb_tensor)\n",
        "\n",
        "        # 通用 MLP\n",
        "        self.mlp_user = self.build_mlp(emb_dim, mlp_hidden_size, dropout)\n",
        "        self.mlp_item = self.build_mlp(emb_dim, mlp_hidden_size, dropout)\n",
        "\n",
        "        # 前融合用：将 [item; modal] -> emb_dim\n",
        "        self.mlp_item_modal = self.build_mlp(emb_dim + lmdb_dim, modal_hidden_size, modal_dropout)\n",
        "\n",
        "        # 后融合用：全局 alpha（标量，sigmoid 后 ∈ (0,1)）# sigmoid(0)=0.5 起步\n",
        "        self.alpha_param = nn.Parameter(torch.tensor(0.0)) if fusion_mode == 'late' else None\n",
        "\n",
        "    def build_mlp(self, input_dim, hidden_sizes, dropout):\n",
        "        layers = []\n",
        "        for h in hidden_sizes:\n",
        "            layers += [nn.Linear(input_dim, h), nn.BatchNorm1d(h), nn.Tanh(), nn.Dropout(dropout)]\n",
        "            input_dim = h\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _item_vec_id_only(self, item_id):\n",
        "        i_id = self.item_emb(item_id)      # (B, emb_dim)\n",
        "        return self.mlp_item(i_id)         # (B, d)\n",
        "\n",
        "    def _item_vec_early(self, item_id):\n",
        "        # 前融合路径： [item_emb; modal] -> emb_dim -> mlp_item -> d\n",
        "        i_id = self.item_emb(item_id)\n",
        "        modal = self.frozen_extra_emb.to(item_id.device)[item_id]\n",
        "        i_cat = torch.cat([i_id, modal], dim=-1)\n",
        "        i_emb = self.mlp_item_modal(i_cat)   # (B, emb_dim)\n",
        "        return self.mlp_item(i_emb)          # (B, d)\n",
        "\n",
        "    def _item_vec_late(self, item_id):\n",
        "        # 后融合：向量级\n",
        "        i_vec_id = self._item_vec_id_only(item_id)   # (B, d)\n",
        "        i_vec_mm = self._item_vec_early(item_id)     # (B, d) —— 复用 early 路径的“模态子塔”\n",
        "        alpha = torch.sigmoid(self.alpha_param)      # 标量\n",
        "        return alpha * i_vec_id + (1.0 - alpha) * i_vec_mm\n",
        "\n",
        "    def forward(self, user_id, item_id, l2_norm=L2_NORM):\n",
        "        # 用户向量\n",
        "        u = self.user_emb(user_id)\n",
        "        u_vec = self.mlp_user(u)\n",
        "\n",
        "        # 物品向量（按模式）\n",
        "        if self.fusion_mode == 'base':\n",
        "            i_vec = self._item_vec_id_only(item_id)\n",
        "        elif self.fusion_mode == 'early':\n",
        "            i_vec = self._item_vec_early(item_id)\n",
        "        else:  # 'late'\n",
        "            i_vec = self._item_vec_late(item_id)\n",
        "\n",
        "        if l2_norm:\n",
        "            u_vec = F.normalize(u_vec, p=2, dim=1)\n",
        "            i_vec = F.normalize(i_vec, p=2, dim=1)\n",
        "        return u_vec, i_vec\n",
        "\n",
        "    def get_users_embedding(self, user_ids, l2_norm=L2_NORM):\n",
        "        u = self.user_emb(user_ids)\n",
        "        u_vec = self.mlp_user(u)\n",
        "        if l2_norm: u_vec = F.normalize(u_vec, p=2, dim=1)\n",
        "        return u_vec\n",
        "\n",
        "    def get_items_embedding(self, item_ids, l2_norm=L2_NORM):\n",
        "        if self.fusion_mode == 'base':\n",
        "            i_vec = self._item_vec_id_only(item_ids)\n",
        "        elif self.fusion_mode == 'early':\n",
        "            i_vec = self._item_vec_early(item_ids)\n",
        "        else:\n",
        "            i_vec = self._item_vec_late(item_ids)\n",
        "        if l2_norm: i_vec = F.normalize(i_vec, p=2, dim=1)\n",
        "        return i_vec\n",
        "\n",
        "    def save_embeddings(self, num_users, num_items, device, save_dir='./embeddings', l2_norm=L2_NORM):\n",
        "        import os, faiss\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        self.eval().to(device)\n",
        "        user_ids = torch.arange(num_users, dtype=torch.long, device=device)\n",
        "        item_ids = torch.arange(num_items, dtype=torch.long, device=device)\n",
        "        with torch.no_grad():\n",
        "            user_embeds = self.get_users_embedding(user_ids, l2_norm=l2_norm)\n",
        "            item_embeds = self.get_items_embedding(item_ids, l2_norm=l2_norm)\n",
        "        user_embeds = user_embeds.cpu().numpy().astype(np.float32)\n",
        "        item_embeds = item_embeds.cpu().numpy().astype(np.float32)\n",
        "        np.save(f\"{save_dir}/user_embeddings.npy\", user_embeds)\n",
        "        np.save(f\"{save_dir}/item_embeddings.npy\", item_embeds)\n",
        "        dim = item_embeds.shape[1]\n",
        "        index = faiss.IndexFlatIP(dim)\n",
        "        index.add(item_embeds)\n",
        "        faiss.write_index(index, f\"{save_dir}/item_index.faiss\")"
      ],
      "id": "9c4d28e9e285faa9"
    },
    {
      "metadata": {
        "id": "f80a999c1ccde5e8"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 11,
      "source": [
        "def train_model(model,train_df,val_df,top_k,\n",
        "                epochs,\n",
        "                batch_size,\n",
        "                lr,\n",
        "                val_mode,\n",
        "                device=None,\n",
        "                patience=PATIENCE,         # 早停容忍\n",
        "                monitor=MONITOR,       # \"hr\" 或 \"ndcg\"\n",
        "                record_path = record_path\n",
        "                ):\n",
        "\n",
        "    if device is None:\n",
        "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # 你需要一个 data_loader 返回 (user_id, pos_item_id) 对，无负样本\n",
        "    train_loader = customdataset.build_train_loader_inbatch(train_df, batch_size=batch_size,user_col=user_id, item_col=item_id)\n",
        "    val_loader = customdataset.build_test_loader(val_df, num_items ,user_col = user_id, item_col = item_id, batch_size=1024, num_workers=NUM_WORKERS)\n",
        "\n",
        "    # 训练过程记录\n",
        "    hist = {\n",
        "        \"epoch\": [],\n",
        "        \"loss\": [],\n",
        "        f\"hr@{top_k}\": [],\n",
        "        f\"ndcg@{top_k}\": [],\n",
        "        \"alpha\": [],\n",
        "        \"beta\": [],\n",
        "    }\n",
        "\n",
        "    # 早停配置\n",
        "    best_metric = -math.inf\n",
        "    best_epoch  = -1\n",
        "    patience_cnt = 0\n",
        "    monitor_key = f\"{monitor}@{top_k}\"\n",
        "\n",
        "    print(f\"[EarlyStopping] monitor={monitor_key} , patience={patience}\")\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        dt_start = datetime.now()\n",
        "        epoch_loss = 0.0\n",
        "        for batch in train_loader:\n",
        "            user_ids, pos_item_ids = batch\n",
        "            user_ids = user_ids.to(device)\n",
        "            pos_item_ids = pos_item_ids.to(device)\n",
        "\n",
        "            # 1. 前向传播（返回 user / item 向量）\n",
        "            u_vec, i_vec = model(user_ids, pos_item_ids, l2_norm=L2_NORM)\n",
        "\n",
        "            # 2. 得分矩阵：每个 user 对所有正 item 的打分\n",
        "            logits = torch.matmul(u_vec, i_vec.T)  # shape: (B, B)\n",
        "\n",
        "            # 3. 构造标签：每个 user 的正确 item 在对角线（即位置 i）\n",
        "            labels = torch.arange(logits.size(0), device=device)  # [0, 1, ..., B-1]\n",
        "\n",
        "            # 4. Cross Entropy Loss\n",
        "            loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "            # 5. 反向传播\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        # 日志\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        dt_end = datetime.now()\n",
        "        dt = (dt_end - dt_start).total_seconds()\n",
        "        model.save_embeddings(num_users=num_users,num_items=num_items,device=device,save_dir=save_dir)\n",
        "        faiss_index = faiss.read_index(f\"{save_dir}/item_index.faiss\")\n",
        "        model.eval()\n",
        "        hr_m, ndcg_m = evaluate.evaluate_model(val_loader, model, faiss_index, device, top_k=top_k)\n",
        "\n",
        "        # gates（若存在）\n",
        "        alpha_val = float(torch.sigmoid(model.alpha_param).item()) if hasattr(model, \"alpha_param\") and model.alpha_param is not None else float(\"nan\")\n",
        "        beta_val  = float(torch.sigmoid(model.beta_param).item())  if hasattr(model, \"beta_param\") and model.beta_param is not None else float(\"nan\")\n",
        "\n",
        "        print(f\"[Epoch {epoch:02d}/{epochs}] avg InBatch Softmax Loss = {avg_loss:.4f}, \"\n",
        "              f\"HR@{top_k} = {hr_m:.4f}, NDCG@{top_k} = {ndcg_m:.4f}, \"\n",
        "              f\"alpha={alpha_val if not math.isnan(alpha_val) else 'NA'}, \"\n",
        "              f\"beta={beta_val if not math.isnan(beta_val) else 'NA'}, \"\n",
        "              f\"time = {dt:.2f}s\")\n",
        "\n",
        "        # —— 记录历史 ——\n",
        "        hist[\"epoch\"].append(epoch)\n",
        "        hist[\"loss\"].append(avg_loss)\n",
        "        hist[f\"hr@{top_k}\"].append(hr_m)\n",
        "        hist[f\"ndcg@{top_k}\"].append(ndcg_m)\n",
        "        hist[\"alpha\"].append(alpha_val)\n",
        "        hist[\"beta\"].append(beta_val)\n",
        "\n",
        "        # —— 早停判断（最大化 monitor 指标）——\n",
        "        current_metric = hr_m if monitor == \"hr\" else ndcg_m\n",
        "        if current_metric > best_metric:\n",
        "            best_metric = current_metric\n",
        "            best_epoch = epoch\n",
        "            patience_cnt = 0\n",
        "            print(f\"current best {monitor_key}={best_metric:.4f} @ epoch {epoch}.\")\n",
        "                        # ==== 保存最佳 hr / ndcg / epoch ====\n",
        "            best_info_path = os.path.join(record_path,\n",
        "                                          \"validation mode\" if val_mode else \"train mode\",\n",
        "                                          \"best_result.txt\")\n",
        "            os.makedirs(os.path.dirname(best_info_path), exist_ok=True)\n",
        "            with open(best_info_path, \"w\") as f:\n",
        "                f.write(f\"epoch: {epoch}\\n\")\n",
        "                f.write(f\"HR@{top_k}: {hr_m:.4f}\\n\")\n",
        "                f.write(f\"NDCG@{top_k}: {ndcg_m:.4f}\\n\")\n",
        "            print(f\"Best result info saved to {best_info_path}\")\n",
        "        else:\n",
        "            patience_cnt += 1\n",
        "            if patience_cnt >= patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "    # —— 导出历史 CSV ——\n",
        "    csv_path = os.path.join(record_path,\"validation mode\" if val_mode else \"train mode\",\"training_history.csv\")\n",
        "    os.makedirs(os.path.dirname(csv_path), exist_ok=True)  # 确保目录存在\n",
        "    with open(csv_path, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"epoch\", \"loss\", f\"hr@{top_k}\", f\"ndcg@{top_k}\", \"alpha\", \"beta\", \"time_sec\"])\n",
        "        for i in range(len(hist[\"epoch\"])):\n",
        "            writer.writerow([\n",
        "                hist[\"epoch\"][i],\n",
        "                hist[\"loss\"][i],\n",
        "                hist[f\"hr@{top_k}\"][i],\n",
        "                hist[f\"ndcg@{top_k}\"][i],\n",
        "                hist[\"alpha\"][i],\n",
        "                hist[\"beta\"][i],\n",
        "            ])\n",
        "    # —— 绘图：Loss ——\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(hist[\"epoch\"], hist[\"loss\"])\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"In-Batch CE Loss\"); plt.title(\"Training Loss\")\n",
        "    plt.grid(True, linestyle=\"--\", alpha=0.4); plt.tight_layout()\n",
        "    plt.xticks(range(1, max(hist[\"epoch\"]) + 1, 1))\n",
        "    fig1_path = os.path.join(record_path,\"validation mode\" if val_mode else \"train mode\",\"curve_loss.png\")\n",
        "    os.makedirs(os.path.dirname(fig1_path), exist_ok=True)  # 确保目录存在\n",
        "\n",
        "    plt.savefig(fig1_path, dpi=150); plt.close()\n",
        "    print(f\"Saved {fig1_path}\")\n",
        "\n",
        "    # —— 绘图：HR/NDCG ——\n",
        "    plt.figure()\n",
        "    plt.plot(hist[\"epoch\"], hist[f\"hr@{top_k}\"], label=f\"HR@{top_k}\")\n",
        "    plt.plot(hist[\"epoch\"], hist[f\"ndcg@{top_k}\"], label=f\"NDCG@{top_k}\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Metric\"); plt.title(\"Validation Metrics\")\n",
        "    plt.legend(); plt.grid(True, linestyle=\"--\", alpha=0.4); plt.tight_layout()\n",
        "    plt.xticks(range(1, max(hist[\"epoch\"]) + 1, 1))\n",
        "    fig2_path = os.path.join(record_path,\"validation mode\" if val_mode else \"train mode\",\"curve_metrics.png\")\n",
        "    os.makedirs(os.path.dirname(fig2_path), exist_ok=True)  # 确保目录存在\n",
        "    plt.savefig(fig2_path, dpi=150); plt.close()\n",
        "    print(f\"Saved {fig2_path}\")\n",
        "\n",
        "    # —— 绘图：alpha/beta（如存在） ——\n",
        "    if not all(math.isnan(v) for v in hist[\"alpha\"]) or not all(math.isnan(v) for v in hist[\"beta\"]):\n",
        "        plt.figure()\n",
        "        if not all(math.isnan(v) for v in hist[\"alpha\"]):\n",
        "            plt.plot(hist[\"epoch\"], hist[\"alpha\"], label=\"alpha (item late)\")\n",
        "        if not all(math.isnan(v) for v in hist[\"beta\"]):\n",
        "            plt.plot(hist[\"epoch\"], hist[\"beta\"],  label=\"beta (user late)\")\n",
        "        plt.xlabel(\"Epoch\"); plt.ylabel(\"Gate (sigmoid)\"); plt.title(\"Late Fusion Gates\")\n",
        "        plt.ylim(0, 1); plt.legend(); plt.grid(True, linestyle=\"--\", alpha=0.4); plt.tight_layout()\n",
        "        plt.xticks(range(1, max(hist[\"epoch\"]) + 1, 1))\n",
        "        fig3_path = os.path.join(record_path,\"validation mode\" if val_mode else \"train mode\",\"curve_alpha_beta.png\")\n",
        "        os.makedirs(os.path.dirname(fig3_path), exist_ok=True)  # 确保目录存在\n",
        "        plt.savefig(fig3_path, dpi=150); plt.close()\n",
        "        print(f\"Saved {fig3_path}\")\n",
        "\n",
        "    print(f\"Best {monitor_key}={best_metric:.4f} at epoch {best_epoch}\")\n",
        "    return"
      ],
      "id": "f80a999c1ccde5e8"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-07-12T14:01:55.008421Z",
          "start_time": "2025-07-12T14:01:54.939903Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "848b63e397b62c61",
        "outputId": "dbaae06d-4e6c-4a73-bf78-5563d110e5ae"
      },
      "cell_type": "code",
      "source": [
        "model = DSSM(num_users,num_items)\n",
        "model.to(device)\n",
        "train_model(model=model,epochs=EPOCHS, train_df=train_df,val_df=val_df,batch_size=BATCH_SIZE,top_k=TOP_K,lr=LR,val_mode=True)\n",
        "model = DSSM(num_users,num_items)\n",
        "model.to(device)\n",
        "train_model(model=model,epochs=EPOCHS, train_df=train_all_df,val_df=test_df,batch_size=BATCH_SIZE,top_k=TOP_K,lr=LR,val_mode=False)"
      ],
      "id": "848b63e397b62c61",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EarlyStopping] monitor=hr@10 , patience=5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 01/50] avg InBatch Softmax Loss = 9.7129, HR@10 = 0.0004, NDCG@10 = 0.0002, alpha=NA, beta=NA, time = 6.55s\n",
            "current best hr@10=0.0004 @ epoch 1.\n",
            "Best result info saved to ./records/validation mode/best_result.txt\n",
            "[Epoch 02/50] avg InBatch Softmax Loss = 8.1679, HR@10 = 0.0005, NDCG@10 = 0.0002, alpha=NA, beta=NA, time = 3.26s\n",
            "current best hr@10=0.0005 @ epoch 2.\n",
            "Best result info saved to ./records/validation mode/best_result.txt\n",
            "[Epoch 03/50] avg InBatch Softmax Loss = 7.4777, HR@10 = 0.0004, NDCG@10 = 0.0003, alpha=NA, beta=NA, time = 2.90s\n",
            "[Epoch 04/50] avg InBatch Softmax Loss = 7.1383, HR@10 = 0.0006, NDCG@10 = 0.0003, alpha=NA, beta=NA, time = 2.98s\n",
            "current best hr@10=0.0006 @ epoch 4.\n",
            "Best result info saved to ./records/validation mode/best_result.txt\n",
            "[Epoch 05/50] avg InBatch Softmax Loss = 6.8930, HR@10 = 0.0042, NDCG@10 = 0.0018, alpha=NA, beta=NA, time = 3.00s\n",
            "current best hr@10=0.0042 @ epoch 5.\n",
            "Best result info saved to ./records/validation mode/best_result.txt\n",
            "[Epoch 06/50] avg InBatch Softmax Loss = 6.5968, HR@10 = 0.0094, NDCG@10 = 0.0043, alpha=NA, beta=NA, time = 3.34s\n",
            "current best hr@10=0.0094 @ epoch 6.\n",
            "Best result info saved to ./records/validation mode/best_result.txt\n",
            "[Epoch 07/50] avg InBatch Softmax Loss = 6.3071, HR@10 = 0.0118, NDCG@10 = 0.0049, alpha=NA, beta=NA, time = 2.86s\n",
            "current best hr@10=0.0118 @ epoch 7.\n",
            "Best result info saved to ./records/validation mode/best_result.txt\n",
            "[Epoch 08/50] avg InBatch Softmax Loss = 6.0196, HR@10 = 0.0135, NDCG@10 = 0.0057, alpha=NA, beta=NA, time = 3.40s\n",
            "current best hr@10=0.0135 @ epoch 8.\n",
            "Best result info saved to ./records/validation mode/best_result.txt\n",
            "[Epoch 09/50] avg InBatch Softmax Loss = 5.7311, HR@10 = 0.0159, NDCG@10 = 0.0070, alpha=NA, beta=NA, time = 2.92s\n",
            "current best hr@10=0.0159 @ epoch 9.\n",
            "Best result info saved to ./records/validation mode/best_result.txt\n",
            "[Epoch 10/50] avg InBatch Softmax Loss = 5.4674, HR@10 = 0.0188, NDCG@10 = 0.0084, alpha=NA, beta=NA, time = 4.57s\n",
            "current best hr@10=0.0188 @ epoch 10.\n",
            "Best result info saved to ./records/validation mode/best_result.txt\n",
            "[Epoch 11/50] avg InBatch Softmax Loss = 5.2466, HR@10 = 0.0199, NDCG@10 = 0.0087, alpha=NA, beta=NA, time = 2.91s\n",
            "current best hr@10=0.0199 @ epoch 11.\n",
            "Best result info saved to ./records/validation mode/best_result.txt\n",
            "[Epoch 12/50] avg InBatch Softmax Loss = 5.0634, HR@10 = 0.0214, NDCG@10 = 0.0092, alpha=NA, beta=NA, time = 3.60s\n",
            "current best hr@10=0.0214 @ epoch 12.\n",
            "Best result info saved to ./records/validation mode/best_result.txt\n",
            "[Epoch 13/50] avg InBatch Softmax Loss = 4.9139, HR@10 = 0.0208, NDCG@10 = 0.0088, alpha=NA, beta=NA, time = 2.96s\n",
            "[Epoch 14/50] avg InBatch Softmax Loss = 4.7900, HR@10 = 0.0232, NDCG@10 = 0.0098, alpha=NA, beta=NA, time = 2.88s\n",
            "current best hr@10=0.0232 @ epoch 14.\n",
            "Best result info saved to ./records/validation mode/best_result.txt\n",
            "[Epoch 15/50] avg InBatch Softmax Loss = 4.6895, HR@10 = 0.0234, NDCG@10 = 0.0100, alpha=NA, beta=NA, time = 3.09s\n",
            "current best hr@10=0.0234 @ epoch 15.\n",
            "Best result info saved to ./records/validation mode/best_result.txt\n",
            "[Epoch 16/50] avg InBatch Softmax Loss = 4.6045, HR@10 = 0.0244, NDCG@10 = 0.0104, alpha=NA, beta=NA, time = 2.92s\n",
            "current best hr@10=0.0244 @ epoch 16.\n",
            "Best result info saved to ./records/validation mode/best_result.txt\n",
            "[Epoch 17/50] avg InBatch Softmax Loss = 4.5317, HR@10 = 0.0237, NDCG@10 = 0.0101, alpha=NA, beta=NA, time = 3.25s\n",
            "[Epoch 18/50] avg InBatch Softmax Loss = 4.4732, HR@10 = 0.0240, NDCG@10 = 0.0099, alpha=NA, beta=NA, time = 2.94s\n",
            "[Epoch 19/50] avg InBatch Softmax Loss = 4.4163, HR@10 = 0.0248, NDCG@10 = 0.0103, alpha=NA, beta=NA, time = 3.38s\n",
            "current best hr@10=0.0248 @ epoch 19.\n",
            "Best result info saved to ./records/validation mode/best_result.txt\n",
            "[Epoch 20/50] avg InBatch Softmax Loss = 4.3751, HR@10 = 0.0250, NDCG@10 = 0.0105, alpha=NA, beta=NA, time = 2.88s\n",
            "current best hr@10=0.0250 @ epoch 20.\n",
            "Best result info saved to ./records/validation mode/best_result.txt\n",
            "[Epoch 21/50] avg InBatch Softmax Loss = 4.3324, HR@10 = 0.0254, NDCG@10 = 0.0107, alpha=NA, beta=NA, time = 3.53s\n",
            "current best hr@10=0.0254 @ epoch 21.\n",
            "Best result info saved to ./records/validation mode/best_result.txt\n",
            "[Epoch 22/50] avg InBatch Softmax Loss = 4.2929, HR@10 = 0.0250, NDCG@10 = 0.0104, alpha=NA, beta=NA, time = 3.21s\n",
            "[Epoch 23/50] avg InBatch Softmax Loss = 4.2585, HR@10 = 0.0242, NDCG@10 = 0.0099, alpha=NA, beta=NA, time = 3.11s\n",
            "[Epoch 24/50] avg InBatch Softmax Loss = 4.2258, HR@10 = 0.0261, NDCG@10 = 0.0108, alpha=NA, beta=NA, time = 2.91s\n",
            "current best hr@10=0.0261 @ epoch 24.\n",
            "Best result info saved to ./records/validation mode/best_result.txt\n",
            "[Epoch 25/50] avg InBatch Softmax Loss = 4.1972, HR@10 = 0.0250, NDCG@10 = 0.0104, alpha=NA, beta=NA, time = 3.01s\n",
            "[Epoch 26/50] avg InBatch Softmax Loss = 4.1720, HR@10 = 0.0251, NDCG@10 = 0.0104, alpha=NA, beta=NA, time = 2.94s\n",
            "[Epoch 27/50] avg InBatch Softmax Loss = 4.1468, HR@10 = 0.0258, NDCG@10 = 0.0106, alpha=NA, beta=NA, time = 2.95s\n",
            "[Epoch 28/50] avg InBatch Softmax Loss = 4.1184, HR@10 = 0.0261, NDCG@10 = 0.0108, alpha=NA, beta=NA, time = 2.87s\n",
            "current best hr@10=0.0261 @ epoch 28.\n",
            "Best result info saved to ./records/validation mode/best_result.txt\n",
            "[Epoch 29/50] avg InBatch Softmax Loss = 4.0959, HR@10 = 0.0257, NDCG@10 = 0.0105, alpha=NA, beta=NA, time = 2.89s\n",
            "[Epoch 30/50] avg InBatch Softmax Loss = 4.0764, HR@10 = 0.0255, NDCG@10 = 0.0105, alpha=NA, beta=NA, time = 2.90s\n",
            "[Epoch 31/50] avg InBatch Softmax Loss = 4.0539, HR@10 = 0.0264, NDCG@10 = 0.0107, alpha=NA, beta=NA, time = 2.88s\n",
            "current best hr@10=0.0264 @ epoch 31.\n",
            "Best result info saved to ./records/validation mode/best_result.txt\n",
            "[Epoch 32/50] avg InBatch Softmax Loss = 4.0372, HR@10 = 0.0261, NDCG@10 = 0.0105, alpha=NA, beta=NA, time = 3.39s\n",
            "[Epoch 33/50] avg InBatch Softmax Loss = 4.0122, HR@10 = 0.0246, NDCG@10 = 0.0100, alpha=NA, beta=NA, time = 2.90s\n",
            "[Epoch 34/50] avg InBatch Softmax Loss = 3.9989, HR@10 = 0.0250, NDCG@10 = 0.0101, alpha=NA, beta=NA, time = 3.21s\n",
            "[Epoch 35/50] avg InBatch Softmax Loss = 3.9799, HR@10 = 0.0250, NDCG@10 = 0.0101, alpha=NA, beta=NA, time = 2.93s\n",
            "[Epoch 36/50] avg InBatch Softmax Loss = 3.9672, HR@10 = 0.0256, NDCG@10 = 0.0104, alpha=NA, beta=NA, time = 3.20s\n",
            "Early stopping triggered.\n",
            "Saved ./records/validation mode/curve_loss.png\n",
            "Saved ./records/validation mode/curve_metrics.png\n",
            "Best hr@10=0.0264 at epoch 31\n",
            "[EarlyStopping] monitor=hr@10 , patience=5\n",
            "[Epoch 01/50] avg InBatch Softmax Loss = 9.5133, HR@10 = 0.0006, NDCG@10 = 0.0003, alpha=NA, beta=NA, time = 3.46s\n",
            "current best hr@10=0.0006 @ epoch 1.\n",
            "Best result info saved to ./records/train mode/best_result.txt\n",
            "[Epoch 02/50] avg InBatch Softmax Loss = 7.9195, HR@10 = 0.0007, NDCG@10 = 0.0003, alpha=NA, beta=NA, time = 4.09s\n",
            "current best hr@10=0.0007 @ epoch 2.\n",
            "Best result info saved to ./records/train mode/best_result.txt\n",
            "[Epoch 03/50] avg InBatch Softmax Loss = 7.2755, HR@10 = 0.0004, NDCG@10 = 0.0002, alpha=NA, beta=NA, time = 3.52s\n",
            "[Epoch 04/50] avg InBatch Softmax Loss = 7.0308, HR@10 = 0.0005, NDCG@10 = 0.0002, alpha=NA, beta=NA, time = 4.14s\n",
            "[Epoch 05/50] avg InBatch Softmax Loss = 6.8508, HR@10 = 0.0049, NDCG@10 = 0.0022, alpha=NA, beta=NA, time = 3.76s\n",
            "current best hr@10=0.0049 @ epoch 5.\n",
            "Best result info saved to ./records/train mode/best_result.txt\n",
            "[Epoch 06/50] avg InBatch Softmax Loss = 6.6095, HR@10 = 0.0087, NDCG@10 = 0.0037, alpha=NA, beta=NA, time = 3.45s\n",
            "current best hr@10=0.0087 @ epoch 6.\n",
            "Best result info saved to ./records/train mode/best_result.txt\n",
            "[Epoch 07/50] avg InBatch Softmax Loss = 6.3582, HR@10 = 0.0134, NDCG@10 = 0.0057, alpha=NA, beta=NA, time = 3.89s\n",
            "current best hr@10=0.0134 @ epoch 7.\n",
            "Best result info saved to ./records/train mode/best_result.txt\n",
            "[Epoch 08/50] avg InBatch Softmax Loss = 6.0948, HR@10 = 0.0126, NDCG@10 = 0.0055, alpha=NA, beta=NA, time = 3.46s\n",
            "[Epoch 09/50] avg InBatch Softmax Loss = 5.8426, HR@10 = 0.0153, NDCG@10 = 0.0071, alpha=NA, beta=NA, time = 4.08s\n",
            "current best hr@10=0.0153 @ epoch 9.\n",
            "Best result info saved to ./records/train mode/best_result.txt\n",
            "[Epoch 10/50] avg InBatch Softmax Loss = 5.6242, HR@10 = 0.0167, NDCG@10 = 0.0083, alpha=NA, beta=NA, time = 3.45s\n",
            "current best hr@10=0.0167 @ epoch 10.\n",
            "Best result info saved to ./records/train mode/best_result.txt\n",
            "[Epoch 11/50] avg InBatch Softmax Loss = 5.4343, HR@10 = 0.0188, NDCG@10 = 0.0081, alpha=NA, beta=NA, time = 4.06s\n",
            "current best hr@10=0.0188 @ epoch 11.\n",
            "Best result info saved to ./records/train mode/best_result.txt\n",
            "[Epoch 12/50] avg InBatch Softmax Loss = 5.2799, HR@10 = 0.0201, NDCG@10 = 0.0088, alpha=NA, beta=NA, time = 3.48s\n",
            "current best hr@10=0.0201 @ epoch 12.\n",
            "Best result info saved to ./records/train mode/best_result.txt\n",
            "[Epoch 13/50] avg InBatch Softmax Loss = 5.1550, HR@10 = 0.0205, NDCG@10 = 0.0086, alpha=NA, beta=NA, time = 3.65s\n",
            "current best hr@10=0.0205 @ epoch 13.\n",
            "Best result info saved to ./records/train mode/best_result.txt\n",
            "[Epoch 14/50] avg InBatch Softmax Loss = 5.0527, HR@10 = 0.0219, NDCG@10 = 0.0097, alpha=NA, beta=NA, time = 3.50s\n",
            "current best hr@10=0.0219 @ epoch 14.\n",
            "Best result info saved to ./records/train mode/best_result.txt\n",
            "[Epoch 15/50] avg InBatch Softmax Loss = 4.9682, HR@10 = 0.0229, NDCG@10 = 0.0099, alpha=NA, beta=NA, time = 3.85s\n",
            "current best hr@10=0.0229 @ epoch 15.\n",
            "Best result info saved to ./records/train mode/best_result.txt\n",
            "[Epoch 16/50] avg InBatch Softmax Loss = 4.8971, HR@10 = 0.0213, NDCG@10 = 0.0094, alpha=NA, beta=NA, time = 4.38s\n",
            "[Epoch 17/50] avg InBatch Softmax Loss = 4.8383, HR@10 = 0.0224, NDCG@10 = 0.0094, alpha=NA, beta=NA, time = 3.46s\n",
            "[Epoch 18/50] avg InBatch Softmax Loss = 4.7892, HR@10 = 0.0232, NDCG@10 = 0.0101, alpha=NA, beta=NA, time = 4.15s\n",
            "current best hr@10=0.0232 @ epoch 18.\n",
            "Best result info saved to ./records/train mode/best_result.txt\n",
            "[Epoch 19/50] avg InBatch Softmax Loss = 4.7396, HR@10 = 0.0255, NDCG@10 = 0.0109, alpha=NA, beta=NA, time = 3.53s\n",
            "current best hr@10=0.0255 @ epoch 19.\n",
            "Best result info saved to ./records/train mode/best_result.txt\n",
            "[Epoch 20/50] avg InBatch Softmax Loss = 4.6984, HR@10 = 0.0239, NDCG@10 = 0.0101, alpha=NA, beta=NA, time = 3.90s\n",
            "[Epoch 21/50] avg InBatch Softmax Loss = 4.6640, HR@10 = 0.0248, NDCG@10 = 0.0105, alpha=NA, beta=NA, time = 3.47s\n",
            "[Epoch 22/50] avg InBatch Softmax Loss = 4.6286, HR@10 = 0.0250, NDCG@10 = 0.0106, alpha=NA, beta=NA, time = 3.45s\n",
            "[Epoch 23/50] avg InBatch Softmax Loss = 4.5993, HR@10 = 0.0242, NDCG@10 = 0.0104, alpha=NA, beta=NA, time = 3.67s\n",
            "[Epoch 24/50] avg InBatch Softmax Loss = 4.5695, HR@10 = 0.0240, NDCG@10 = 0.0102, alpha=NA, beta=NA, time = 3.45s\n",
            "Early stopping triggered.\n",
            "Saved ./records/train mode/curve_loss.png\n",
            "Saved ./records/train mode/curve_metrics.png\n",
            "Best hr@10=0.0255 at epoch 19\n"
          ]
        }
      ],
      "execution_count": 12
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-07-12T14:02:03.509536Z",
          "start_time": "2025-07-12T14:01:55.208715Z"
        },
        "id": "d626cca926d2732f"
      },
      "cell_type": "code",
      "source": [
        "model.save_embeddings(num_users=num_users,num_items=num_items,device=device,save_dir=save_dir)"
      ],
      "id": "d626cca926d2732f",
      "outputs": [],
      "execution_count": 13
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-07-12T14:02:11.793103Z",
          "start_time": "2025-07-12T14:02:03.541546Z"
        },
        "id": "8be6dec3309abad7"
      },
      "cell_type": "code",
      "source": [
        "test_loader = customdataset.build_test_loader(test_df, num_items ,user_col = user_id, item_col = item_id, batch_size=1024, num_workers=NUM_WORKERS)\n",
        "item_pool = list(range(num_items))\n",
        "faiss_index = faiss.read_index(f\"{save_dir}/item_index.faiss\")"
      ],
      "id": "8be6dec3309abad7",
      "outputs": [],
      "execution_count": 14
    },
    {
      "metadata": {
        "id": "79cc0d7cfa3e17c2",
        "outputId": "af968c6a-e980-44ae-8fc5-e44b6816c417",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random HR@10 = 0.0005, NDCG@10 = 0.0002\n",
            "Popular HR@10 = 0.0030, NDCG@10 = 0.0014\n",
            "Model   HR@10 = 0.0240, NDCG@10 = 0.0102\n"
          ]
        }
      ],
      "execution_count": 15,
      "source": [
        "hr_r, ndcg_r = evaluate.evaluate_random(test_loader, item_pool ,top_k=TOP_K)\n",
        "print(f\"Random HR@{TOP_K} = {hr_r:.4f}, NDCG@{TOP_K} = {ndcg_r:.4f}\")\n",
        "hr_p, ndcg_p = evaluate.evaluate_popular(test_loader, train_all_df,top_k=TOP_K)\n",
        "print(f\"Popular HR@{TOP_K} = {hr_p:.4f}, NDCG@{TOP_K} = {ndcg_p:.4f}\")\n",
        "hr_m, ndcg_m = evaluate.evaluate_model(test_loader, model, faiss_index, device,top_k=TOP_K)\n",
        "print(f\"Model   HR@{TOP_K} = {hr_m:.4f}, NDCG@{TOP_K} = {ndcg_m:.4f}\")\n"
      ],
      "id": "79cc0d7cfa3e17c2"
    },
    {
      "metadata": {
        "id": "301476e764e49dbc",
        "outputId": "3cfb5622-0b4b-4eb3-b6ec-8a7b654be7ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "execution_count": 16,
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 挂载 Google Drive\n",
        "drive.mount('/content/drive')\n",
        "# 目标路径\n",
        "target_dir = None\n",
        "if(FUSION_MODE==\"base\"):\n",
        "    target_dir = f\"/content/drive/MyDrive/REC/{PROJECT_NAME}/{FUSION_MODE}/\"\n",
        "else:\n",
        "    target_dir = f\"/content/drive/MyDrive/REC/{PROJECT_NAME}/{FUSION_MODE}/{CURRENT_MODAL}\"\n",
        "# 创建目标路径（包含上层目录）\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "# 复制 records 到目标路径\n",
        "!cp -r /content/records \"{target_dir}\"\n",
        "!rm -rf /content/records"
      ],
      "id": "301476e764e49dbc"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
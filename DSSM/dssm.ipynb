{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "import shutil\n",
    "import os\n",
    "def copy_from_drive(src_path, dst_path):\n",
    "\n",
    "    if os.path.exists(dst_path):\n",
    "        print(f\"skip:{dst_path} exists\")\n",
    "        return\n",
    "\n",
    "    if os.path.isdir(src_path):\n",
    "        shutil.copytree(src_path, dst_path)\n",
    "    elif os.path.isfile(src_path):\n",
    "        shutil.copy(src_path, dst_path)\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "copy_from_drive('/content/drive/MyDrive/tool', '/content/tool')\n",
    "copy_from_drive('/content/drive/MyDrive/MicroLens-50k_pairs.csv','/content/MicroLens-50k_pairs.csv')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S3TTppBdUSal",
    "outputId": "35c5aa90-e97c-43fd-a8b4-16ef5dfab10b"
   },
   "id": "S3TTppBdUSal",
   "execution_count": 72,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "skip:/content/tool exists\n",
      "skip:/content/MicroLens-50k_pairs.csv exists\n"
     ]
    }
   ]
  },
  {
   "metadata": {
    "id": "2cf7762f790b03a9",
    "outputId": "970c3850-e701-4ac2-d638-f40abfb0ef06",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: lmdb in /usr/local/lib/python3.11/dist-packages (1.7.2)\n"
     ]
    }
   ],
   "execution_count": 73,
   "source": [
    "!pip install faiss-cpu\n",
    "!pip install lmdb\n",
    "from tool import preprocess\n",
    "from tool import customdataset\n",
    "from tool import evaluate\n",
    "import faiss\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime"
   ],
   "id": "2cf7762f790b03a9"
  },
  {
   "metadata": {
    "id": "79b35d503cf0a9e2",
    "outputId": "19155e24-ee60-4cf7-f838-1e9eb54990a4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function tool.preprocess.set_seed.<locals>.seed_worker(worker_id)>"
      ],
      "text/html": [
       "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
       "      pre.function-repr-contents {\n",
       "        overflow-x: auto;\n",
       "        padding: 8px 12px;\n",
       "        max-height: 500px;\n",
       "      }\n",
       "\n",
       "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
       "        cursor: pointer;\n",
       "        max-height: 100px;\n",
       "      }\n",
       "    </style>\n",
       "    <pre style=\"white-space: initial; background:\n",
       "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
       "         border-bottom: 1px solid var(--colab-border-color);\"><b>tool.preprocess.set_seed.&lt;locals&gt;.seed_worker</b><br/>def seed_worker(worker_id)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/tool/preprocess.py</a>&lt;no docstring&gt;</pre>\n",
       "      <script>\n",
       "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
       "        for (const element of document.querySelectorAll('.filepath')) {\n",
       "          element.style.display = 'block'\n",
       "          element.onclick = (event) => {\n",
       "            event.preventDefault();\n",
       "            event.stopPropagation();\n",
       "            google.colab.files.view(element.textContent, 18);\n",
       "          };\n",
       "        }\n",
       "      }\n",
       "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
       "        element.onclick = (event) => {\n",
       "          event.preventDefault();\n",
       "          event.stopPropagation();\n",
       "          element.classList.toggle('function-repr-contents-collapsed');\n",
       "        };\n",
       "      }\n",
       "      </script>\n",
       "      </div>"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "execution_count": 74,
   "source": [
    "preprocess.set_seed(42)"
   ],
   "id": "79b35d503cf0a9e2"
  },
  {
   "metadata": {
    "id": "d97293349e89d19f"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 75,
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "id": "d97293349e89d19f"
  },
  {
   "metadata": {
    "id": "7577a987b7a2c3c0"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 76,
   "source": [
    "path = 'MicroLens-50k_pairs.csv'\n",
    "user = 'user'\n",
    "item = 'item'\n",
    "user_id = 'user_id'\n",
    "item_id = 'item_id'\n",
    "timestamp = 'timestamp'\n",
    "save_dir = './embeddings'\n",
    "top_k = 10\n",
    "num_workers = 10\n",
    "k_neg = 10\n",
    "L2_NORM = False\n",
    "# path = pd.read_csv('MicroLens-50k_pairs.csv')"
   ],
   "id": "7577a987b7a2c3c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T13:58:50.578506Z",
     "start_time": "2025-07-12T13:58:50.287146Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de02a54222d2235",
    "outputId": "50c831a3-4fec-4de7-90e3-bbb63c68ddf9"
   },
   "cell_type": "code",
   "source": [
    "dataset_pd,num_users,num_items = preprocess.openAndSort(path,user_id=user,item_id=item,timestamp='timestamp')"
   ],
   "id": "de02a54222d2235",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dataset base informationï¼š\n",
      "- number of usersï¼š50000\n",
      "- number of itemsï¼š19220\n",
      "- number of rowsï¼š359708\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "id": "3db1146761e45165",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T13:58:50.861004Z",
     "start_time": "2025-07-12T13:58:50.639579Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3db1146761e45165",
    "outputId": "0c81ee6c-8544-4481-ff60-eaec30968b43"
   },
   "source": [
    "\n",
    "train_df, test_df = preprocess.split(dataset_pd,user, item, timestamp)\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Test size: {len(test_df)}\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train size: 309708\n",
      "Test size: 49424\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "cell_type": "code",
   "id": "ce079817",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T13:58:51.882961Z",
     "start_time": "2025-07-12T13:58:50.882539Z"
    },
    "id": "ce079817"
   },
   "source": [
    "# maintain a map from new id to old id, new id for constructing matrix\n",
    "user2id = {u: i for i, u in enumerate(dataset_pd[user].unique())}\n",
    "item2id = {i: j for j, i in enumerate(dataset_pd[item].unique())}\n",
    "\n",
    "# apply to train_df and test_df\n",
    "train_df[user_id] = train_df[user].map(user2id)\n",
    "train_df[item_id] = train_df[item].map(item2id)\n",
    "test_df[user_id] = test_df[user].map(user2id)\n",
    "test_df[item_id] = test_df[item].map(item2id)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "id": "9c4d28e9e285faa9"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 80,
   "source": [
    "# DSSM implementation in PyTorch\n",
    "\n",
    "\n",
    "class DSSM(nn.Module):\n",
    "    \"\"\"\n",
    "    åŒå¡” DSSMï¼šç”¨æˆ·å¡” + ç‰©å“å¡”\n",
    "    hidden_dims å¦‚ [128, 64]ï¼Œæœ€åè¾“å‡ºç»´åº¦ = hidden_dims[-1]\n",
    "    \"\"\"\n",
    "    def __init__(self, num_users, num_items, emb_dim=128, mlp_hidden_size=[128, 64, 32], dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, emb_dim)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_dim)\n",
    "\n",
    "        # ğŸ”¸ åˆå§‹åŒ–ï¼ˆå¯é€‰ä½†æ¨èï¼‰\n",
    "        nn.init.xavier_uniform_(self.user_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.item_emb.weight)\n",
    "                # æ„å»º MLP å±‚\n",
    "        self.mlp_user = self.build_mlp(emb_dim, mlp_hidden_size, dropout)\n",
    "        self.mlp_item = self.build_mlp(emb_dim, mlp_hidden_size, dropout)\n",
    "\n",
    "    def build_mlp(self, input_dim, hidden_sizes, dropout):\n",
    "          layers = []\n",
    "          for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(input_dim, h))\n",
    "            layers.append(nn.BatchNorm1d(h))\n",
    "            layers.append(nn.Tanh())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            input_dim = h\n",
    "          return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, user_id, item_id, l2_norm=L2_NORM):\n",
    "        \"\"\"\n",
    "        è¿”å›:\n",
    "          score: (B,) ç‚¹ç§¯åˆ†æ•°\n",
    "          u_vec, i_vec: (B, d) ä¸¤ä¾§å‘é‡\n",
    "        \"\"\"\n",
    "        u = self.user_emb(user_id)          # (B, emb_dim)\n",
    "        i = self.item_emb(item_id)          # (B, emb_dim)\n",
    "\n",
    "        u_vec = self.mlp_user(u)            # (B, d)\n",
    "        i_vec = self.mlp_item(i)            # (B, d)\n",
    "\n",
    "        if l2_norm:\n",
    "            u_vec = F.normalize(u_vec, p=2, dim=1)\n",
    "            i_vec = F.normalize(i_vec, p=2, dim=1)\n",
    "\n",
    "        return u_vec, i_vec\n",
    "    def get_users_embedding(self,user_ids,l2_norm=L2_NORM):\n",
    "        u = self.user_emb(user_ids)          # (B, emb_dim)\n",
    "\n",
    "        u_vec = self.mlp_user(u)            # (B, d)\n",
    "\n",
    "        if l2_norm:\n",
    "            u_vec = F.normalize(u_vec, p=2, dim=1)\n",
    "        return u_vec\n",
    "    def get_items_embedding(self,item_ids,l2_norm=L2_NORM):\n",
    "        i = self.item_emb(item_ids)          # (B, emb_dim)\n",
    "\n",
    "        i_vec = self.mlp_item(i)            # (B, d)\n",
    "\n",
    "        if l2_norm:\n",
    "            i_vec = F.normalize(i_vec, p=2, dim=1)\n",
    "        return i_vec\n",
    "\n",
    "    def save_embeddings(self, num_users, num_items, device, save_dir='./embeddings',l2_norm = L2_NORM):\n",
    "        import os\n",
    "        import faiss\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        self.eval()\n",
    "        self.to(device)\n",
    "\n",
    "        user_ids = torch.arange(num_users, dtype=torch.long, device=device)\n",
    "        item_ids = torch.arange(num_items, dtype=torch.long, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            user_embeds = self.get_users_embedding(user_ids, l2_norm=l2_norm)\n",
    "            item_embeds = self.get_items_embedding(item_ids, l2_norm=l2_norm)\n",
    "\n",
    "        user_embeds = user_embeds.cpu().numpy().astype(np.float32)\n",
    "        item_embeds = item_embeds.cpu().numpy().astype(np.float32)\n",
    "\n",
    "        # ä¿å­˜å‘é‡\n",
    "        np.save(f\"{save_dir}/user_embeddings.npy\", user_embeds)\n",
    "        np.save(f\"{save_dir}/item_embeddings.npy\", item_embeds)\n",
    "\n",
    "        # æ„å»º FAISS indexï¼ˆä½¿ç”¨å†…ç§¯ï¼‰\n",
    "        dim = item_embeds.shape[1]\n",
    "        index = faiss.IndexFlatIP(dim)\n",
    "        index.add(item_embeds)\n",
    "\n",
    "        faiss.write_index(index, f\"{save_dir}/item_index.faiss\")\n",
    "        print(\"Saved user/item embeddings and FAISS index.\")\n",
    "\n"
   ],
   "id": "9c4d28e9e285faa9"
  },
  {
   "metadata": {
    "id": "f80a999c1ccde5e8"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 81,
   "source": [
    "\n",
    "def train_model(model,train_df,\n",
    "                epochs=10,\n",
    "                batch_size=64,\n",
    "                lr=1e-3,\n",
    "                device=None):\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # ä½ éœ€è¦ä¸€ä¸ª data_loader è¿”å› (user_id, pos_item_id) å¯¹ï¼Œæ— è´Ÿæ ·æœ¬\n",
    "    train_loader = customdataset.build_train_loader_inbatch(train_df, batch_size=batch_size,user_col=user_id, item_col=item_id)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        dt_start = datetime.now()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            user_ids, pos_item_ids = batch\n",
    "            user_ids = user_ids.to(device)\n",
    "            pos_item_ids = pos_item_ids.to(device)\n",
    "\n",
    "            # 1. å‰å‘ä¼ æ’­ï¼ˆè¿”å› user / item å‘é‡ï¼‰\n",
    "            u_vec, i_vec = model(user_ids, pos_item_ids, l2_norm=L2_NORM)\n",
    "\n",
    "            # 2. å¾—åˆ†çŸ©é˜µï¼šæ¯ä¸ª user å¯¹æ‰€æœ‰æ­£ item çš„æ‰“åˆ†\n",
    "            logits = torch.matmul(u_vec, i_vec.T)  # shape: (B, B)\n",
    "\n",
    "            # 3. æ„é€ æ ‡ç­¾ï¼šæ¯ä¸ª user çš„æ­£ç¡® item åœ¨å¯¹è§’çº¿ï¼ˆå³ä½ç½® iï¼‰\n",
    "            labels = torch.arange(logits.size(0), device=device)  # [0, 1, ..., B-1]\n",
    "\n",
    "            # 4. Cross Entropy Loss\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "            # 5. åå‘ä¼ æ’­\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # æ—¥å¿—\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        dt_end = datetime.now()\n",
    "        dt = dt_end - dt_start\n",
    "\n",
    "        print(f\"[Epoch {epoch:02d}/{epochs}] avg InBatch Softmax Loss = {avg_loss:.4f}, time = {dt.total_seconds():.2f}s\")\n",
    "\n",
    "    return\n",
    "\n",
    "\n"
   ],
   "id": "f80a999c1ccde5e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T14:01:55.008421Z",
     "start_time": "2025-07-12T14:01:54.939903Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "848b63e397b62c61",
    "outputId": "1b385fc0-348d-4f44-ea92-584765955541"
   },
   "cell_type": "code",
   "source": [
    "model = DSSM(num_users,num_items,emb_dim=128)\n",
    "model.to(device)\n",
    "train_model(model=model,epochs=50, train_df=train_df,batch_size=1024)"
   ],
   "id": "848b63e397b62c61",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved user/item embeddings and FAISS index.\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T14:02:03.509536Z",
     "start_time": "2025-07-12T14:01:55.208715Z"
    },
    "id": "d626cca926d2732f"
   },
   "cell_type": "code",
   "source": "model.save_embeddings(num_users=num_users,num_items=num_items,device=device,save_dir=save_dir)",
   "id": "d626cca926d2732f",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T14:02:11.793103Z",
     "start_time": "2025-07-12T14:02:03.541546Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8be6dec3309abad7",
    "outputId": "a7633ab3-7d13-4f6f-d42c-eae829f37bbe"
   },
   "cell_type": "code",
   "source": [
    "test_loader = customdataset.build_test_loader(test_df, num_items ,user_col = user_id, item_col = item_id, batch_size=1024, num_workers=num_workers)\n",
    "item_pool = list(range(num_items))\n",
    "faiss_index = faiss.read_index(f\"{save_dir}/item_index.faiss\")"
   ],
   "id": "8be6dec3309abad7",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Random HR@10 = 0.0005, nDCG@10 = 0.0002\n",
      "Popular HR@10 = 0.0029, nDCG@10 = 0.0014\n",
      "Model   HR@10 = 0.0241, nDCG@10 = 0.0104\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "hr_r, ndcg_r = evaluate.evaluate_random(test_loader, item_pool ,top_k=top_k)\n",
    "print(f\"Random HR@{top_k} = {hr_r:.4f}, nDCG@{top_k} = {ndcg_r:.4f}\")\n",
    "hr_p, ndcg_p = evaluate.evaluate_popular(test_loader, train_df,top_k=top_k)\n",
    "print(f\"Popular HR@{top_k} = {hr_p:.4f}, nDCG@{top_k} = {ndcg_p:.4f}\")\n",
    "hr_m, ndcg_m = evaluate.evaluate_model(test_loader, model, faiss_index, device,top_k=top_k)\n",
    "print(f\"Model   HR@{top_k} = {hr_m:.4f}, nDCG@{top_k} = {ndcg_m:.4f}\")\n"
   ],
   "id": "79cc0d7cfa3e17c2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "colab": {
   "provenance": [],
   "gpuType": "L4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

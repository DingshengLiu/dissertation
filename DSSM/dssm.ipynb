{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T13:58:47.060557Z",
     "start_time": "2025-07-12T13:58:39.865457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from setuptools.sandbox import save_argv\n",
    "\n",
    "from tool import preprocess\n",
    "from tool import customdataset"
   ],
   "id": "672e53594437abc",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "b828e981",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T13:58:50.021886Z",
     "start_time": "2025-07-12T13:58:47.066569Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from networkx.readwrite.json_graph import adjacency\n",
    "import faiss\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "e99a31348e0a553",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T13:58:50.234186Z",
     "start_time": "2025-07-12T13:58:50.180933Z"
    }
   },
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T13:58:50.267980Z",
     "start_time": "2025-07-12T13:58:50.261090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path = 'D:\\\\VideoRecSystem\\\\MicroLens\\\\DataSet\\\\MicroLens-50k_pairs.csv'\n",
    "user = 'user'\n",
    "item = 'item'\n",
    "user_id = 'user_id'\n",
    "item_id = 'item_id'\n",
    "timestamp = 'timestamp'\n",
    "save_dir = './embeddings'\n",
    "top_k = 10\n",
    "# path = pd.read_csv('MicroLens-50k_pairs.csv')"
   ],
   "id": "b20989f1541fb122",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T13:58:50.578506Z",
     "start_time": "2025-07-12T13:58:50.287146Z"
    }
   },
   "cell_type": "code",
   "source": "dataset_pd,num_users,num_items = preprocess.openAndSort(path,user_id=user,item_id=item,timestamp='timestamp')",
   "id": "de02a54222d2235",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset base informationÔºö\n",
      "- number of usersÔºö50000\n",
      "- number of itemsÔºö19220\n",
      "- number of rowsÔºö359708\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "8b8ae0c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T13:58:50.601335Z",
     "start_time": "2025-07-12T13:58:50.596244Z"
    }
   },
   "source": [
    "# # order by user,timestamp\n",
    "# filtered_df = dataset_pd.sort_values(by=[\"user\", \"timestamp\"])\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "2d2d4de256fd88a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T13:58:50.630564Z",
     "start_time": "2025-07-12T13:58:50.625626Z"
    }
   },
   "source": [
    "# def split(df, user_id, item_id, timestamp):\n",
    "#\n",
    "#     # Ëé∑ÂèñÊØè‰∏™Áî®Êà∑ÁöÑÊúÄÂêé‰∏ÄÊù°ËÆ∞ÂΩï‰Ωú‰∏∫ test\n",
    "#     test_df = df.groupby(user_id).tail(1)\n",
    "#     train_df = df.drop(index=test_df.index)\n",
    "#\n",
    "#     # ËøáÊª§ test ‰∏≠ÈÇ£‰∫õ user/item ‰∏çÂú® train ‰∏≠ÁöÑ\n",
    "#     train_users = set(train_df[user_id])\n",
    "#     train_items = set(train_df[item_id])\n",
    "#\n",
    "#     # Á°Æ‰øùÊµãËØïÈõÜ‰∏≠Âá∫Áé∞ÁöÑÁî®Êà∑/Áâ©ÂìÅÈÉΩÂú®ËÆ≠ÁªÉÈõÜ‰∏≠Âá∫Áé∞ËøáÔºåÈÅøÂÖçÊüê‰∏™Áâ©ÂìÅ‰ªÖÂá∫Áé∞Âú®ÊµãËØïÈõÜ‰∏≠ÔºåÊ≤°ÊúâÂú®ËÆ≠ÁªÉÈõÜ‰∏≠ÂæóÂà∞ËøáËÆ≠ÁªÉ\n",
    "#     test_df = test_df[\n",
    "#         test_df[user_id].isin(train_users) &\n",
    "#         test_df[item_id].isin(train_items)\n",
    "#     ]\n",
    "#     # .reset_indexÈáçÁΩÆ df ÁöÑÁ¥¢ÂºïÔºå‰ΩøÂæó‰∏çËøûÁª≠ÁöÑÁ¥¢ÂºïÈáçÊñ∞ÊéíÂàóÊï¥ÈΩêÔºådrop=TrueË°®ÊòéÊóßÁöÑÁ¥¢Âºï‰∏çÂÜç‰øùÁïô\n",
    "#     return train_df.reset_index(drop=True), test_df.reset_index(drop=True)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "3db1146761e45165",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T13:58:50.861004Z",
     "start_time": "2025-07-12T13:58:50.639579Z"
    }
   },
   "source": [
    "\n",
    "train_df, test_df = preprocess.split(dataset_pd,user, item, timestamp)\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Test size: {len(test_df)}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 309708\n",
      "Test size: 49424\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "ce079817",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T13:58:51.882961Z",
     "start_time": "2025-07-12T13:58:50.882539Z"
    }
   },
   "source": [
    "# maintain a map from new id to old id, new id for constructing matrix\n",
    "user2id = {u: i for i, u in enumerate(dataset_pd[user].unique())}\n",
    "item2id = {i: j for j, i in enumerate(dataset_pd[item].unique())}\n",
    "\n",
    "# apply to train_df and test_df\n",
    "train_df[user_id] = train_df[user].map(user2id)\n",
    "train_df[item_id] = train_df[item].map(item2id)\n",
    "test_df[user_id] = test_df[user].map(user2id)\n",
    "test_df[item_id] = test_df[item].map(item2id)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "a7aba75c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T13:58:51.926849Z",
     "start_time": "2025-07-12T13:58:51.911489Z"
    }
   },
   "source": [
    "# DSSM implementation in PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class DSSM(nn.Module):\n",
    "    \"\"\"\n",
    "    ÂèåÂ°î DSSMÔºöÁî®Êà∑Â°î + Áâ©ÂìÅÂ°î\n",
    "    hidden_dims Â¶Ç [128, 64]ÔºåÊúÄÂêéËæìÂá∫Áª¥Â∫¶ = hidden_dims[-1]\n",
    "    \"\"\"\n",
    "    def __init__(self, num_users, num_items, emb_dim=64, hidden_dims=(128, 64), dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, emb_dim)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_dim)\n",
    "\n",
    "        # üî∏ ÂàùÂßãÂåñÔºàÂèØÈÄâ‰ΩÜÊé®ËçêÔºâ\n",
    "        nn.init.xavier_uniform_(self.user_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.item_emb.weight)\n",
    "\n",
    "        def _build_mlp(in_dim, hidden_dims):\n",
    "            layers = []\n",
    "            for h in hidden_dims:\n",
    "                layers += [nn.Linear(in_dim, h),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(dropout)]\n",
    "                in_dim = h\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        self.mlp_user = _build_mlp(emb_dim, hidden_dims)\n",
    "        self.mlp_item = _build_mlp(emb_dim, hidden_dims)\n",
    "\n",
    "    def forward(self, user_id, item_id, l2_norm=True):\n",
    "        \"\"\"\n",
    "        ËøîÂõû:\n",
    "          score: (B,) ÁÇπÁßØÂàÜÊï∞\n",
    "          u_vec, i_vec: (B, d) ‰∏§‰æßÂêëÈáè\n",
    "        \"\"\"\n",
    "        u = self.user_emb(user_id)          # (B, emb_dim)\n",
    "        i = self.item_emb(item_id)          # (B, emb_dim)\n",
    "\n",
    "        u_vec = self.mlp_user(u)            # (B, d)\n",
    "        i_vec = self.mlp_item(i)            # (B, d)\n",
    "\n",
    "        if l2_norm:\n",
    "            u_vec = F.normalize(u_vec, p=2, dim=1)\n",
    "            i_vec = F.normalize(i_vec, p=2, dim=1)\n",
    "\n",
    "        score = (u_vec * i_vec).sum(dim=1)  # (B,)\n",
    "        return score, u_vec, i_vec\n",
    "    def get_users_embedding(self,user_ids,l2_norm=True):\n",
    "        u = self.user_emb(user_ids)          # (B, emb_dim)\n",
    "\n",
    "        u_vec = self.mlp_user(u)            # (B, d)\n",
    "\n",
    "        if l2_norm:\n",
    "            u_vec = F.normalize(u_vec, p=2, dim=1)\n",
    "        return u_vec\n",
    "    def get_items_embedding(self,item_ids,l2_norm=True):\n",
    "        i = self.item_emb(item_ids)          # (B, emb_dim)\n",
    "\n",
    "        i_vec = self.mlp_item(i)            # (B, d)\n",
    "\n",
    "        if l2_norm:\n",
    "            i_vec = F.normalize(i_vec, p=2, dim=1)\n",
    "        return i_vec\n",
    "\n",
    "    def save_embeddings(self, num_users, num_items, device, save_dir='./embeddings'):\n",
    "        import os\n",
    "        import faiss\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        self.eval()\n",
    "        self.to(device)\n",
    "\n",
    "        user_ids = torch.arange(num_users, dtype=torch.long, device=device)\n",
    "        item_ids = torch.arange(num_items, dtype=torch.long, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            user_embeds = self.get_users_embedding(user_ids, l2_norm=True)\n",
    "            item_embeds = self.get_items_embedding(item_ids, l2_norm=True)\n",
    "\n",
    "        user_embeds = user_embeds.cpu().numpy().astype(np.float32)\n",
    "        item_embeds = item_embeds.cpu().numpy().astype(np.float32)\n",
    "\n",
    "        # ‰øùÂ≠òÂêëÈáè\n",
    "        np.save(f\"{save_dir}/user_embeddings.npy\", user_embeds)\n",
    "        np.save(f\"{save_dir}/item_embeddings.npy\", item_embeds)\n",
    "\n",
    "        # ÊûÑÂª∫ FAISS indexÔºà‰ΩøÁî®ÂÜÖÁßØÔºâ\n",
    "        dim = item_embeds.shape[1]\n",
    "        index = faiss.IndexFlatIP(dim)\n",
    "        index.add(item_embeds)\n",
    "\n",
    "        faiss.write_index(index, f\"{save_dir}/item_index.faiss\")\n",
    "        print(\"Saved user/item embeddings and FAISS index.\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "07e268c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T13:58:51.967812Z",
     "start_time": "2025-07-12T13:58:51.955879Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train_model(model,\n",
    "                train_df,\n",
    "                num_items,\n",
    "                epochs=10,\n",
    "                batch_size=64,\n",
    "                lr=1e-3,\n",
    "                print_every=1,\n",
    "                max_grad_norm=None,\n",
    "                device=None):\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    train_data_loader = customdataset.build_train_loader(train_df,num_items,user_id, item_id, batch_size=batch_size ,k_neg=1,num_workers=1)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()         # train model\n",
    "        dt_start = datetime.now()\n",
    "        epoch_loss = 0.0\n",
    "        for batch in train_data_loader:\n",
    "\n",
    "            # 1.Ë¥üÈááÊ†∑\n",
    "            user_ids, pos_ids, neg_ids = batch\n",
    "            # 2.Êê¨ËÆæÂ§á\n",
    "            user_ids = user_ids.to(device)   # Â∑≤ÁªèÊòØ LongTensor\n",
    "            pos_ids  = pos_ids.to(device)\n",
    "            neg_ids  = neg_ids.to(device)\n",
    "\n",
    "                # -------- Â±ïÂπ≥Ë¥üÊ†∑Êú¨ --------\n",
    "            if neg_ids.ndim == 1:          # ÂçïË¥üÊ†∑Êú¨\n",
    "                B = neg_ids.size(0)\n",
    "                u_rep   = user_ids\n",
    "                neg_flat = neg_ids\n",
    "                K = 1\n",
    "            else:                          # Â§öË¥üÊ†∑Êú¨\n",
    "                B, K = neg_ids.shape\n",
    "                u_rep   = user_ids.unsqueeze(1).expand(-1, K).reshape(-1)\n",
    "                neg_flat = neg_ids.reshape(-1)\n",
    "                # Êää user Â±ïÂπ≥ÈáçÂ§ç K Ê¨°ÔºåÂÜç‰∏éÂ±ïÂπ≥ÂêéÁöÑ neg_id ÂØπÈΩê\n",
    "                u_rep   = user_ids.unsqueeze(1).expand(-1, K).reshape(-1)   # (B*K,)\n",
    "                neg_flat = neg_ids.reshape(-1)                              # (B*K,)\n",
    "\n",
    "\n",
    "            # 3.ËÆ°ÁÆóÂæóÂàÜ\n",
    "            neg_score, _, _ = model(u_rep, neg_flat)                    # (B*K,)\n",
    "            neg_score = neg_score.view(B, K)                            # (B, K)\n",
    "            pos_score, _, _ = model(user_ids, pos_ids)                  # (B,)\n",
    "\n",
    "            if neg_ids.ndim == 1:                    # K = 1\n",
    "             loss = -torch.log(torch.sigmoid(pos_score - neg_score)).mean()\n",
    "            else:                                    # K > 1\n",
    "             loss = -torch.log(torch.sigmoid(pos_score.unsqueeze(1) - neg_score)).mean()\n",
    "\n",
    "            # 5.ÂèçÂêë & Êõ¥Êñ∞\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            if max_grad_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # ‚Äî‚Äî epoch ÁªìÊùüÔºåÊâìÂç∞Êó•Âøó ‚Äî‚Äî\n",
    "        avg_loss = epoch_loss / len(train_data_loader)\n",
    "        dt_end = datetime.now()\n",
    "        dt = dt_end - dt_start\n",
    "\n",
    "        print(f\"[Epoch {epoch:02d}/{epochs}]  avg BPR Loss = {avg_loss:.4f},takes {dt.total_seconds()}s\")\n",
    "\n",
    "    return\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "cd29a40c68841c9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T14:01:54.878124Z",
     "start_time": "2025-07-12T13:58:51.995794Z"
    }
   },
   "source": [
    "model = DSSM(num_users,num_items,emb_dim=64)\n",
    "model.to(device)\n",
    "train_model(model=model,epochs=2, train_df=train_df,num_items=num_items,batch_size=512)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 01/2]  avg BPR Loss = 0.6892,takes 90.085939s\n",
      "[Epoch 02/2]  avg BPR Loss = 0.6789,takes 86.551551s\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T14:01:55.008421Z",
     "start_time": "2025-07-12T14:01:54.939903Z"
    }
   },
   "cell_type": "code",
   "source": "model.save_embeddings(num_users=num_users,num_items=num_items,device=device,save_dir=save_dir)",
   "id": "848b63e397b62c61",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved user/item embeddings and FAISS index.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T14:01:55.022942Z",
     "start_time": "2025-07-12T14:01:55.015944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def hit_rate_at_k(ranked_list, true_item):\n",
    "    return int(true_item in ranked_list)"
   ],
   "id": "25398aa2359bb59a",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T14:01:55.088537Z",
     "start_time": "2025-07-12T14:01:55.082268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ndcg_at_k(ranked_list, true_item):\n",
    "    if true_item in ranked_list:\n",
    "        index = ranked_list.index(true_item)\n",
    "        return 1 / np.log2(index + 2)\n",
    "    else:\n",
    "        return 0.0"
   ],
   "id": "e836833fc9dd8385",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T14:01:55.101683Z",
     "start_time": "2025-07-12T14:01:55.094548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_random(test_loader, item_pool, top_k=10):\n",
    "    hits, ndcgs = [], []\n",
    "    for _, item_batch in test_loader:\n",
    "        item_batch = item_batch.numpy()\n",
    "        for true_item in item_batch:\n",
    "            # ‰ªéÂÖ®‰ΩìÁâ©ÂìÅ‰∏≠ÈöèÊú∫ÊäΩ top_k ‰∏™ÔºàÂåÖÂê´Êàñ‰∏çÂåÖÂê´ true_itemÔºâ\n",
    "            rec_list = random.sample(item_pool, top_k)\n",
    "            hits.append(hit_rate_at_k(rec_list, true_item))\n",
    "            ndcgs.append(ndcg_at_k(rec_list, true_item))\n",
    "    return np.mean(hits), np.mean(ndcgs)\n"
   ],
   "id": "a784ef14493efab3",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T14:01:55.139911Z",
     "start_time": "2025-07-12T14:01:55.131997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def evaluate_popular(test_loader, train_df, top_k=10):\n",
    "    item_counts = Counter(train_df['item_id'].values)\n",
    "    popular_items = [item for item, _ in item_counts.most_common(top_k)]\n",
    "\n",
    "    hits, ndcgs = [], []\n",
    "    for _, item_batch in test_loader:\n",
    "        item_batch = item_batch.numpy()\n",
    "        for true_item in item_batch:\n",
    "            hits.append(hit_rate_at_k(popular_items, true_item))\n",
    "            ndcgs.append(ndcg_at_k(popular_items, true_item))\n",
    "    return np.mean(hits), np.mean(ndcgs)"
   ],
   "id": "2fd356468126edd2",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T14:01:55.180169Z",
     "start_time": "2025-07-12T14:01:55.170652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_model(test_loader, model, faiss_index, device, top_k=10):\n",
    "    hits, ndcgs = [], []\n",
    "    model.eval()\n",
    "\n",
    "    for user_batch, item_batch in test_loader:\n",
    "        user_batch = user_batch.to(device)\n",
    "        item_batch = item_batch.cpu().numpy()  # true items\n",
    "\n",
    "        with torch.no_grad():\n",
    "            user_vecs = model.get_users_embedding(user_batch)\n",
    "            user_vecs = user_vecs.cpu().numpy().astype(np.float32)\n",
    "\n",
    "        # FAISS ÊâπÈáè topK\n",
    "        _, I = faiss_index.search(user_vecs, top_k)  # (B, K)\n",
    "        topk_lists = I.tolist()\n",
    "\n",
    "        for rec_list, true_item in zip(topk_lists, item_batch):\n",
    "            hits.append(hit_rate_at_k(rec_list, true_item))\n",
    "            ndcgs.append(ndcg_at_k(rec_list, true_item))\n",
    "\n",
    "    return np.mean(hits), np.mean(ndcgs)"
   ],
   "id": "bd73712d1405d906",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T14:02:03.509536Z",
     "start_time": "2025-07-12T14:01:55.208715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_loader = customdataset.build_test_loader(test_df, num_items ,user_col = user_id, item_col = item_id, batch_size=64)\n",
    "item_pool = list(range(num_items))\n",
    "faiss_index = faiss.read_index(f\"{save_dir}/item_index.faiss\")\n",
    "\n",
    "hr_r, ndcg_r = evaluate_random(test_loader, item_pool ,top_k=top_k)\n",
    "\n",
    "print(f\"Random HR@{top_k} = {hr_r:.4f}, nDCG@{top_k} = {ndcg_r:.4f}\")\n"
   ],
   "id": "d626cca926d2732f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random HR@10 = 0.0006, nDCG@10 = 0.0002\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T14:02:11.793103Z",
     "start_time": "2025-07-12T14:02:03.541546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hr_p, ndcg_p = evaluate_popular(test_loader, train_df,top_k=top_k)\n",
    "print(f\"Popular HR@{top_k} = {hr_p:.4f}, nDCG@{top_k} = {ndcg_p:.4f}\")\n"
   ],
   "id": "8be6dec3309abad7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Popular HR@10 = 0.0029, nDCG@10 = 0.0014\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T14:02:26.356147Z",
     "start_time": "2025-07-12T14:02:11.828289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hr_m, ndcg_m = evaluate_model(test_loader, model, faiss_index, device,top_k=top_k)\n",
    "print(f\"Model   HR@{top_k} = {hr_m:.4f}, nDCG@{top_k} = {ndcg_m:.4f}\")\n"
   ],
   "id": "744d21ae291f0a70",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model   HR@10 = 0.0017, nDCG@10 = 0.0007\n"
     ]
    }
   ],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DP1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

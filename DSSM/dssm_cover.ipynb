{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "import shutil\n",
    "import os\n",
    "def copy_from_drive(src_path, dst_path):\n",
    "\n",
    "    if os.path.exists(dst_path):\n",
    "        print(f\"skip:{dst_path} exists\")\n",
    "        return\n",
    "\n",
    "    if os.path.isdir(src_path):\n",
    "        shutil.copytree(src_path, dst_path)\n",
    "    elif os.path.isfile(src_path):\n",
    "        shutil.copy(src_path, dst_path)\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "copy_from_drive('/content/drive/MyDrive/tool', '/content/tool')\n",
    "copy_from_drive('/content/drive/MyDrive/MicroLens-50k_pairs.csv','/content/MicroLens-50k_pairs.csv')\n",
    "copy_from_drive('/content/drive/MyDrive/cover_emb128.lmdb','/content/cover_emb128.lmdb')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S3TTppBdUSal",
    "outputId": "d113c9e2-05a4-415c-ebfd-1075817b980a"
   },
   "id": "S3TTppBdUSal",
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install faiss-cpu\n",
    "!pip install lmdb\n",
    "from tool import preprocess\n",
    "from tool import customdataset\n",
    "from tool import evaluate\n",
    "import faiss\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n"
   ],
   "id": "13eb6594b2620bb2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "preprocess.set_seed(42)",
   "id": "23b32d6818cff5cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
   "id": "e4778a621548558e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "path = 'MicroLens-50k_pairs.csv'\n",
    "lmdb_path = 'cover_emb128.lmdb'\n",
    "lmdb_dim = 128\n",
    "user = 'user'\n",
    "item = 'item'\n",
    "user_id = 'user_id'\n",
    "item_id = 'item_id'\n",
    "timestamp = 'timestamp'\n",
    "save_dir = './embeddings'\n",
    "top_k = 10\n",
    "num_workers = 10\n",
    "k_neg = 10\n",
    "# path = pd.read_csv('MicroLens-50k_pairs.csv')"
   ],
   "id": "a0b7b4e132a082e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T13:58:50.578506Z",
     "start_time": "2025-07-12T13:58:50.287146Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de02a54222d2235",
    "outputId": "eb18cefc-a03f-4251-aaeb-4f2cae37e92d"
   },
   "cell_type": "code",
   "source": [
    "dataset_pd,num_users,num_items = preprocess.openAndSort(path,user_id=user,item_id=item,timestamp='timestamp')"
   ],
   "id": "de02a54222d2235",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dataset base informationï¼š\n",
      "- number of usersï¼š50000\n",
      "- number of itemsï¼š19220\n",
      "- number of rowsï¼š359708\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "3db1146761e45165",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T13:58:50.861004Z",
     "start_time": "2025-07-12T13:58:50.639579Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3db1146761e45165",
    "outputId": "e062c4f4-0ca2-4866-e745-9930edf529f7"
   },
   "source": [
    "\n",
    "train_df, test_df = preprocess.split(dataset_pd,user, item, timestamp)\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Test size: {len(test_df)}\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train size: 309708\n",
      "Test size: 49424\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "ce079817",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T13:58:51.882961Z",
     "start_time": "2025-07-12T13:58:50.882539Z"
    },
    "id": "ce079817"
   },
   "source": [
    "# maintain a map from new id to old id, new id for constructing matrix\n",
    "user2id = {u: i for i, u in enumerate(dataset_pd[user].unique())}\n",
    "item2id = {i: j for j, i in enumerate(dataset_pd[item].unique())}\n",
    "\n",
    "# apply to train_df and test_df\n",
    "train_df[user_id] = train_df[user].map(user2id)\n",
    "train_df[item_id] = train_df[item].map(item2id)\n",
    "test_df[user_id] = test_df[user].map(user2id)\n",
    "test_df[item_id] = test_df[item].map(item2id)\n",
    "\n",
    "# 1. æ„å»º item_id åˆ° item çš„æ˜ å°„ï¼ˆæ¥è‡ª train_dfï¼‰\n",
    "item_id_to_item = dict(zip(train_df['item_id'], train_df['item']))"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# DSSM implementation in PyTorch\n",
    "\n",
    "\n",
    "class DSSM(nn.Module):\n",
    "    \"\"\"\n",
    "    åŒå¡” DSSMï¼šç”¨æˆ·å¡” + ç‰©å“å¡”\n",
    "    hidden_dims å¦‚ [128, 64]ï¼Œæœ€åè¾“å‡ºç»´åº¦ = hidden_dims[-1]\n",
    "    \"\"\"\n",
    "    def __init__(self, num_users, num_items, emb_dim=128, mlp_hidden_size=[128, 64, 32], dropout=0.2 ,cover_hidden_size=[128], cover_dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, emb_dim)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_dim)\n",
    "\n",
    "        # ğŸ”¸ åˆå§‹åŒ–ï¼ˆå¯é€‰ä½†æ¨èï¼‰\n",
    "        nn.init.xavier_uniform_(self.user_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.item_emb.weight)\n",
    "\n",
    "        # load cover_emb_tensor from lmdb\n",
    "        self.cover_emb_tensor = preprocess.load_tensor_from_lmdb(lmdb_path,num_items,item_id_to_item,lmdb_dim)\n",
    "\n",
    "        # æ³¨å†Œä¸º bufferï¼Œè¡¨ç¤ºè¯¥å‚æ•°ä¸å‚ä¸æ¢¯åº¦æ›´æ–°\n",
    "        self.register_buffer('frozen_extra_emb', self.cover_emb_tensor)\n",
    "\n",
    "\n",
    "        # æ„å»º MLP å±‚\n",
    "        self.mlp_user = self.build_mlp(emb_dim, mlp_hidden_size, dropout)\n",
    "        self.mlp_item = self.build_mlp(emb_dim, mlp_hidden_size, dropout)\n",
    "\n",
    "        # å°†item+coveræ˜ å°„å›itemåŸæœ‰çš„ç»´åº¦\n",
    "        self.mlp_item_cover = self.build_mlp(emb_dim+lmdb_dim, cover_hidden_size, cover_dropout)\n",
    "\n",
    "    def build_mlp(self, input_dim, hidden_sizes, dropout):\n",
    "          layers = []\n",
    "          for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(input_dim, h))\n",
    "            layers.append(nn.BatchNorm1d(h))\n",
    "            layers.append(nn.Tanh())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            input_dim = h\n",
    "          return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, user_id, item_id, l2_norm=True):\n",
    "        \"\"\"\n",
    "        è¿”å›:\n",
    "          score: (B,) ç‚¹ç§¯åˆ†æ•°\n",
    "          u_vec, i_vec: (B, d) ä¸¤ä¾§å‘é‡\n",
    "        \"\"\"\n",
    "        u = self.user_emb(user_id)          # (B, emb_dim)\n",
    "        i = self.item_emb(item_id)          # (B, emb_dim)\n",
    "        cover = self.cover_emb_tensor[item_id]\n",
    "        i = torch.cat([i, cover], dim=-1) # (B, emb_dim+lmdb_dim)\n",
    "        i = self.mlp_item_cover(i)          # (B, emb_dim)\n",
    "        u_vec = self.mlp_user(u)            # (B, d)\n",
    "        i_vec = self.mlp_item(i)            # (B, d)\n",
    "\n",
    "        if l2_norm:\n",
    "            u_vec = F.normalize(u_vec, p=2, dim=1)\n",
    "            i_vec = F.normalize(i_vec, p=2, dim=1)\n",
    "\n",
    "        return u_vec, i_vec\n",
    "    def get_users_embedding(self,user_ids,l2_norm=True):\n",
    "        u = self.user_emb(user_ids)          # (B, emb_dim)\n",
    "\n",
    "        u_vec = self.mlp_user(u)            # (B, d)\n",
    "\n",
    "        if l2_norm:\n",
    "            u_vec = F.normalize(u_vec, p=2, dim=1)\n",
    "        return u_vec\n",
    "\n",
    "    def get_items_embedding(self, item_ids, l2_norm=True, use_cover=True):\n",
    "        i = self.item_emb(item_ids)\n",
    "        if use_cover:\n",
    "            cover = self.cover_emb_tensor[item_ids]\n",
    "            i = torch.cat([i, cover], dim=-1)\n",
    "            i = self.mlp_item_cover(i)\n",
    "        i_vec = self.mlp_item(i)\n",
    "\n",
    "        if l2_norm:\n",
    "            i_vec = F.normalize(i_vec, p=2, dim=1)\n",
    "        return i_vec\n",
    "\n",
    "    def save_embeddings(self, num_users, num_items, device, save_dir='./embeddings'):\n",
    "        import os\n",
    "        import faiss\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        self.eval()\n",
    "        self.to(device)\n",
    "\n",
    "        user_ids = torch.arange(num_users, dtype=torch.long, device=device)\n",
    "        item_ids = torch.arange(num_items, dtype=torch.long, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            user_embeds = self.get_users_embedding(user_ids, l2_norm=True)\n",
    "            item_embeds = self.get_items_embedding(item_ids, l2_norm=True)\n",
    "\n",
    "        user_embeds = user_embeds.cpu().numpy().astype(np.float32)\n",
    "        item_embeds = item_embeds.cpu().numpy().astype(np.float32)\n",
    "\n",
    "        # ä¿å­˜å‘é‡\n",
    "        np.save(f\"{save_dir}/user_embeddings.npy\", user_embeds)\n",
    "        np.save(f\"{save_dir}/item_embeddings.npy\", item_embeds)\n",
    "\n",
    "        # æ„å»º FAISS indexï¼ˆä½¿ç”¨å†…ç§¯ï¼‰\n",
    "        dim = item_embeds.shape[1]\n",
    "        index = faiss.IndexFlatIP(dim)\n",
    "        index.add(item_embeds)\n",
    "\n",
    "        faiss.write_index(index, f\"{save_dir}/item_index.faiss\")\n",
    "        print(\"Saved user/item embeddings and FAISS index.\")\n",
    "\n"
   ],
   "id": "ee0a975eddbe0e72"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def train_model(model,train_df,\n",
    "                epochs=10,\n",
    "                batch_size=64,\n",
    "                lr=1e-3,\n",
    "                device=None):\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # ä½ éœ€è¦ä¸€ä¸ª data_loader è¿”å› (user_id, pos_item_id) å¯¹ï¼Œæ— è´Ÿæ ·æœ¬\n",
    "    train_loader = customdataset.build_train_loader_inbatch(train_df, batch_size=batch_size,user_col=user_id, item_col=item_id)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        dt_start = datetime.now()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            user_ids, pos_item_ids = batch\n",
    "            user_ids = user_ids.to(device)\n",
    "            pos_item_ids = pos_item_ids.to(device)\n",
    "\n",
    "            # 1. å‰å‘ä¼ æ’­ï¼ˆè¿”å› user / item å‘é‡ï¼‰\n",
    "            u_vec, i_vec = model(user_ids, pos_item_ids, l2_norm=False)\n",
    "\n",
    "            # 2. å¾—åˆ†çŸ©é˜µï¼šæ¯ä¸ª user å¯¹æ‰€æœ‰æ­£ item çš„æ‰“åˆ†\n",
    "            logits = torch.matmul(u_vec, i_vec.T)  # shape: (B, B)\n",
    "\n",
    "            # 3. æ„é€ æ ‡ç­¾ï¼šæ¯ä¸ª user çš„æ­£ç¡® item åœ¨å¯¹è§’çº¿ï¼ˆå³ä½ç½® iï¼‰\n",
    "            labels = torch.arange(logits.size(0), device=device)  # [0, 1, ..., B-1]\n",
    "\n",
    "            # 4. Cross Entropy Loss\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "            # 5. åå‘ä¼ æ’­\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # æ—¥å¿—\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        dt_end = datetime.now()\n",
    "        dt = dt_end - dt_start\n",
    "\n",
    "        print(f\"[Epoch {epoch:02d}/{epochs}] avg InBatch Softmax Loss = {avg_loss:.4f}, time = {dt.total_seconds():.2f}s\")\n",
    "\n",
    "    return\n",
    "\n",
    "\n"
   ],
   "id": "8c24b4c547f30d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = DSSM(num_users,num_items,emb_dim=128)\n",
    "model.to(device)\n",
    "train_model(model=model,epochs=50, train_df=train_df,batch_size=1024)"
   ],
   "id": "c580d6a283147382"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T14:01:55.008421Z",
     "start_time": "2025-07-12T14:01:54.939903Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "848b63e397b62c61",
    "outputId": "44bfea28-88d7-4ee7-f723-9a29a336634b"
   },
   "cell_type": "code",
   "source": [
    "model.save_embeddings(num_users=num_users,num_items=num_items,device=device,save_dir=save_dir)"
   ],
   "id": "848b63e397b62c61",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved user/item embeddings and FAISS index.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T14:02:03.509536Z",
     "start_time": "2025-07-12T14:01:55.208715Z"
    },
    "id": "d626cca926d2732f"
   },
   "cell_type": "code",
   "source": [
    "test_loader = customdataset.build_test_loader(test_df, num_items ,user_col = user_id, item_col = item_id, batch_size=1024, num_workers=num_workers)\n",
    "item_pool = list(range(num_items))\n",
    "faiss_index = faiss.read_index(f\"{save_dir}/item_index.faiss\")"
   ],
   "id": "d626cca926d2732f",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T14:02:11.793103Z",
     "start_time": "2025-07-12T14:02:03.541546Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8be6dec3309abad7",
    "outputId": "94b0bc4a-bc33-4459-e594-937497af12bc"
   },
   "cell_type": "code",
   "source": [
    "hr_r, ndcg_r = evaluate.evaluate_random(test_loader, item_pool ,top_k=top_k)\n",
    "print(f\"Random HR@{top_k} = {hr_r:.4f}, nDCG@{top_k} = {ndcg_r:.4f}\")\n",
    "hr_p, ndcg_p = evaluate.evaluate_popular(test_loader, train_df,top_k=top_k)\n",
    "print(f\"Popular HR@{top_k} = {hr_p:.4f}, nDCG@{top_k} = {ndcg_p:.4f}\")\n",
    "hr_m, ndcg_m = evaluate.evaluate_model(test_loader, model, faiss_index, device,top_k=top_k)\n",
    "print(f\"Model   HR@{top_k} = {hr_m:.4f}, nDCG@{top_k} = {ndcg_m:.4f}\")\n"
   ],
   "id": "8be6dec3309abad7",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Random HR@10 = 0.0005, nDCG@10 = 0.0003\n",
      "Popular HR@10 = 0.0029, nDCG@10 = 0.0014\n",
      "Model   HR@10 = 0.0245, nDCG@10 = 0.0106\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

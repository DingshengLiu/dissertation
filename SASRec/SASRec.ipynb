{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from google.colab import drive\n",
    "import shutil\n",
    "import os\n",
    "def copy_from_drive(src_path, dst_path):\n",
    "\n",
    "    if os.path.exists(dst_path):\n",
    "        print(f\"skip:{dst_path} exists\")\n",
    "        return\n",
    "\n",
    "    if os.path.isdir(src_path):\n",
    "        shutil.copytree(src_path, dst_path)\n",
    "    elif os.path.isfile(src_path):\n",
    "        shutil.copy(src_path, dst_path)\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "copy_from_drive('/content/drive/MyDrive/tool', '/content/tool')\n",
    "copy_from_drive('/content/drive/MyDrive/MicroLens-50k_pairs.csv','/content/MicroLens-50k_pairs.csv')"
   ],
   "id": "df5fadd3b5922093"
  },
  {
   "cell_type": "code",
   "id": "b828e981",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T18:42:28.345525Z",
     "start_time": "2025-06-12T18:42:18.772256Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from networkx.readwrite.json_graph import adjacency\n",
    "import random, math, time, os\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from tool import preprocess\n",
    "from tool import customdataset\n",
    "from tool import evaluate\n",
    "!pip install faiss-cpu\n",
    "import faiss\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "e99a31348e0a553",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T18:42:28.396608Z",
     "start_time": "2025-06-12T18:42:28.351537Z"
    }
   },
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "76a2087d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T18:42:29.259369Z",
     "start_time": "2025-06-12T18:42:29.105143Z"
    }
   },
   "source": [
    "path = 'MicroLens-50k_pairs.csv'\n",
    "user = 'user'\n",
    "item = 'item'\n",
    "user_id = 'user_id'\n",
    "item_id = 'item_id'\n",
    "timestamp = 'timestamp'\n",
    "save_dir = './embeddings'\n",
    "top_k = 10\n",
    "num_workers = 10\n",
    "k_neg = 10\n",
    "# path = pd.read_csv('MicroLens-50k_pairs.csv')"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "943fc6c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T18:42:29.290418Z",
     "start_time": "2025-06-12T18:42:29.273675Z"
    }
   },
   "source": "dataset_pd,num_users,num_items = preprocess.openAndSort(path,user_id=user,item_id=item,timestamp='timestamp')",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    user   item      timestamp\n",
       "0  36121   9580  1583378629552\n",
       "1  26572   9580  1583436719018\n",
       "2  37550   9580  1584412681021\n",
       "3  14601   9580  1584848802432\n",
       "4  15061   9580  1585388171106\n",
       "5   6364   9580  1585390736041\n",
       "6   3542   9580  1585404918503\n",
       "7  21038   9580  1590144594477\n",
       "8  12538  14631  1634867362929\n",
       "9  47592  14631  1634872254913"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36121</td>\n",
       "      <td>9580</td>\n",
       "      <td>1583378629552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26572</td>\n",
       "      <td>9580</td>\n",
       "      <td>1583436719018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37550</td>\n",
       "      <td>9580</td>\n",
       "      <td>1584412681021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14601</td>\n",
       "      <td>9580</td>\n",
       "      <td>1584848802432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15061</td>\n",
       "      <td>9580</td>\n",
       "      <td>1585388171106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6364</td>\n",
       "      <td>9580</td>\n",
       "      <td>1585390736041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3542</td>\n",
       "      <td>9580</td>\n",
       "      <td>1585404918503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>21038</td>\n",
       "      <td>9580</td>\n",
       "      <td>1590144594477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12538</td>\n",
       "      <td>14631</td>\n",
       "      <td>1634867362929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>47592</td>\n",
       "      <td>14631</td>\n",
       "      <td>1634872254913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_df, test_df = preprocess.split(dataset_pd,user, item, timestamp)\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Test size: {len(test_df)}\")"
   ],
   "id": "c1952473721f3f99"
  },
  {
   "cell_type": "code",
   "id": "69160caa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T18:42:29.358698Z",
     "start_time": "2025-06-12T18:42:29.349731Z"
    }
   },
   "source": [
    "# maintain a map from new id to old id, new id for constructing matrix\n",
    "user2id = {u: i for i, u in enumerate(dataset_pd[user].unique())}\n",
    "item2id = {i: j for j, i in enumerate(dataset_pd[item].unique())}\n",
    "\n",
    "# apply to train_df and test_df\n",
    "train_df[user_id] = train_df[user].map(user2id)\n",
    "train_df[item_id] = train_df[item].map(item2id)\n",
    "test_df[user_id] = test_df[user].map(user2id)\n",
    "test_df[item_id] = test_df[item].map(item2id)\n",
    "\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.count of          user   item      timestamp\n",
       "0       36121   9580  1583378629552\n",
       "1       26572   9580  1583436719018\n",
       "2       37550   9580  1584412681021\n",
       "3       14601   9580  1584848802432\n",
       "4       15061   9580  1585388171106\n",
       "...       ...    ...            ...\n",
       "359703  48702   1363  1662984066647\n",
       "359704  27203   7291  1662984082974\n",
       "359705  29261  19649  1662984103874\n",
       "359706  28341  19188  1662984123833\n",
       "359707  38967   7254  1662984132429\n",
       "\n",
       "[359708 rows x 3 columns]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T18:42:31.415745Z",
     "start_time": "2025-06-12T18:42:31.391214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------- 超参数 ----------\n",
    "MAX_SEQ_LEN   = 20          # 序列长度\n",
    "EMBEDDING_DIM = 64          # item / user embedding 维度\n",
    "N_HEADS       = 2           # Multi-Head Attention 头数\n",
    "N_LAYERS      = 2           # Transformer block 层数\n",
    "DROPOUT       = 0.2\n",
    "NEG_SAMPLE    = 5\n",
    "BATCH_SIZE    = 1024\n",
    "EPOCHS        = 10\n",
    "LR            = 1e-3\n",
    "SEED          = 42\n",
    "# ----------------------------\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# ---------- 常量（来自你已有变量） ----------\n",
    "PAD_IDX = num_items          # 专用 padding id\n",
    "N_ITEMS = num_items + 1      # Embedding 行数（含 PAD）\n",
    "\n",
    "# -------------------------------------------\n"
   ],
   "id": "6e56658ac26b07e2",
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "a7aba75c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T18:42:31.455790Z",
     "start_time": "2025-06-12T18:42:31.445397Z"
    }
   },
   "source": [
    "# ======== SASRec 模型 ======== #\n",
    "class SASRec(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_items=N_ITEMS,\n",
    "                 embedding_dim=EMBEDDING_DIM,\n",
    "                 n_heads=N_HEADS,\n",
    "                 n_layers=N_LAYERS,\n",
    "                 max_len=MAX_SEQ_LEN,\n",
    "                 pad_idx=PAD_IDX,\n",
    "                 dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_items, embedding_dim, padding_idx=pad_idx)\n",
    "        self.pos_emb  = nn.Embedding(max_len, embedding_dim)\n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=embedding_dim*4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation='gelu')\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "\n",
    "        # 为 causal mask 预生成上三角矩阵\n",
    "        self.register_buffer(\n",
    "            \"mask\", torch.triu(torch.ones(max_len, max_len), diagonal=1).bool())\n",
    "\n",
    "    def forward(self, seq):                       # seq : (B,T)\n",
    "        B, T = seq.size()\n",
    "        item_e = self.embedding(seq)               # (B,T,D)\n",
    "        pos_ids = torch.arange(T, device=seq.device).unsqueeze(0).expand(B, -1)\n",
    "        x = item_e + self.pos_emb(pos_ids)        # 加位置编码\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # causal attention mask\n",
    "        causal_mask = self.mask[:T, :T]\n",
    "        x = self.encoder(x, src_key_padding_mask=(seq == PAD_IDX), mask=causal_mask)\n",
    "        h = x[:, -1, :]                           # 取最后位置向量 (B,D)\n",
    "        return h\n",
    "\n",
    "    def get_items_embedding(self,item_ids,l2_norm=True):\n",
    "        i_vec = self.embedding(item_ids)          # (B, emb_dim)\n",
    "        if l2_norm:\n",
    "            i_vec = F.normalize(i_vec, p=2, dim=1)\n",
    "        return i_vec\n",
    "\n",
    "    def save_embeddings(self, num_users, num_items, device, save_dir='./embeddings'):\n",
    "        import os\n",
    "        import faiss\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        self.eval()\n",
    "        self.to(device)\n",
    "\n",
    "        item_ids = torch.arange(num_items, dtype=torch.long, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            item_embeds = self.get_items_embedding(item_ids, l2_norm=True)\n",
    "\n",
    "        item_embeds = item_embeds.cpu().numpy().astype(np.float32)\n",
    "\n",
    "        # 保存向量\n",
    "        np.save(f\"{save_dir}/item_embeddings.npy\", item_embeds)\n",
    "\n",
    "        # 构建 FAISS index（使用内积）\n",
    "        dim = item_embeds.shape[1]\n",
    "        index = faiss.IndexFlatIP(dim)\n",
    "        index.add(item_embeds)\n",
    "\n",
    "        faiss.write_index(index, f\"{save_dir}/item_index.faiss\")\n",
    "        print(\"Saved user/item embeddings and FAISS index.\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "07e268c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T18:42:31.532424Z",
     "start_time": "2025-06-12T18:42:31.522660Z"
    }
   },
   "source": [
    "# ======== 训练流程 ======== #\n",
    "def train_model(model, train_df, epochs,lr , batch_size, test_df=None, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "    train_loader  = customdataset.build_seq_neg_loader(train_df, batch_size=batch_size,\n",
    "                         shuffle=True, num_workers=10,pad_idx=PAD_IDX,max_len=MAX_SEQ_LEN,user_id=user,item_id=item_id,k_neg=1,neg_sampling='pop',alpha=0.75,num_items=num_items)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        dt_start = datetime.now()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "\n",
    "            # 1.负采样\n",
    "            hist_ids, pos_ids, neg_ids = batch\n",
    "            # 2.搬设备\n",
    "            hist_ids = hist_ids.to(device)   # 已经是 LongTensor\n",
    "            pos_ids  = pos_ids.to(device)\n",
    "            neg_ids  = neg_ids.to(device)\n",
    "\n",
    "            # 3.Forward ---- #\n",
    "            user_emb = model(hist_ids)                      # (B, d)\n",
    "            pos_emb  = model.get_items_embedding(pos_ids)              # (B, d)\n",
    "            pos_score = (user_emb * pos_emb).sum(-1)        # (B,)\n",
    "\n",
    "            neg_emb  = model.get_items_embedding(neg_ids)              # (B, k, d)\n",
    "            neg_score = (user_emb.unsqueeze(1) * neg_emb).sum(-1)   # (B, k)\n",
    "\n",
    "            # 4.BPR (pairwise) Loss ---- #\n",
    "            if k_neg == 1:\n",
    "                loss = -F.logsigmoid(pos_score - neg_score.squeeze(1)).mean()\n",
    "            else:\n",
    "                loss = -F.logsigmoid(pos_score.unsqueeze(1) - neg_score).mean()\n",
    "\n",
    "            # 5. 反向传播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # 日志\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        dt_end = datetime.now()\n",
    "        dt = dt_end - dt_start\n",
    "\n",
    "        print(f\"[Epoch {epoch:02d}/{epochs}] avg InBatch Softmax Loss = {avg_loss:.4f}, time = {dt.total_seconds():.2f}s\")\n",
    "\n",
    "    return\n"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T18:42:31.567359Z",
     "start_time": "2025-06-12T18:42:31.561346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_hist_matrix(df,\n",
    "                      num_users,\n",
    "                      max_len=MAX_SEQ_LEN,\n",
    "                      pad_idx=PAD_IDX,\n",
    "                      user_col=user_id,\n",
    "                      item_col=item_id):\n",
    "    \"\"\"\n",
    "    返回形状为 (num_users, max_len) 的 LongTensor。\n",
    "    第 i 行是用户 i 的历史序列，左侧 PAD，右对齐。\n",
    "    不存在历史的用户整行都是 pad_idx。\n",
    "    \"\"\"\n",
    "    # 先全部填 PAD\n",
    "    hist = torch.full((num_users, max_len), pad_idx, dtype=torch.long)\n",
    "\n",
    "    # groupby 遍历每个用户已有交互\n",
    "    for uid, items in df.groupby(user_col)[item_col]:\n",
    "        seq = items.to_numpy()[-max_len:]             # 取最近 max_len 条\n",
    "        hist[uid, -len(seq):] = torch.as_tensor(seq, dtype=torch.long)\n",
    "\n",
    "    return hist    # (U, T)\n"
   ],
   "id": "9bc49db2aee62849",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T21:01:52.253176Z",
     "start_time": "2025-06-12T21:01:52.245281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = SASRec(n_items=N_ITEMS,\n",
    "                 embedding_dim=EMBEDDING_DIM,\n",
    "                 pad_idx=PAD_IDX)\n",
    "model = model.to(device)\n",
    "train_model(model=model,epochs=50, train_df=train_df,batch_size=1024,lr=LR,test_df=test_df,device=device)"
   ],
   "id": "acd41e7d550823fb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "cd29a40c68841c9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T19:15:27.293580Z",
     "start_time": "2025-06-12T18:42:31.637895Z"
    }
   },
   "source": [
    "    # ------------------ 训练 ------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "train_model(model=model,epochs=50, train_df=train_df,batch_size=1024,lr=LR,test_df=test_df,device=device)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ep 1] step 1/508 | loss 4.4320\n",
      "[Ep 1] step 100/508 | loss 3.3622\n",
      "[Ep 1] step 200/508 | loss 2.3144\n",
      "[Ep 1] step 300/508 | loss 1.7428\n",
      "[Ep 1] step 400/508 | loss 1.2451\n",
      "[Ep 1] step 500/508 | loss 0.9118\n",
      "Ep 1 done | avg loss 2.1889 | 197.6s\n",
      "\n",
      "[Ep 2] step 1/508 | loss 0.9144\n",
      "[Ep 2] step 100/508 | loss 0.7561\n",
      "[Ep 2] step 200/508 | loss 0.7230\n",
      "[Ep 2] step 300/508 | loss 0.6795\n",
      "[Ep 2] step 400/508 | loss 0.6865\n",
      "[Ep 2] step 500/508 | loss 0.6798\n",
      "Ep 2 done | avg loss 0.7240 | 190.1s\n",
      "\n",
      "[Ep 3] step 1/508 | loss 0.6465\n",
      "[Ep 3] step 100/508 | loss 0.6473\n",
      "[Ep 3] step 200/508 | loss 0.6133\n",
      "[Ep 3] step 300/508 | loss 0.6335\n",
      "[Ep 3] step 400/508 | loss 0.6117\n",
      "[Ep 3] step 500/508 | loss 0.5873\n",
      "Ep 3 done | avg loss 0.6296 | 227.0s\n",
      "\n",
      "[Ep 4] step 1/508 | loss 0.6314\n",
      "[Ep 4] step 100/508 | loss 0.6080\n",
      "[Ep 4] step 200/508 | loss 0.5872\n",
      "[Ep 4] step 300/508 | loss 0.5316\n",
      "[Ep 4] step 400/508 | loss 0.5851\n",
      "[Ep 4] step 500/508 | loss 0.5617\n",
      "Ep 4 done | avg loss 0.5716 | 205.8s\n",
      "\n",
      "[Ep 5] step 1/508 | loss 0.5435\n",
      "[Ep 5] step 100/508 | loss 0.5218\n",
      "[Ep 5] step 200/508 | loss 0.5102\n",
      "[Ep 5] step 300/508 | loss 0.5326\n",
      "[Ep 5] step 400/508 | loss 0.5185\n",
      "[Ep 5] step 500/508 | loss 0.5042\n",
      "Ep 5 done | avg loss 0.5257 | 193.0s\n",
      "\n",
      "[Ep 6] step 1/508 | loss 0.5037\n",
      "[Ep 6] step 100/508 | loss 0.4976\n",
      "[Ep 6] step 200/508 | loss 0.4990\n",
      "[Ep 6] step 300/508 | loss 0.5004\n",
      "[Ep 6] step 400/508 | loss 0.5104\n",
      "[Ep 6] step 500/508 | loss 0.4863\n",
      "Ep 6 done | avg loss 0.4979 | 192.0s\n",
      "\n",
      "[Ep 7] step 1/508 | loss 0.4537\n",
      "[Ep 7] step 100/508 | loss 0.4995\n",
      "[Ep 7] step 200/508 | loss 0.4860\n",
      "[Ep 7] step 300/508 | loss 0.5135\n",
      "[Ep 7] step 400/508 | loss 0.4689\n",
      "[Ep 7] step 500/508 | loss 0.4904\n",
      "Ep 7 done | avg loss 0.4840 | 190.5s\n",
      "\n",
      "[Ep 8] step 1/508 | loss 0.5001\n",
      "[Ep 8] step 100/508 | loss 0.4589\n",
      "[Ep 8] step 200/508 | loss 0.4968\n",
      "[Ep 8] step 300/508 | loss 0.5000\n",
      "[Ep 8] step 400/508 | loss 0.4878\n",
      "[Ep 8] step 500/508 | loss 0.4749\n",
      "Ep 8 done | avg loss 0.4785 | 193.0s\n",
      "\n",
      "[Ep 9] step 1/508 | loss 0.4921\n",
      "[Ep 9] step 100/508 | loss 0.4827\n",
      "[Ep 9] step 200/508 | loss 0.4735\n",
      "[Ep 9] step 300/508 | loss 0.4841\n",
      "[Ep 9] step 400/508 | loss 0.4718\n",
      "[Ep 9] step 500/508 | loss 0.4771\n",
      "Ep 9 done | avg loss 0.4748 | 189.1s\n",
      "\n",
      "[Ep 10] step 1/508 | loss 0.4527\n",
      "[Ep 10] step 100/508 | loss 0.4661\n",
      "[Ep 10] step 200/508 | loss 0.4665\n",
      "[Ep 10] step 300/508 | loss 0.4854\n",
      "[Ep 10] step 400/508 | loss 0.4557\n",
      "[Ep 10] step 500/508 | loss 0.4758\n",
      "Ep 10 done | avg loss 0.4731 | 188.8s\n",
      "\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model.save_embeddings(num_users=num_users,num_items=num_items,device=device,save_dir=save_dir)\n",
   "id": "fad220ca31a695e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_loader = customdataset.build_test_loader(test_df, num_items ,user_col = user_id, item_col = item_id, batch_size=1024, num_workers=num_workers)\n",
    "item_pool = list(range(num_items))\n",
    "faiss_index = faiss.read_index(f\"{save_dir}/item_index.faiss\")\n",
    "hist_tensors = build_hist_matrix(train_df, max_len=MAX_SEQ_LEN, pad_idx=PAD_IDX,num_users=num_users).to(device)"
   ],
   "id": "1795184be9664f14"
  },
  {
   "cell_type": "code",
   "id": "bbd4f9d7af8b5c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T23:16:30.822603Z",
     "start_time": "2025-06-12T21:10:31.413734Z"
    }
   },
   "source": [
    "hr_r, ndcg_r = evaluate.evaluate_random(test_loader, item_pool ,top_k=top_k)\n",
    "print(f\"Random HR@{top_k} = {hr_r:.4f}, nDCG@{top_k} = {ndcg_r:.4f}\")\n",
    "hr_p, ndcg_p = evaluate.evaluate_popular(test_loader, train_df,top_k=top_k)\n",
    "print(f\"Popular HR@{top_k} = {hr_p:.4f}, nDCG@{top_k} = {ndcg_p:.4f}\")\n",
    "hr_m, ndcg_m = evaluate.evaluate_seq_model(test_loader, model, faiss_index, device,top_k=top_k,hist_tensors=hist_tensors)\n",
    "print(f\"Model   HR@{top_k} = {hr_m:.4f}, nDCG@{top_k} = {ndcg_m:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SASRec   Hit@10=0.9847  NDCG@10=0.9845\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[30], line 11\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# ---------------- baseline：Popular ----------------\u001B[39;00m\n\u001B[0;32m     10\u001B[0m pop_score_fn  \u001B[38;5;241m=\u001B[39m make_popularity_score_fn(train_df)\n\u001B[1;32m---> 11\u001B[0m hit_pop, ndcg_pop \u001B[38;5;241m=\u001B[39m evaluate_ranking(\n\u001B[0;32m     12\u001B[0m     test_df, train_df, pop_score_fn,\n\u001B[0;32m     13\u001B[0m     num_items\u001B[38;5;241m=\u001B[39mnum_items, k\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m\n\u001B[0;32m     14\u001B[0m )\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPopular  Hit@10=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhit_pop\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m  NDCG@10=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mndcg_pop\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# ---------------- baseline：Random -----------------\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[26], line 43\u001B[0m, in \u001B[0;36mevaluate_ranking\u001B[1;34m(test_df, train_df, score_fn, num_items, k, num_neg, user_col, item_col, seed)\u001B[0m\n\u001B[0;32m     41\u001B[0m users_t  \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mLongTensor([u] \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mlen\u001B[39m(item_candidates))\n\u001B[0;32m     42\u001B[0m items_t  \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mLongTensor(item_candidates)\n\u001B[1;32m---> 43\u001B[0m scores   \u001B[38;5;241m=\u001B[39m score_fn(users_t, items_t)        \u001B[38;5;66;03m# ← 只依赖 score_fn\u001B[39;00m\n\u001B[0;32m     44\u001B[0m rank_idx \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margsort(scores)[::\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]          \u001B[38;5;66;03m# 降序\u001B[39;00m\n\u001B[0;32m     45\u001B[0m ranked_items \u001B[38;5;241m=\u001B[39m [item_candidates[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m rank_idx]\n",
      "Cell \u001B[1;32mIn[27], line 5\u001B[0m, in \u001B[0;36mmake_popularity_score_fn.<locals>._score_fn\u001B[1;34m(users_t, items_t)\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_score_fn\u001B[39m(users_t, items_t):\n\u001B[1;32m----> 5\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39marray([item_cnt\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;28mint\u001B[39m(i), default_score) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m items_t])\n",
      "File \u001B[1;32m~\\.conda\\envs\\DP1\\Lib\\site-packages\\torch\\_tensor.py:1164\u001B[0m, in \u001B[0;36mTensor.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1155\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_get_tracing_state():\n\u001B[0;32m   1156\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m   1157\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIterating over a tensor might cause the trace to be incorrect. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1158\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPassing a tensor of different shape won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt change the number of \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1162\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[0;32m   1163\u001B[0m     )\n\u001B[1;32m-> 1164\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28miter\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munbind(\u001B[38;5;241m0\u001B[39m))\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 30
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DP1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

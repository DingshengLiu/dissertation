{
 "cells": [
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "df5fadd3b5922093",
    "outputId": "4887e050-05c4-47b3-b0df-edc3cdf34906"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "skip:/content/tool exists\n",
      "skip:/content/MicroLens-50k_pairs.csv exists\n",
      "skip:/content/cover_emb128.lmdb exists\n"
     ]
    }
   ],
   "execution_count": 33,
   "source": [
    "from google.colab import drive\n",
    "import shutil\n",
    "import os\n",
    "def copy_from_drive(src_path, dst_path):\n",
    "\n",
    "    if os.path.exists(dst_path):\n",
    "        print(f\"skip:{dst_path} exists\")\n",
    "        return\n",
    "\n",
    "    if os.path.isdir(src_path):\n",
    "        shutil.copytree(src_path, dst_path)\n",
    "    elif os.path.isfile(src_path):\n",
    "        shutil.copy(src_path, dst_path)\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "copy_from_drive('/content/drive/MyDrive/tool', '/content/tool')\n",
    "copy_from_drive('/content/drive/MyDrive/MicroLens-50k_pairs.csv','/content/MicroLens-50k_pairs.csv')\n",
    "copy_from_drive('/content/drive/MyDrive/cover_emb128.lmdb','/content/cover_emb128.lmdb')\n",
    "copy_from_drive('/content/drive/MyDrive/title_emb1024.lmdb','/content/title_emb1024.lmdb')\n"
   ],
   "id": "df5fadd3b5922093"
  },
  {
   "cell_type": "code",
   "id": "b828e981",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T18:42:28.345525Z",
     "start_time": "2025-06-12T18:42:18.772256Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b828e981",
    "outputId": "4e759086-43f8-4638-dd91-d553027a96d5"
   },
   "source": [
    "!pip install faiss-cpu\n",
    "!pip install lmdb\n",
    "import os\n",
    "from tool import preprocess\n",
    "from tool import customdataset\n",
    "from tool import evaluate\n",
    "import faiss\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "import math\n",
    "import csv\n",
    "from matplotlib import pyplot as plt\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: lmdb in /usr/local/lib/python3.11/dist-packages (1.7.3)\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "id": "e53a0288b0498fe4",
    "outputId": "ba03e307-1127-4b25-e768-bf3a58a61bb0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function tool.preprocess.set_seed.<locals>.seed_worker(worker_id)>"
      ],
      "text/html": [
       "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
       "      pre.function-repr-contents {\n",
       "        overflow-x: auto;\n",
       "        padding: 8px 12px;\n",
       "        max-height: 500px;\n",
       "      }\n",
       "\n",
       "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
       "        cursor: pointer;\n",
       "        max-height: 100px;\n",
       "      }\n",
       "    </style>\n",
       "    <pre style=\"white-space: initial; background:\n",
       "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
       "         border-bottom: 1px solid var(--colab-border-color);\"><b>tool.preprocess.set_seed.&lt;locals&gt;.seed_worker</b><br/>def seed_worker(worker_id)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/tool/preprocess.py</a>&lt;no docstring&gt;</pre>\n",
       "      <script>\n",
       "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
       "        for (const element of document.querySelectorAll('.filepath')) {\n",
       "          element.style.display = 'block'\n",
       "          element.onclick = (event) => {\n",
       "            event.preventDefault();\n",
       "            event.stopPropagation();\n",
       "            google.colab.files.view(element.textContent, 18);\n",
       "          };\n",
       "        }\n",
       "      }\n",
       "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
       "        element.onclick = (event) => {\n",
       "          event.preventDefault();\n",
       "          event.stopPropagation();\n",
       "          element.classList.toggle('function-repr-contents-collapsed');\n",
       "        };\n",
       "      }\n",
       "      </script>\n",
       "      </div>"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "execution_count": 35,
   "source": [
    "preprocess.set_seed(42)"
   ],
   "id": "e53a0288b0498fe4"
  },
  {
   "cell_type": "code",
   "id": "e99a31348e0a553",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T18:42:28.396608Z",
     "start_time": "2025-06-12T18:42:28.351537Z"
    },
    "id": "e99a31348e0a553"
   },
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "path = 'MicroLens-50k_pairs.csv'\n",
    "user = 'user'\n",
    "item = 'item'\n",
    "user_id = 'user_id'\n",
    "item_id = 'item_id'\n",
    "timestamp = 'timestamp'\n",
    "save_dir = './embeddings'\n",
    "cover_lmdb_path = 'cover_emb128.lmdb'\n",
    "title_lmdb_path = 'title_emb1024.lmdb'\n",
    "record_path = './records'\n",
    "TOP_K= 10\n",
    "NUM_WORKERS = 10\n",
    "PATIENCE = 5\n",
    "MONITOR = 'hr'"
   ],
   "id": "7c29b12201e7624c"
  },
  {
   "cell_type": "code",
   "id": "943fc6c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T18:42:29.290418Z",
     "start_time": "2025-06-12T18:42:29.273675Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "943fc6c5",
    "outputId": "89aa05df-038f-4b67-ed83-3b1008cb966d"
   },
   "source": [
    "dataset_pd,num_users,num_items = preprocess.openAndSort(path,user_id=user,item_id=item,timestamp='timestamp')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dataset base information：\n",
      "- number of users：50000\n",
      "- number of items：19220\n",
      "- number of rows：359708\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ---------- 超参数 ----------\n",
    "MAX_SEQ_LEN   = 20         # 序列长度\n",
    "EMBEDDING_DIM = 64          # item / user embedding 维度\n",
    "N_HEADS       = 1           # Multi-Head Attention 头数\n",
    "N_LAYERS      = 2           # Transformer block 层数\n",
    "DROPOUT       = 0.1\n",
    "BATCH_SIZE    = 1024\n",
    "EPOCHS        = 50\n",
    "LR            = 1e-3\n",
    "SEED          = 42\n",
    "PROJECT_NAME = \"SASRec\"\n",
    "MODAL = {'COVER':{\"LMDB_DIM\":128, \"HIDDEN_SIZE\":[EMBEDDING_DIM],\"DROPOUT\":0.2} , 'TITLE':{\"LMDB_DIM\":1024,\"HIDDEN_SIZE\":[EMBEDDING_DIM],\"DROPOUT\":0.2}\n",
    "         ,'COVER-TITLE': {\"LMDB_DIM\":128+1024, \"HIDDEN_SIZE\":[EMBEDDING_DIM],\"DROPOUT\":0.2}}\n",
    "FUSION_MODE = \"base\"\n",
    "CURRENT_MODAL = \"COVER\"\n",
    "MODAL_CONFIG = MODAL[CURRENT_MODAL]\n",
    "MODAL_HIDDEN_SIZE = MODAL_CONFIG.get('HIDDEN_SIZE')\n",
    "LMDB_DIM = MODAL_CONFIG.get('LMDB_DIM')\n",
    "MODAL_DROPOUT = MODAL_CONFIG.get('DROPOUT')\n",
    "L2_NORM = False\n",
    "\n",
    "# ----------------------------\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# ---------- 常量（来自你已有变量） ----------\n",
    "PAD_IDX = num_items          # 专用 padding id\n",
    "N_ITEMS = num_items + 1      # Embedding 行数（含 PAD）\n",
    "\n",
    "# -------------------------------------------\n"
   ],
   "id": "dcc111731c37e935"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c1952473721f3f99",
    "outputId": "cb778490-977e-4cae-c2ab-6cdcd0afda22"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train size: 309708\n",
      "Test size: 49424\n"
     ]
    }
   ],
   "execution_count": 39,
   "source": [
    "train_df, val_df, test_df, train_all_df = preprocess.split_with_val(dataset_pd,user, item, timestamp)\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Val_df size: {len(val_df)}\")\n",
    "print(f\"Test_df size: {len(test_df)}\")\n",
    "print(f\"Train_all_df size: {len(train_all_df)}\")"
   ],
   "id": "c1952473721f3f99"
  },
  {
   "cell_type": "code",
   "id": "69160caa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T18:42:29.358698Z",
     "start_time": "2025-06-12T18:42:29.349731Z"
    },
    "id": "69160caa"
   },
   "source": [
    "# maintain a map from new id to old id, new id for constructing matrix\n",
    "user2id = {u: i for i, u in enumerate(dataset_pd[user].unique())}\n",
    "item2id = {i: j for j, i in enumerate(dataset_pd[item].unique())}\n",
    "\n",
    "# apply to train_df and test_df\n",
    "train_df[user_id] = train_df[user].map(user2id)\n",
    "train_df[item_id] = train_df[item].map(item2id)\n",
    "val_df[user_id] = val_df[user].map(user2id)\n",
    "val_df[item_id] = val_df[item].map(item2id)\n",
    "test_df[user_id] = test_df[user].map(user2id)\n",
    "test_df[item_id] = test_df[item].map(item2id)\n",
    "train_all_df[user_id] = train_all_df[user].map(user2id)\n",
    "train_all_df[item_id] = train_all_df[item].map(item2id)\n",
    "\n",
    "# 1. 构建 item_id 到 item 的映射（来自 train_df）\n",
    "item_id_to_item = {v: k for k, v in item2id.items()}"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "id": "a7aba75c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T18:42:31.455790Z",
     "start_time": "2025-06-12T18:42:31.445397Z"
    },
    "id": "a7aba75c"
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class SASRec(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_items=N_ITEMS,\n",
    "                 embedding_dim=EMBEDDING_DIM,\n",
    "                 n_heads=N_HEADS,\n",
    "                 n_layers=N_LAYERS,\n",
    "                 max_len=MAX_SEQ_LEN,\n",
    "                 pad_idx=PAD_IDX,\n",
    "                 dropout=DROPOUT,\n",
    "                 lmdb_dim=LMDB_DIM,\n",
    "                 modal_hidden_size=MODAL_HIDDEN_SIZE,\n",
    "                 modal_dropout=MODAL_DROPOUT,\n",
    "                 fusion_mode=FUSION_MODE  # 'base' | 'early' | 'late1' | 'late2'\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        assert fusion_mode in {'base', 'early', 'late1', 'late2'}\n",
    "        self.fusion_mode = fusion_mode\n",
    "        self.item_ln = nn.LayerNorm(embedding_dim)\n",
    "        self.user_ln = nn.LayerNorm(embedding_dim)\n",
    "        self.id_scale = nn.Parameter(torch.tensor(1.0))\n",
    "        self.mm_scale = nn.Parameter(torch.tensor(1.0))\n",
    "        # ---------- Item / Pos embeddings ----------\n",
    "        self.embedding = nn.Embedding(n_items, embedding_dim, padding_idx=pad_idx)\n",
    "        self.pos_emb   = nn.Embedding(max_len + 1, embedding_dim, padding_idx=0)\n",
    "        self.dropout   = nn.Dropout(dropout)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=embedding_dim * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation='gelu'\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "\n",
    "        # 初始化\n",
    "        nn.init.normal_(self.embedding.weight, mean=0.0, std=0.05)\n",
    "        nn.init.normal_(self.pos_emb.weight,  mean=0.0, std=0.05)\n",
    "        with torch.no_grad():\n",
    "            self.embedding.weight[self.embedding.padding_idx].zero_()\n",
    "            self.pos_emb.weight[0].zero_()\n",
    "\n",
    "        # modal 向量（冻结）\n",
    "        if FUSION_MODE!='base':\n",
    "            modal_emb_tensor = None\n",
    "            if CURRENT_MODAL=='COVER':\n",
    "                modal_emb_tensor = preprocess.load_tensor_from_lmdb(\n",
    "                    cover_lmdb_path, num_items, item_id_to_item, lmdb_dim\n",
    "                )\n",
    "            if CURRENT_MODAL=='TITLE':\n",
    "                modal_emb_tensor = preprocess.load_tensor_from_lmdb(\n",
    "                    title_lmdb_path, num_items, item_id_to_item, lmdb_dim\n",
    "                )\n",
    "            if CURRENT_MODAL=='COVER-TITLE':\n",
    "                cover_emb_tensor = preprocess.load_tensor_from_lmdb(\n",
    "                    cover_lmdb_path, num_items, item_id_to_item, 128\n",
    "                )\n",
    "                title_emb_tensor = preprocess.load_tensor_from_lmdb(\n",
    "                    title_lmdb_path, num_items, item_id_to_item, 1024\n",
    "                )\n",
    "                modal_emb_tensor = torch.cat([cover_emb_tensor, title_emb_tensor], dim=-1)\n",
    "            pad_vec = torch.zeros(1, lmdb_dim)\n",
    "            modal_emb_tensor = torch.cat([modal_emb_tensor, pad_vec], dim=0)\n",
    "            self.register_buffer('frozen_extra_emb', modal_emb_tensor)\n",
    "\n",
    "        # ---------- 前融合投影：[item_emb; modal] -> emb_dim ----------\n",
    "        self.mlp_item_modal = self.build_mlp(embedding_dim + lmdb_dim, modal_hidden_size, modal_dropout)\n",
    "\n",
    "        # ---------- 后融合用 α（全局标量，向量级加权） ----------\n",
    "        # sigmoid(0)=0.5；若想更稳可设为 1.0 使初期更偏向 ID\n",
    "        self.alpha_param = nn.Parameter(torch.tensor(0.0)) if fusion_mode == 'late1' or fusion_mode == 'late2' else None\n",
    "        self.beta_param  = nn.Parameter(torch.tensor(0.0)) if fusion_mode == 'late2' else None\n",
    "        self.pad_idx = pad_idx\n",
    "        self.n_items = n_items\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def build_mlp(self, input_dim, hidden_sizes, dropout):\n",
    "        layers = []\n",
    "        for h in hidden_sizes:\n",
    "            layers += [nn.Linear(input_dim, h), nn.LayerNorm(h), nn.Tanh(), nn.Dropout(dropout)]\n",
    "            input_dim = h\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    # ===================== 序列输入的两种构造 =====================\n",
    "    def _seq_emb_id_only(self, seq):\n",
    "        \"\"\"仅用 ID embedding 作为 Transformer 输入\"\"\"\n",
    "        return self.embedding(seq)  # (B,T,D)\n",
    "\n",
    "    def _seq_emb_early(self, seq):\n",
    "        \"\"\"前融合：按时间步拼接 modal，再映射回 emb_dim\"\"\"\n",
    "        modal = self.frozen_extra_emb.to(seq.device)[seq]        # (B,T,C)\n",
    "        item  = self.embedding(seq)                               # (B,T,D)\n",
    "        x     = torch.cat([item, modal], dim=-1)                  # (B,T,D+C)\n",
    "        x     = self.mlp_item_modal(x)                            # (B,T,D)\n",
    "        return x\n",
    "\n",
    "    # ===================== 前向：输出用户向量 =====================\n",
    "    def forward(self, seq):\n",
    "        pad = self.embedding.padding_idx\n",
    "\n",
    "        # 用户侧：base/early 与现状一致；late：对称融合（β）+ 尺度校准\n",
    "        if self.fusion_mode == 'base':\n",
    "            x_item = self._seq_emb_id_only(seq)\n",
    "        elif self.fusion_mode == 'early':\n",
    "            x_item = self._seq_emb_early(seq)\n",
    "        elif self.fusion_mode == 'late1':\n",
    "            x_item = self._seq_emb_id_only(seq)\n",
    "        else:  # 'late2'，做两套输入，再融合\n",
    "            x_id  = self._seq_emb_id_only(seq)\n",
    "            x_mm  = self._seq_emb_early(seq)\n",
    "            # 加位置、encoder、取最后一步 —— 封装成一个小函数以避免重复\n",
    "            def encode(xi):\n",
    "                nonpad = (seq != pad).int()\n",
    "                pos_ids = (torch.cumsum(nonpad, dim=1) * nonpad).clamp(max=self.pos_emb.num_embeddings - 1)\n",
    "                x = xi + self.pos_emb(pos_ids)\n",
    "                x = self.dropout(x)\n",
    "                key_padding_mask = (seq == pad)\n",
    "                x = self.encoder(x, src_key_padding_mask=key_padding_mask)\n",
    "                seq_lens = (seq != pad).sum(dim=1).clamp(min=1)\n",
    "                last_idx = (seq_lens - 1).unsqueeze(1).unsqueeze(2).expand(-1, 1, x.size(-1))\n",
    "                return x.gather(dim=1, index=last_idx).squeeze(1)  # (B,D)\n",
    "            u_id = encode(x_id)\n",
    "            u_mm = encode(x_mm)\n",
    "            u_id = self.user_ln(u_id)\n",
    "            u_mm = self.user_ln(u_mm)\n",
    "            beta = torch.sigmoid(self.beta_param)\n",
    "            h = beta * u_id + (1.0 - beta) * u_mm\n",
    "            return F.normalize(h, p=2, dim=1) if L2_NORM else h\n",
    "\n",
    "        # base / early 的通路（与原来一致）\n",
    "        nonpad  = (seq != pad).int()\n",
    "        pos_ids = (torch.cumsum(nonpad, dim=1) * nonpad).clamp(max=self.pos_emb.num_embeddings - 1)\n",
    "        x = x_item + self.pos_emb(pos_ids)\n",
    "        x = self.dropout(x)\n",
    "        key_padding_mask = (seq == pad)\n",
    "        x = self.encoder(x, src_key_padding_mask=key_padding_mask)\n",
    "        seq_lens = (seq != pad).sum(dim=1).clamp(min=1)\n",
    "        last_idx = (seq_lens - 1).unsqueeze(1).unsqueeze(2).expand(-1, 1, x.size(-1))\n",
    "        h = x.gather(dim=1, index=last_idx).squeeze(1)\n",
    "        return F.normalize(h, p=2, dim=1) if L2_NORM else h\n",
    "\n",
    "    # ===================== 物品侧：三种模式的候选向量 =====================\n",
    "    def _item_vec_id_only(self, item_ids, l2_norm=False):\n",
    "        i_vec = self.embedding(item_ids)  # (B,D)\n",
    "        if l2_norm:\n",
    "            i_vec = F.normalize(i_vec, p=2, dim=1)\n",
    "        return i_vec\n",
    "\n",
    "    def _item_vec_early(self, item_ids, l2_norm=False):\n",
    "        modal = self.frozen_extra_emb.to(item_ids.device)[item_ids]  # (B,C)\n",
    "        i_vec = self.embedding(item_ids)                              # (B,D)\n",
    "        i_vec = torch.cat([i_vec, modal], dim=-1)                     # (B,D+C)\n",
    "        i_vec = self.mlp_item_modal(i_vec)                            # (B,D)\n",
    "        if l2_norm:\n",
    "            i_vec = F.normalize(i_vec, p=2, dim=1)\n",
    "        return i_vec\n",
    "\n",
    "    def _item_vec_late(self, item_ids, l2_norm=False):\n",
    "        # 向量级后融合：i = α * i_id + (1-α) * i_mm\n",
    "        i_id = self._item_vec_id_only(item_ids, l2_norm=False)\n",
    "        i_mm = self._item_vec_early(item_ids, l2_norm=False)\n",
    "        i_id = self.item_ln(i_id) * self.id_scale\n",
    "        i_mm = self.item_ln(i_mm) * self.mm_scale\n",
    "        alpha = torch.sigmoid(self.alpha_param)  # 标量\n",
    "        i_vec = alpha * i_id + (1.0 - alpha) * i_mm\n",
    "        if l2_norm:\n",
    "            i_vec = F.normalize(i_vec, p=2, dim=1)\n",
    "        return i_vec\n",
    "\n",
    "    def get_items_embedding(self, item_ids, l2_norm=False):\n",
    "        if self.fusion_mode == 'base':\n",
    "            return self._item_vec_id_only(item_ids, l2_norm=l2_norm)\n",
    "        elif self.fusion_mode == 'early':\n",
    "            return self._item_vec_early(item_ids, l2_norm=l2_norm)\n",
    "        else:  # 'late'\n",
    "            return self._item_vec_late(item_ids, l2_norm=l2_norm)\n",
    "\n",
    "    # ===================== 导出（与现有管线兼容） =====================\n",
    "    def save_embeddings(self, num_users, num_items, device, save_dir='./embeddings', l2_norm=False):\n",
    "        import os, faiss\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        self.eval()\n",
    "        self.to(device)\n",
    "\n",
    "        item_ids = torch.arange(num_items, dtype=torch.long, device=device)\n",
    "        with torch.no_grad():\n",
    "            item_embeds = self.get_items_embedding(item_ids, l2_norm=l2_norm)\n",
    "\n",
    "        item_embeds = item_embeds.cpu().numpy().astype(np.float32)\n",
    "        np.save(f\"{save_dir}/item_embeddings.npy\", item_embeds)\n",
    "\n",
    "        dim = item_embeds.shape[1]\n",
    "        index = faiss.IndexFlatIP(dim)\n",
    "        index.add(item_embeds)\n",
    "        faiss.write_index(index, f\"{save_dir}/item_index.faiss\")\n",
    "        print(\"Saved item embeddings and FAISS index.\")\n"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "id": "07e268c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T18:42:31.532424Z",
     "start_time": "2025-06-12T18:42:31.522660Z"
    },
    "id": "07e268c2"
   },
   "source": [
    "# ======== 训练流程 ======== #\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "def train_model(model,\n",
    "                train_df,\n",
    "                val_df,\n",
    "                top_k,\n",
    "                epochs,\n",
    "                lr ,\n",
    "                val_mode,\n",
    "                batch_size ,\n",
    "                device=None,\n",
    "                patience=PATIENCE, # 早停容忍\n",
    "                monitor=MONITOR,       # \"hr\" 或 \"ndcg\"\n",
    "                record_path = record_path):\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "    train_loader  = customdataset.build_seq_loader(train_df, batch_size=batch_size,\n",
    "                         shuffle=True, num_workers=10,pad_idx=PAD_IDX,max_len=MAX_SEQ_LEN,user_id=user_id,item_id=item_id)\n",
    "    val_loader = customdataset.build_test_loader(val_df, num_items ,user_col = user_id, item_col = item_id, batch_size=1024, num_workers=NUM_WORKERS)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # 训练过程记录\n",
    "    hist = {\n",
    "        \"epoch\": [],\n",
    "        \"loss\": [],\n",
    "        f\"hr@{top_k}\": [],\n",
    "        f\"ndcg@{top_k}\": [],\n",
    "        \"alpha\": [],\n",
    "        \"beta\": [],\n",
    "    }\n",
    "\n",
    "    # 早停配置\n",
    "    best_metric = -math.inf\n",
    "    best_epoch  = -1\n",
    "    patience_cnt = 0\n",
    "    monitor_key = f\"{monitor}@{top_k}\"\n",
    "\n",
    "    print(f\"[EarlyStopping] monitor={monitor_key} , patience={patience}\")\n",
    "\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        dt_start = datetime.now()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            hist, pos = batch\n",
    "            hist, pos = hist.to(device), pos.to(device)\n",
    "\n",
    "            # 1. 前向传播（返回预测向量）\n",
    "            predict = model(hist)                      # (B, D)\n",
    "            i_vec = model.get_items_embedding(pos,l2_norm=True)\n",
    "\n",
    "            # 2. 得分矩阵：每个 user 对所有正 item 的打分\n",
    "            logits = torch.matmul(predict, i_vec.T)  # shape: (B, B)\n",
    "\n",
    "            # 3. 构造标签：每个 user 的正确 item 在对角线（即位置 i）\n",
    "            labels = torch.arange(logits.size(0), device=device)  # [0, 1, ..., B-1]\n",
    "\n",
    "            # 4. Cross Entropy Loss\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "            # 5. 反向传播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # 日志\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        dt_end = datetime.now()\n",
    "        dt = dt_end - dt_start\n",
    "\n",
    "        model.save_embeddings(num_users=num_users,num_items=num_items,device=device,save_dir=save_dir)\n",
    "        faiss_index = faiss.read_index(f\"{save_dir}/item_index.faiss\")\n",
    "        model.eval()\n",
    "        hr_m, ndcg_m = evaluate.evaluate_model(val_loader, model, faiss_index, device, top_k=top_k)\n",
    "\n",
    "        # gates（若存在）\n",
    "        alpha_val = float(torch.sigmoid(model.alpha_param).item()) if hasattr(model, \"alpha_param\") and model.alpha_param is not None else float(\"nan\")\n",
    "        beta_val  = float(torch.sigmoid(model.beta_param).item())  if hasattr(model, \"beta_param\") and model.beta_param is not None else float(\"nan\")\n",
    "\n",
    "        print(f\"[Epoch {epoch:02d}/{epochs}] avg InBatch Softmax Loss = {avg_loss:.4f}, \"\n",
    "              f\"HR@{top_k} = {hr_m:.4f}, NDCG@{top_k} = {ndcg_m:.4f}, \"\n",
    "              f\"alpha={alpha_val if not math.isnan(alpha_val) else 'NA'}, \"\n",
    "              f\"beta={beta_val if not math.isnan(beta_val) else 'NA'}, \"\n",
    "              f\"time = {dt:.2f}s\")\n",
    "\n",
    "        # —— 记录历史 ——\n",
    "        hist[\"epoch\"].append(epoch)\n",
    "        hist[\"loss\"].append(avg_loss)\n",
    "        hist[f\"hr@{top_k}\"].append(hr_m)\n",
    "        hist[f\"ndcg@{top_k}\"].append(ndcg_m)\n",
    "        hist[\"alpha\"].append(alpha_val)\n",
    "        hist[\"beta\"].append(beta_val)\n",
    "\n",
    "        # —— 早停判断（最大化 monitor 指标）——\n",
    "        current_metric = hr_m if monitor == \"hr\" else ndcg_m\n",
    "        if current_metric > best_metric:\n",
    "            best_metric = current_metric\n",
    "            best_epoch = epoch\n",
    "            patience_cnt = 0\n",
    "            print(f\"current best {monitor_key}={best_metric:.4f} @ epoch {epoch}.\")\n",
    "                        # ==== 保存最佳 hr / ndcg / epoch ====\n",
    "            best_info_path = os.path.join(record_path,\n",
    "                                          \"validation mode\" if val_mode else \"train mode\",\n",
    "                                          \"best_result.txt\")\n",
    "            os.makedirs(os.path.dirname(best_info_path), exist_ok=True)\n",
    "            with open(best_info_path, \"w\") as f:\n",
    "                f.write(f\"epoch: {epoch}\\n\")\n",
    "                f.write(f\"HR@{top_k}: {hr_m:.4f}\\n\")\n",
    "                f.write(f\"NDCG@{top_k}: {ndcg_m:.4f}\\n\")\n",
    "            print(f\"Best result info saved to {best_info_path}\")\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "            if patience_cnt >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # —— 导出历史 CSV ——\n",
    "    csv_path = os.path.join(record_path,\"validation mode\" if val_mode else \"train mode\",\"training_history.csv\")\n",
    "    os.makedirs(os.path.dirname(csv_path), exist_ok=True)  # 确保目录存在\n",
    "    with open(csv_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"epoch\", \"loss\", f\"hr@{top_k}\", f\"ndcg@{top_k}\", \"alpha\", \"beta\", \"time_sec\"])\n",
    "        for i in range(len(hist[\"epoch\"])):\n",
    "            writer.writerow([\n",
    "                hist[\"epoch\"][i],\n",
    "                hist[\"loss\"][i],\n",
    "                hist[f\"hr@{top_k}\"][i],\n",
    "                hist[f\"ndcg@{top_k}\"][i],\n",
    "                hist[\"alpha\"][i],\n",
    "                hist[\"beta\"][i],\n",
    "            ])\n",
    "    # —— 绘图：Loss ——\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(hist[\"epoch\"], hist[\"loss\"])\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"In-Batch CE Loss\"); plt.title(\"Training Loss\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.4); plt.tight_layout()\n",
    "    plt.xticks(range(1, max(hist[\"epoch\"]) + 1, 1))\n",
    "    fig1_path = os.path.join(record_path,\"validation mode\" if val_mode else \"train mode\",\"curve_loss.png\")\n",
    "    os.makedirs(os.path.dirname(fig1_path), exist_ok=True)  # 确保目录存在\n",
    "\n",
    "    plt.savefig(fig1_path, dpi=150); plt.close()\n",
    "    print(f\"Saved {fig1_path}\")\n",
    "\n",
    "    # —— 绘图：HR/NDCG ——\n",
    "    plt.figure()\n",
    "    plt.plot(hist[\"epoch\"], hist[f\"hr@{top_k}\"], label=f\"HR@{top_k}\")\n",
    "    plt.plot(hist[\"epoch\"], hist[f\"ndcg@{top_k}\"], label=f\"NDCG@{top_k}\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Metric\"); plt.title(\"Validation Metrics\")\n",
    "    plt.legend(); plt.grid(True, linestyle=\"--\", alpha=0.4); plt.tight_layout()\n",
    "    plt.xticks(range(1, max(hist[\"epoch\"]) + 1, 1))\n",
    "    fig2_path = os.path.join(record_path,\"validation mode\" if val_mode else \"train mode\",\"curve_metrics.png\")\n",
    "    os.makedirs(os.path.dirname(fig2_path), exist_ok=True)  # 确保目录存在\n",
    "    plt.savefig(fig2_path, dpi=150); plt.close()\n",
    "    print(f\"Saved {fig2_path}\")\n",
    "\n",
    "    # —— 绘图：alpha/beta（如存在） ——\n",
    "    if not all(math.isnan(v) for v in hist[\"alpha\"]) or not all(math.isnan(v) for v in hist[\"beta\"]):\n",
    "        plt.figure()\n",
    "        if not all(math.isnan(v) for v in hist[\"alpha\"]):\n",
    "            plt.plot(hist[\"epoch\"], hist[\"alpha\"], label=\"alpha (item late)\")\n",
    "        if not all(math.isnan(v) for v in hist[\"beta\"]):\n",
    "            plt.plot(hist[\"epoch\"], hist[\"beta\"],  label=\"beta (user late)\")\n",
    "        plt.xlabel(\"Epoch\"); plt.ylabel(\"Gate (sigmoid)\"); plt.title(\"Late Fusion Gates\")\n",
    "        plt.ylim(0, 1); plt.legend(); plt.grid(True, linestyle=\"--\", alpha=0.4); plt.tight_layout()\n",
    "        plt.xticks(range(1, max(hist[\"epoch\"]) + 1, 1))\n",
    "        fig3_path = os.path.join(record_path,\"validation mode\" if val_mode else \"train mode\",\"curve_alpha_beta.png\")\n",
    "        os.makedirs(os.path.dirname(fig3_path), exist_ok=True)  # 确保目录存在\n",
    "        plt.savefig(fig3_path, dpi=150); plt.close()\n",
    "        print(f\"Saved {fig3_path}\")\n",
    "\n",
    "    print(f\"Best {monitor_key}={best_metric:.4f} at epoch {best_epoch}\")\n",
    "    return\n"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T18:42:31.567359Z",
     "start_time": "2025-06-12T18:42:31.561346Z"
    },
    "id": "9bc49db2aee62849"
   },
   "cell_type": "code",
   "source": [
    "def build_hist_matrix(df,\n",
    "                      num_users,\n",
    "                      max_len=MAX_SEQ_LEN,\n",
    "                      pad_idx=PAD_IDX,\n",
    "                      user_col=user_id,\n",
    "                      item_col=item_id):\n",
    "    \"\"\"\n",
    "    返回形状为 (num_users, max_len) 的 LongTensor。\n",
    "    第 i 行是用户 i 的历史序列，左侧 PAD，右对齐。\n",
    "    不存在历史的用户整行都是 pad_idx。\n",
    "    \"\"\"\n",
    "    # 先全部填 PAD\n",
    "    hist = torch.full((num_users, max_len), pad_idx, dtype=torch.long)\n",
    "\n",
    "    # groupby 遍历每个用户已有交互\n",
    "    for uid, items in df.groupby(user_col)[item_col]:\n",
    "        seq = items.to_numpy()[-max_len:]             # 取最近 max_len 条\n",
    "        hist[uid, -len(seq):] = torch.as_tensor(seq, dtype=torch.long)\n",
    "\n",
    "    return hist    # (U, T)\n"
   ],
   "id": "9bc49db2aee62849",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T21:01:52.253176Z",
     "start_time": "2025-06-12T21:01:52.245281Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "acd41e7d550823fb",
    "outputId": "dfcbb65e-5222-4456-f649-6eb5f7e44d0a"
   },
   "cell_type": "code",
   "source": [
    "model = SASRec(n_items=N_ITEMS,\n",
    "                 embedding_dim=EMBEDDING_DIM,\n",
    "                 pad_idx=PAD_IDX)\n",
    "model = model.to(device)\n",
    "train_model(model=model,epochs=EPOCHS, train_df=train_df,batch_size=BATCH_SIZE,lr=LR,val_df=val_df,device=device,patience=PATIENCE,monitor=MONITOR,record_path=record_path,top_k=TOP_K,val_mode=True)\n",
    "model = SASRec(n_items=N_ITEMS,\n",
    "                 embedding_dim=EMBEDDING_DIM,\n",
    "                 pad_idx=PAD_IDX)\n",
    "model = model.to(device)\n",
    "train_model(model=model,epochs=EPOCHS, train_df=train_all_df,batch_size=BATCH_SIZE,lr=LR,val_df=test_df,device=device,patience=PATIENCE,monitor=MONITOR,record_path=record_path,top_k=TOP_K,val_mode=False)"
   ],
   "id": "acd41e7d550823fb",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Epoch 01/12] avg InBatch Softmax Loss = 6.5704, time = 4.20s\n",
      "[Epoch 02/12] avg InBatch Softmax Loss = 6.0798, time = 4.17s\n",
      "[Epoch 03/12] avg InBatch Softmax Loss = 5.7919, time = 4.20s\n",
      "[Epoch 04/12] avg InBatch Softmax Loss = 5.6550, time = 4.02s\n",
      "[Epoch 05/12] avg InBatch Softmax Loss = 5.5787, time = 3.67s\n",
      "[Epoch 06/12] avg InBatch Softmax Loss = 5.5206, time = 3.87s\n",
      "[Epoch 07/12] avg InBatch Softmax Loss = 5.4713, time = 3.73s\n",
      "[Epoch 08/12] avg InBatch Softmax Loss = 5.4312, time = 3.80s\n",
      "[Epoch 09/12] avg InBatch Softmax Loss = 5.3918, time = 3.82s\n",
      "[Epoch 10/12] avg InBatch Softmax Loss = 5.3582, time = 4.09s\n",
      "[Epoch 11/12] avg InBatch Softmax Loss = 5.3257, time = 4.10s\n",
      "[Epoch 12/12] avg InBatch Softmax Loss = 5.2981, time = 3.66s\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fad220ca31a695e9",
    "outputId": "1efd7272-dded-4849-f0c6-3992757b9df6"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved user/item embeddings and FAISS index.\n"
     ]
    }
   ],
   "execution_count": 46,
   "source": [
    "model.save_embeddings(num_users=num_users,num_items=num_items,device=device,save_dir=save_dir,l2_norm=L2_NORM)\n"
   ],
   "id": "fad220ca31a695e9"
  },
  {
   "metadata": {
    "id": "1795184be9664f14"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 47,
   "source": [
    "test_loader = customdataset.build_test_loader(test_df, num_items ,user_col = user_id, item_col = item_id, batch_size=1024, num_workers=NUM_WORKERS)\n",
    "item_pool = list(range(num_items))\n",
    "faiss_index = faiss.read_index(f\"{save_dir}/item_index.faiss\")\n",
    "hist_tensors = build_hist_matrix(train_df, max_len=MAX_SEQ_LEN, pad_idx=PAD_IDX,num_users=num_users).to(device)"
   ],
   "id": "1795184be9664f14"
  },
  {
   "cell_type": "code",
   "id": "bbd4f9d7af8b5c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T23:16:30.822603Z",
     "start_time": "2025-06-12T21:10:31.413734Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bbd4f9d7af8b5c3",
    "outputId": "b8ec964f-c3c2-4e14-9818-e3d754e3435c"
   },
   "source": [
    "hr_r, ndcg_r = evaluate.evaluate_random(test_loader, item_pool ,top_k=TOP_K)\n",
    "print(f\"Random HR@{TOP_K} = {hr_r:.4f}, NDCG@{TOP_K} = {ndcg_r:.4f}\")\n",
    "hr_p, ndcg_p = evaluate.evaluate_popular(test_loader, train_df,top_k=TOP_K)\n",
    "print(f\"Popular HR@{TOP_K} = {hr_p:.4f}, NDCG@{TOP_K} = {ndcg_p:.4f}\")\n",
    "hr_m, ndcg_m = evaluate.evaluate_seq_model(test_loader, model, faiss_index, device,top_k=TOP_K,hist_tensors=hist_tensors)\n",
    "print(f\"Model   HR@{TOP_K} = {hr_m:.4f}, NDCG@{TOP_K} = {ndcg_m:.4f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Random HR@10 = 0.0006, nDCG@10 = 0.0003\n",
      "Popular HR@10 = 0.0029, nDCG@10 = 0.0014\n",
      "Model   HR@10 = 0.0764, nDCG@10 = 0.0375\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 挂载 Google Drive\n",
    "drive.mount('/content/drive')\n",
    "# 目标路径\n",
    "target_dir = None\n",
    "if(FUSION_MODE==\"base\"):\n",
    "    target_dir = f\"/content/drive/MyDrive/REC/{PROJECT_NAME}/{FUSION_MODE}/\"\n",
    "else:\n",
    "    target_dir = f\"/content/drive/MyDrive/REC/{PROJECT_NAME}/{FUSION_MODE}/{CURRENT_MODAL}\"\n",
    "# 创建目标路径（包含上层目录）\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "# 复制 records 到目标路径\n",
    "!cp -r /content/records \"{target_dir}\"\n",
    "!rm -rf /content/records"
   ],
   "id": "4b71161c3939a015"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "colab": {
   "provenance": [],
   "gpuType": "L4",
   "machine_shape": "hm"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

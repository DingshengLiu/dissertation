{
 "cells": [
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "df5fadd3b5922093",
    "outputId": "966e327b-6857-445e-ab10-6bbc6114fc49"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "skip:/content/tool exists\n",
      "skip:/content/MicroLens-50k_pairs.csv exists\n",
      "skip:/content/cover_emb128.lmdb exists\n",
      "skip:/content/title_emb1024.lmdb exists\n"
     ]
    }
   ],
   "execution_count": 113,
   "source": [
    "from google.colab import drive\n",
    "import shutil\n",
    "import os\n",
    "def copy_from_drive(src_path, dst_path):\n",
    "\n",
    "    if os.path.exists(dst_path):\n",
    "        print(f\"skip:{dst_path} exists\")\n",
    "        return\n",
    "\n",
    "    if os.path.isdir(src_path):\n",
    "        shutil.copytree(src_path, dst_path)\n",
    "    elif os.path.isfile(src_path):\n",
    "        shutil.copy(src_path, dst_path)\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "copy_from_drive('/content/drive/MyDrive/tool', '/content/tool')\n",
    "copy_from_drive('/content/drive/MyDrive/MicroLens-50k_pairs.csv','/content/MicroLens-50k_pairs.csv')\n",
    "copy_from_drive('/content/drive/MyDrive/cover_emb128.lmdb','/content/cover_emb128.lmdb')\n",
    "copy_from_drive('/content/drive/MyDrive/title_emb1024.lmdb','/content/title_emb1024.lmdb')\n",
    "\n"
   ],
   "id": "df5fadd3b5922093"
  },
  {
   "cell_type": "code",
   "id": "b828e981",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T18:42:28.345525Z",
     "start_time": "2025-06-12T18:42:18.772256Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b828e981",
    "outputId": "64397f0a-f0ff-4f20-896a-160578d43d31"
   },
   "source": [
    "!pip install faiss-cpu\n",
    "!pip install lmdb\n",
    "import os\n",
    "from tool import preprocess\n",
    "from tool import customdataset\n",
    "from tool import evaluate\n",
    "import faiss\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: lmdb in /usr/local/lib/python3.11/dist-packages (1.7.3)\n"
     ]
    }
   ],
   "execution_count": 114
  },
  {
   "metadata": {
    "id": "e53a0288b0498fe4",
    "outputId": "c421a187-79ef-4a93-a6a7-cdeaa8e14e38",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function tool.preprocess.set_seed.<locals>.seed_worker(worker_id)>"
      ],
      "text/html": [
       "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
       "      pre.function-repr-contents {\n",
       "        overflow-x: auto;\n",
       "        padding: 8px 12px;\n",
       "        max-height: 500px;\n",
       "      }\n",
       "\n",
       "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
       "        cursor: pointer;\n",
       "        max-height: 100px;\n",
       "      }\n",
       "    </style>\n",
       "    <pre style=\"white-space: initial; background:\n",
       "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
       "         border-bottom: 1px solid var(--colab-border-color);\"><b>tool.preprocess.set_seed.&lt;locals&gt;.seed_worker</b><br/>def seed_worker(worker_id)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/tool/preprocess.py</a>&lt;no docstring&gt;</pre>\n",
       "      <script>\n",
       "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
       "        for (const element of document.querySelectorAll('.filepath')) {\n",
       "          element.style.display = 'block'\n",
       "          element.onclick = (event) => {\n",
       "            event.preventDefault();\n",
       "            event.stopPropagation();\n",
       "            google.colab.files.view(element.textContent, 18);\n",
       "          };\n",
       "        }\n",
       "      }\n",
       "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
       "        element.onclick = (event) => {\n",
       "          event.preventDefault();\n",
       "          event.stopPropagation();\n",
       "          element.classList.toggle('function-repr-contents-collapsed');\n",
       "        };\n",
       "      }\n",
       "      </script>\n",
       "      </div>"
      ]
     },
     "metadata": {},
     "execution_count": 115
    }
   ],
   "execution_count": 115,
   "source": [
    "preprocess.set_seed(42)"
   ],
   "id": "e53a0288b0498fe4"
  },
  {
   "cell_type": "code",
   "id": "e99a31348e0a553",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T18:42:28.396608Z",
     "start_time": "2025-06-12T18:42:28.351537Z"
    },
    "id": "e99a31348e0a553"
   },
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "execution_count": 116
  },
  {
   "cell_type": "code",
   "id": "76a2087d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T18:42:29.259369Z",
     "start_time": "2025-06-12T18:42:29.105143Z"
    },
    "id": "76a2087d"
   },
   "source": [
    "path = 'MicroLens-50k_pairs.csv'\n",
    "user = 'user'\n",
    "item = 'item'\n",
    "user_id = 'user_id'\n",
    "item_id = 'item_id'\n",
    "timestamp = 'timestamp'\n",
    "save_dir = './embeddings'\n",
    "title_lmdb_path = 'title_emb1024.lmdb'\n",
    "cover_lmdb_path = 'cover_emb128.lmdb'\n",
    "top_k = 10\n",
    "num_workers = 10\n",
    "k_neg = 10\n",
    "# path = pd.read_csv('MicroLens-50k_pairs.csv')"
   ],
   "outputs": [],
   "execution_count": 117
  },
  {
   "cell_type": "code",
   "id": "943fc6c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T18:42:29.290418Z",
     "start_time": "2025-06-12T18:42:29.273675Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "943fc6c5",
    "outputId": "f7d4b60b-2c2e-40c7-f61f-421d3255f309"
   },
   "source": [
    "dataset_pd,num_users,num_items = preprocess.openAndSort(path,user_id=user,item_id=item,timestamp='timestamp')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dataset base information：\n",
      "- number of users：50000\n",
      "- number of items：19220\n",
      "- number of rows：359708\n"
     ]
    }
   ],
   "execution_count": 118
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c1952473721f3f99",
    "outputId": "6e22cc6c-4efb-44e4-ba8b-1d67b2f8dc83"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train size: 309708\n",
      "Test size: 49424\n"
     ]
    }
   ],
   "execution_count": 119,
   "source": [
    "train_df, test_df = preprocess.split(dataset_pd,user, item, timestamp)\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Test size: {len(test_df)}\")"
   ],
   "id": "c1952473721f3f99"
  },
  {
   "cell_type": "code",
   "id": "69160caa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T18:42:29.358698Z",
     "start_time": "2025-06-12T18:42:29.349731Z"
    },
    "id": "69160caa"
   },
   "source": [
    "# maintain a map from new id to old id, new id for constructing matrix\n",
    "user2id = {u: i for i, u in enumerate(dataset_pd[user].unique())}\n",
    "item2id = {i: j for j, i in enumerate(dataset_pd[item].unique())}\n",
    "\n",
    "# apply to train_df and test_df\n",
    "train_df[user_id] = train_df[user].map(user2id)\n",
    "train_df[item_id] = train_df[item].map(item2id)\n",
    "test_df[user_id] = test_df[user].map(user2id)\n",
    "test_df[item_id] = test_df[item].map(item2id)\n",
    "\n",
    "# 1. 构建 item_id 到 item 的映射（来自 train_df）\n",
    "item_id_to_item = {v: k for k, v in item2id.items()}"
   ],
   "outputs": [],
   "execution_count": 120
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T18:42:31.415745Z",
     "start_time": "2025-06-12T18:42:31.391214Z"
    },
    "id": "6e56658ac26b07e2"
   },
   "cell_type": "code",
   "source": [
    "# ---------- 超参数 ----------\n",
    "MAX_SEQ_LEN   = 3          # 序列长度\n",
    "EMBEDDING_DIM = 64          # item / user embedding 维度\n",
    "N_HEADS       = 1           # Multi-Head Attention 头数\n",
    "N_LAYERS      = 2           # Transformer block 层数\n",
    "DROPOUT       = 0.1\n",
    "NEG_SAMPLE    = 5\n",
    "BATCH_SIZE    = 1024\n",
    "EPOCHS        = 20\n",
    "LR            = 1e-3\n",
    "SEED          = 42\n",
    "TITLE_LMDB_DIM = 1024\n",
    "COVER_LMDB_DIM = 128\n",
    "MULTY_HIDDEN_SIZE=[EMBEDDING_DIM]\n",
    "MULTY_DROPOUT = 0.2\n",
    "L2_NORM = False\n",
    "# ----------------------------\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# ---------- 常量（来自你已有变量） ----------\n",
    "PAD_IDX = num_items          # 专用 padding id\n",
    "N_ITEMS = num_items + 1      # Embedding 行数（含 PAD）\n",
    "\n",
    "# -------------------------------------------\n"
   ],
   "id": "6e56658ac26b07e2",
   "outputs": [],
   "execution_count": 121
  },
  {
   "cell_type": "code",
   "id": "a7aba75c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T18:42:31.455790Z",
     "start_time": "2025-06-12T18:42:31.445397Z"
    },
    "id": "a7aba75c"
   },
   "source": [
    "# ======== SASRec 模型 ======== #\n",
    "class SASRec(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_items=N_ITEMS,\n",
    "                 embedding_dim=EMBEDDING_DIM,\n",
    "                 n_heads=N_HEADS,\n",
    "                 n_layers=N_LAYERS,\n",
    "                 max_len=MAX_SEQ_LEN,\n",
    "                 pad_idx=PAD_IDX,\n",
    "                 dropout=DROPOUT,\n",
    "                 title_lmdb_dim=TITLE_LMDB_DIM,\n",
    "                 cover_lmdb_dim=COVER_LMDB_DIM,\n",
    "                 multy_hidden_size=MULTY_HIDDEN_SIZE,\n",
    "                 multy_dropout=MULTY_DROPOUT,):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_items, embedding_dim, padding_idx=pad_idx)\n",
    "        # self.pos_emb  = nn.Embedding(max_len, embedding_dim)\n",
    "        self.pos_emb = nn.Embedding(max_len + 1, embedding_dim, padding_idx=0)\n",
    "\n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=embedding_dim*4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation='gelu')\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "\n",
    "        # 为 causal mask 预生成上三角矩阵\n",
    "        self.register_buffer(\n",
    "            \"mask\", torch.triu(torch.ones(max_len, max_len), diagonal=1).bool())\n",
    "\n",
    "        nn.init.normal_(self.embedding.weight, mean=0.0, std=0.05)\n",
    "        nn.init.normal_(self.pos_emb.weight, mean=0.0, std=0.05)\n",
    "\n",
    "        # load title_emb_tensor from lmdb\n",
    "        self.title_emb_tensor = preprocess.load_tensor_from_lmdb(title_lmdb_path,num_items,item_id_to_item,title_lmdb_dim)\n",
    "        # load cover_emb_tensor from lmdb\n",
    "        self.cover_emb_tensor = preprocess.load_tensor_from_lmdb(cover_lmdb_path,num_items,item_id_to_item,cover_lmdb_dim)\n",
    "        pad_vec = torch.zeros(1, title_lmdb_dim)\n",
    "        self.title_emb_tensor = torch.cat([self.title_emb_tensor, pad_vec], dim=0)\n",
    "        pad_vec = torch.zeros(1, cover_lmdb_dim)\n",
    "        self.cover_emb_tensor = torch.cat([self.cover_emb_tensor, pad_vec], dim=0)\n",
    "        # 注册为 buffer，表示该参数不参与梯度更新\n",
    "        self.register_buffer('frozen_extra_emb', self.title_emb_tensor)\n",
    "        self.register_buffer('frozen_extra_emb', self.cover_emb_tensor)\n",
    "\n",
    "        # 将item+title映射回item原有的维度\n",
    "        self.mlp_item_title = self.build_mlp(embedding_dim+title_lmdb_dim+cover_lmdb_dim, multy_hidden_size, multy_dropout)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        \"\"\"\n",
    "        seq: (B, T)  用户的历史 item 序列（左侧 padding）\n",
    "        返回每个用户的最终表征向量: (B, D)\n",
    "        \"\"\"\n",
    "\n",
    "        B, T = seq.size()\n",
    "        device = seq.device\n",
    "\n",
    "        # 1. item embedding\n",
    "        item_emb = self.embedding(seq)  # (B, T, D)\n",
    "        title = self.title_emb_tensor.to(seq.device)[seq]\n",
    "        cover = self.cover_emb_tensor.to(seq.device)[seq]\n",
    "        item_emb = torch.cat([item_emb, title, cover], dim=-1) # (B, T, emb_dim+lmdb_dim)\n",
    "        item_emb = self.mlp_item_multy(item_emb)          # (B, T, emb_dim)\n",
    "        # 2. 位置编码 —— PAD 位的位置设为 0，其余为 1..T\n",
    "        pos_ids = torch.arange(1, T + 1, device=device).unsqueeze(0).expand(B, -1)  # (B, T)\n",
    "        pos_ids = pos_ids * (seq != self.embedding.padding_idx)  # pad → 0\n",
    "        pos_emb = self.pos_emb(pos_ids)\n",
    "\n",
    "        x = item_emb + pos_emb\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # 3. causal attention mask\n",
    "        causal_mask = self.mask[:T, :T]  # (T, T)\n",
    "        key_padding_mask = (seq == self.embedding.padding_idx)  # (B, T)\n",
    "\n",
    "        x = self.encoder(x, mask=causal_mask, src_key_padding_mask=key_padding_mask)  # (B, T, D)\n",
    "\n",
    "        # 4. 取每个序列中最后一个非 PAD 的位置的输出\n",
    "        seq_lens = (seq != self.embedding.padding_idx).sum(dim=1).clamp(min=1)  # (B,)\n",
    "        last_idx = (seq_lens - 1).unsqueeze(1).unsqueeze(2).expand(-1, 1, x.size(-1))  # (B, 1, D)\n",
    "        h = x.gather(dim=1, index=last_idx).squeeze(1)  # (B, D)\n",
    "        if L2_NORM:\n",
    "            h = F.normalize(h, p=2, dim=1)\n",
    "        return h\n",
    "\n",
    "    def build_mlp(self, input_dim, hidden_sizes, dropout):\n",
    "          layers = []\n",
    "          for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(input_dim, h))\n",
    "            layers.append(nn.LayerNorm(h))\n",
    "            layers.append(nn.Tanh())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            input_dim = h\n",
    "          return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def get_items_embedding(self,item_ids,l2_norm=False):\n",
    "        i_vec = self.embedding(item_ids)          # (B, emb_dim)\n",
    "        title = self.title_emb_tensor.to(item_ids.device)[item_ids]\n",
    "        cover = self.cover_emb_tensor.to(item_ids.device)[item_ids]\n",
    "        i_vec = torch.cat([i_vec, title, cover], dim=-1)\n",
    "        i_vec = self.mlp_item_multy(i_vec)\n",
    "        if l2_norm:\n",
    "            i_vec = F.normalize(i_vec, p=2, dim=1)\n",
    "        return i_vec\n",
    "\n",
    "    def save_embeddings(self, num_users, num_items, device, save_dir='./embeddings',l2_norm=False):\n",
    "        import os\n",
    "        import faiss\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        self.eval()\n",
    "        self.to(device)\n",
    "\n",
    "        item_ids = torch.arange(num_items, dtype=torch.long, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            item_embeds = self.get_items_embedding(item_ids, l2_norm=l2_norm)\n",
    "\n",
    "        item_embeds = item_embeds.cpu().numpy().astype(np.float32)\n",
    "\n",
    "        # 保存向量\n",
    "        np.save(f\"{save_dir}/item_embeddings.npy\", item_embeds)\n",
    "\n",
    "        # 构建 FAISS index（使用内积）\n",
    "        dim = item_embeds.shape[1]\n",
    "        index = faiss.IndexFlatIP(dim)\n",
    "        index.add(item_embeds)\n",
    "\n",
    "        faiss.write_index(index, f\"{save_dir}/item_index.faiss\")\n",
    "        print(\"Saved user/item embeddings and FAISS index.\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 122
  },
  {
   "cell_type": "code",
   "id": "07e268c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T18:42:31.532424Z",
     "start_time": "2025-06-12T18:42:31.522660Z"
    },
    "id": "07e268c2"
   },
   "source": [
    "# ======== 训练流程 ======== #\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "def train_model(model, train_df, epochs,lr , batch_size, test_df=None, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "    train_loader  = customdataset.build_seq_loader(train_df, batch_size=batch_size,\n",
    "                         shuffle=True, num_workers=10,pad_idx=PAD_IDX,max_len=MAX_SEQ_LEN,user_id=user_id,item_id=item_id)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        dt_start = datetime.now()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            hist, pos = batch\n",
    "            hist, pos = hist.to(device), pos.to(device)\n",
    "\n",
    "            # 1. 前向传播（返回预测向量）\n",
    "            predict = model(hist)                      # (B, D)\n",
    "            i_vec = model.get_items_embedding(pos,l2_norm=True)\n",
    "\n",
    "            # 2. 得分矩阵：每个 user 对所有正 item 的打分\n",
    "            logits = torch.matmul(predict, i_vec.T)  # shape: (B, B)\n",
    "\n",
    "            # 3. 构造标签：每个 user 的正确 item 在对角线（即位置 i）\n",
    "            labels = torch.arange(logits.size(0), device=device)  # [0, 1, ..., B-1]\n",
    "\n",
    "            # 4. Cross Entropy Loss\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "            # 5. 反向传播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # 日志\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        dt_end = datetime.now()\n",
    "        dt = dt_end - dt_start\n",
    "\n",
    "        print(f\"[Epoch {epoch:02d}/{epochs}] avg InBatch Softmax Loss = {avg_loss:.4f}, time = {dt.total_seconds():.2f}s\")\n",
    "\n",
    "    return\n"
   ],
   "outputs": [],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T18:42:31.567359Z",
     "start_time": "2025-06-12T18:42:31.561346Z"
    },
    "id": "9bc49db2aee62849"
   },
   "cell_type": "code",
   "source": [
    "def build_hist_matrix(df,\n",
    "                      num_users,\n",
    "                      max_len=MAX_SEQ_LEN,\n",
    "                      pad_idx=PAD_IDX,\n",
    "                      user_col=user_id,\n",
    "                      item_col=item_id):\n",
    "    \"\"\"\n",
    "    返回形状为 (num_users, max_len) 的 LongTensor。\n",
    "    第 i 行是用户 i 的历史序列，左侧 PAD，右对齐。\n",
    "    不存在历史的用户整行都是 pad_idx。\n",
    "    \"\"\"\n",
    "    # 先全部填 PAD\n",
    "    hist = torch.full((num_users, max_len), pad_idx, dtype=torch.long)\n",
    "\n",
    "    # groupby 遍历每个用户已有交互\n",
    "    for uid, items in df.groupby(user_col)[item_col]:\n",
    "        seq = items.to_numpy()[-max_len:]             # 取最近 max_len 条\n",
    "        hist[uid, -len(seq):] = torch.as_tensor(seq, dtype=torch.long)\n",
    "\n",
    "    return hist    # (U, T)\n"
   ],
   "id": "9bc49db2aee62849",
   "outputs": [],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T21:01:52.253176Z",
     "start_time": "2025-06-12T21:01:52.245281Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "acd41e7d550823fb",
    "outputId": "946855a1-eb2e-4274-ab39-f854152fc723"
   },
   "cell_type": "code",
   "source": [
    "model = SASRec(n_items=N_ITEMS,\n",
    "                 embedding_dim=EMBEDDING_DIM,\n",
    "                 pad_idx=PAD_IDX)\n",
    "model = model.to(device)\n",
    "train_model(model=model,epochs=EPOCHS, train_df=train_df,batch_size=BATCH_SIZE,lr=LR,test_df=test_df,device=device)"
   ],
   "id": "acd41e7d550823fb",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Epoch 01/25] avg InBatch Softmax Loss = 6.5833, time = 11.50s\n",
      "[Epoch 02/25] avg InBatch Softmax Loss = 6.0544, time = 11.57s\n",
      "[Epoch 03/25] avg InBatch Softmax Loss = 5.8194, time = 11.32s\n",
      "[Epoch 04/25] avg InBatch Softmax Loss = 5.7163, time = 11.47s\n",
      "[Epoch 05/25] avg InBatch Softmax Loss = 5.6514, time = 11.37s\n",
      "[Epoch 06/25] avg InBatch Softmax Loss = 5.6032, time = 11.33s\n",
      "[Epoch 07/25] avg InBatch Softmax Loss = 5.5641, time = 11.35s\n",
      "[Epoch 08/25] avg InBatch Softmax Loss = 5.5326, time = 11.25s\n",
      "[Epoch 09/25] avg InBatch Softmax Loss = 5.5037, time = 11.29s\n",
      "[Epoch 10/25] avg InBatch Softmax Loss = 5.4788, time = 11.21s\n",
      "[Epoch 11/25] avg InBatch Softmax Loss = 5.4580, time = 11.17s\n",
      "[Epoch 12/25] avg InBatch Softmax Loss = 5.4378, time = 11.34s\n",
      "[Epoch 13/25] avg InBatch Softmax Loss = 5.4190, time = 11.26s\n",
      "[Epoch 14/25] avg InBatch Softmax Loss = 5.3994, time = 11.26s\n",
      "[Epoch 15/25] avg InBatch Softmax Loss = 5.3806, time = 11.24s\n",
      "[Epoch 16/25] avg InBatch Softmax Loss = 5.3600, time = 11.27s\n",
      "[Epoch 17/25] avg InBatch Softmax Loss = 5.3391, time = 11.27s\n",
      "[Epoch 18/25] avg InBatch Softmax Loss = 5.3202, time = 11.30s\n",
      "[Epoch 19/25] avg InBatch Softmax Loss = 5.3013, time = 11.30s\n",
      "[Epoch 20/25] avg InBatch Softmax Loss = 5.2851, time = 11.22s\n",
      "[Epoch 21/25] avg InBatch Softmax Loss = 5.2697, time = 11.25s\n",
      "[Epoch 22/25] avg InBatch Softmax Loss = 5.2514, time = 11.26s\n",
      "[Epoch 23/25] avg InBatch Softmax Loss = 5.2388, time = 11.25s\n",
      "[Epoch 24/25] avg InBatch Softmax Loss = 5.2236, time = 11.24s\n",
      "[Epoch 25/25] avg InBatch Softmax Loss = 5.2104, time = 11.20s\n"
     ]
    }
   ],
   "execution_count": 125
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fad220ca31a695e9",
    "outputId": "3dbd98c7-9eb9-4008-b1c5-d151538cd777"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved user/item embeddings and FAISS index.\n"
     ]
    }
   ],
   "execution_count": 126,
   "source": [
    "model.save_embeddings(num_users=num_users,num_items=num_items,device=device,save_dir=save_dir,l2_norm=L2_NORM)\n"
   ],
   "id": "fad220ca31a695e9"
  },
  {
   "metadata": {
    "id": "1795184be9664f14"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 127,
   "source": [
    "test_loader = customdataset.build_test_loader(test_df, num_items ,user_col = user_id, item_col = item_id, batch_size=1024, num_workers=num_workers)\n",
    "item_pool = list(range(num_items))\n",
    "faiss_index = faiss.read_index(f\"{save_dir}/item_index.faiss\")\n",
    "hist_tensors = build_hist_matrix(train_df, max_len=MAX_SEQ_LEN, pad_idx=PAD_IDX,num_users=num_users).to(device)"
   ],
   "id": "1795184be9664f14"
  },
  {
   "cell_type": "code",
   "id": "bbd4f9d7af8b5c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T23:16:30.822603Z",
     "start_time": "2025-06-12T21:10:31.413734Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bbd4f9d7af8b5c3",
    "outputId": "64152580-2915-45d3-855a-bcec06476dce"
   },
   "source": [
    "hr_r, ndcg_r = evaluate.evaluate_random(test_loader, item_pool ,top_k=top_k)\n",
    "print(f\"Random HR@{top_k} = {hr_r:.4f}, nDCG@{top_k} = {ndcg_r:.4f}\")\n",
    "hr_p, ndcg_p = evaluate.evaluate_popular(test_loader, train_df,top_k=top_k)\n",
    "print(f\"Popular HR@{top_k} = {hr_p:.4f}, nDCG@{top_k} = {ndcg_p:.4f}\")\n",
    "hr_m, ndcg_m = evaluate.evaluate_seq_model(test_loader, model, faiss_index, device,top_k=top_k,hist_tensors=hist_tensors)\n",
    "print(f\"Model   HR@{top_k} = {hr_m:.4f}, nDCG@{top_k} = {ndcg_m:.4f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Random HR@10 = 0.0005, nDCG@10 = 0.0002\n",
      "Popular HR@10 = 0.0029, nDCG@10 = 0.0014\n",
      "Model   HR@10 = 0.0769, nDCG@10 = 0.0395\n"
     ]
    }
   ],
   "execution_count": 128
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "colab": {
   "provenance": [],
   "gpuType": "L4",
   "machine_shape": "hm"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

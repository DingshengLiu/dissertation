{
 "cells": [
  {
   "cell_type": "code",
   "id": "b828e981",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:22.323907Z",
     "start_time": "2025-06-30T21:03:22.316403Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from networkx.readwrite.json_graph import adjacency\n",
    "import random, math, time, os\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "id": "e99a31348e0a553",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:22.357739Z",
     "start_time": "2025-06-30T21:03:22.351438Z"
    }
   },
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "76a2087d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:22.545734Z",
     "start_time": "2025-06-30T21:03:22.390688Z"
    }
   },
   "source": [
    "dataset_pd = pd.read_csv('D:\\\\VideoRecSystem\\\\MicroLens\\\\DataSet\\\\MicroLens-50k_pairs.csv')\n",
    "# dataset_pd = pd.read_csv('MicroLens-50k_pairs.csv')"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "id": "943fc6c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:22.592829Z",
     "start_time": "2025-06-30T21:03:22.578828Z"
    }
   },
   "source": [
    "dataset_pd.head(10)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    user   item      timestamp\n",
       "0  36121   9580  1583378629552\n",
       "1  26572   9580  1583436719018\n",
       "2  37550   9580  1584412681021\n",
       "3  14601   9580  1584848802432\n",
       "4  15061   9580  1585388171106\n",
       "5   6364   9580  1585390736041\n",
       "6   3542   9580  1585404918503\n",
       "7  21038   9580  1590144594477\n",
       "8  12538  14631  1634867362929\n",
       "9  47592  14631  1634872254913"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36121</td>\n",
       "      <td>9580</td>\n",
       "      <td>1583378629552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26572</td>\n",
       "      <td>9580</td>\n",
       "      <td>1583436719018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37550</td>\n",
       "      <td>9580</td>\n",
       "      <td>1584412681021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14601</td>\n",
       "      <td>9580</td>\n",
       "      <td>1584848802432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15061</td>\n",
       "      <td>9580</td>\n",
       "      <td>1585388171106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6364</td>\n",
       "      <td>9580</td>\n",
       "      <td>1585390736041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3542</td>\n",
       "      <td>9580</td>\n",
       "      <td>1585404918503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>21038</td>\n",
       "      <td>9580</td>\n",
       "      <td>1590144594477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12538</td>\n",
       "      <td>14631</td>\n",
       "      <td>1634867362929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>47592</td>\n",
       "      <td>14631</td>\n",
       "      <td>1634872254913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "id": "69160caa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:22.712817Z",
     "start_time": "2025-06-30T21:03:22.701358Z"
    }
   },
   "source": [
    "dataset_pd.count"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.count of          user   item      timestamp\n",
       "0       36121   9580  1583378629552\n",
       "1       26572   9580  1583436719018\n",
       "2       37550   9580  1584412681021\n",
       "3       14601   9580  1584848802432\n",
       "4       15061   9580  1585388171106\n",
       "...       ...    ...            ...\n",
       "359703  48702   1363  1662984066647\n",
       "359704  27203   7291  1662984082974\n",
       "359705  29261  19649  1662984103874\n",
       "359706  28341  19188  1662984123833\n",
       "359707  38967   7254  1662984132429\n",
       "\n",
       "[359708 rows x 3 columns]>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "id": "6d42c375",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:22.944391Z",
     "start_time": "2025-06-30T21:03:22.901686Z"
    }
   },
   "source": [
    "user_counts = dataset_pd['user'].value_counts()\n",
    "item_counts = dataset_pd['item'].value_counts()\n",
    "# valid_users = user_counts[user_counts > 3].index\n",
    "# valid_items = item_counts[item_counts > 3].index\n",
    "# filtered_df = dataset_pd[dataset_pd['user'].isin(valid_users) & dataset_pd['item'].isin(valid_items)]\n",
    "# filtered_df.count"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:23.029784Z",
     "start_time": "2025-06-30T21:03:23.025899Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "34f9b11c265ba767",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8b8ae0c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:23.289458Z",
     "start_time": "2025-06-30T21:03:23.071574Z"
    }
   },
   "source": [
    "# order by user,timestamp \n",
    "filtered_df = dataset_pd.sort_values(by=[\"user\", \"timestamp\"])\n"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "id": "2d2d4de256fd88a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:23.376265Z",
     "start_time": "2025-06-30T21:03:23.365752Z"
    }
   },
   "source": [
    "def split(df, user_col='user', item_col='item', time_col='timestamp'):\n",
    "\n",
    "    df = df.sort_values(by=[user_col, time_col])  # 按用户时间排序\n",
    "\n",
    "    # 获取每个用户的最后一条记录作为 test\n",
    "    test_df = df.groupby(user_col).tail(1)\n",
    "    train_df = df.drop(index=test_df.index)\n",
    "\n",
    "    # 过滤 test 中那些 user/item 不在 train 中的\n",
    "    train_users = set(train_df[user_col])\n",
    "    train_items = set(train_df[item_col])\n",
    "\n",
    "    test_df = test_df[\n",
    "        test_df[user_col].isin(train_users) &\n",
    "        test_df[item_col].isin(train_items)\n",
    "    ]\n",
    "\n",
    "    return train_df.reset_index(drop=True), test_df.reset_index(drop=True)"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "id": "3db1146761e45165",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:23.948140Z",
     "start_time": "2025-06-30T21:03:23.437082Z"
    }
   },
   "source": [
    "\n",
    "train_df, test_df = split(filtered_df,user_col='user', item_col='item', time_col='timestamp')\n",
    "\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Test size: {len(test_df)}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 309708\n",
      "Test size: 49424\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "id": "ce079817",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:25.012678Z",
     "start_time": "2025-06-30T21:03:23.955156Z"
    }
   },
   "source": [
    "# maintain a map from new id to old id, new id for constructing matrix\n",
    "user2id = {u: i for i, u in enumerate(filtered_df['user'].unique())}\n",
    "item2id = {i: j for j, i in enumerate(filtered_df['item'].unique())}\n",
    "\n",
    "# apply to train_df and test_df\n",
    "train_df['user_id'] = train_df['user'].map(user2id)\n",
    "train_df['item_id'] = train_df['item'].map(item2id)\n",
    "test_df['user_id'] = test_df['user'].map(user2id)\n",
    "test_df['item_id'] = test_df['item'].map(item2id)\n",
    "\n",
    "num_users = len(user2id)\n",
    "num_items = len(item2id)\n"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:25.080951Z",
     "start_time": "2025-06-30T21:03:25.048951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import lmdb\n",
    "\n",
    "def load_lmdb_embeddings(lmdb_path, num_items, emb_dim=128):\n",
    "    \"\"\"读取 LMDB 中的 128 维 cover 向量，key 从 1 开始计数\"\"\"\n",
    "    env = lmdb.open(lmdb_path, readonly=True, lock=False,subdir=False)\n",
    "    cover_embs = np.zeros((num_items + 1, emb_dim), dtype=np.float32)  # index 0 保留给 PAD\n",
    "    with env.begin() as txn:\n",
    "        for idx in range(1, num_items + 1):\n",
    "            val = txn.get(str(idx).encode('ascii'))\n",
    "            if val is not None:\n",
    "                cover_embs[idx] = np.frombuffer(val, dtype=np.float32)\n",
    "    env.close()\n",
    "    return torch.tensor(cover_embs)\n",
    "\n",
    "COVER_EMB_PATH = r\"D:/VideoRecSystem/MicroLens/cover_emb128.lmdb\"\n",
    "print(f\"Loading cover embeddings from {COVER_EMB_PATH} ...\")\n",
    "COVER_EMBS = load_lmdb_embeddings(COVER_EMB_PATH, num_items=num_items, emb_dim=128)\n",
    "print(\"COVER_EMBS shape:\", COVER_EMBS.shape)  # (num_items+1, 128)"
   ],
   "id": "cffbe3ff97a5d1a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cover embeddings from D:/VideoRecSystem/MicroLens/cover_emb128.lmdb ...\n",
      "COVER_EMBS shape: torch.Size([19221, 128])\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:25.156012Z",
     "start_time": "2025-06-30T21:03:25.128486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------- 超参数 ----------\n",
    "MAX_SEQ_LEN   = 20          # 序列长度\n",
    "EMBED_DIM     = 64          # item / user embedding 维度\n",
    "N_HEADS       = 2           # Multi-Head Attention 头数\n",
    "N_LAYERS      = 2           # Transformer block 层数\n",
    "DROPOUT       = 0.2\n",
    "NEG_SAMPLE    = 5\n",
    "BATCH_SIZE    = 512\n",
    "EPOCHS        = 10\n",
    "LR            = 1e-3\n",
    "SEED          = 42\n",
    "# ----------------------------\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# ---------- 常量（来自你已有变量） ----------\n",
    "PAD_IDX = num_items          # 专用 padding id\n",
    "N_ITEMS = num_items + 1      # Embedding 行数（含 PAD）\n",
    "ALL_ITEM_IDS = np.arange(num_items, dtype=np.int64)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# -------------------------------------------\n",
    "\n",
    "# ======== Dataset (同前) ======== #\n",
    "# ======== Dataset：与前一致 ======== #\n",
    "class SASRecBPRDataset(Dataset):\n",
    "    def __init__(self, df, max_len=MAX_SEQ_LEN, pad_idx=PAD_IDX, n_neg=NEG_SAMPLE):\n",
    "        self.max_len, self.pad_idx, self.n_neg = max_len, pad_idx, n_neg\n",
    "        self.inputs, self.targets = [], []\n",
    "\n",
    "        for _, hist in df.groupby('user_id'):\n",
    "            seq = hist['item_id'].tolist()\n",
    "            for i in range(1, len(seq)):\n",
    "                s = seq[max(0, i-max_len): i]\n",
    "                s = [pad_idx]*(max_len-len(s)) + s\n",
    "                self.inputs.append(s)\n",
    "                self.targets.append(seq[i])\n",
    "\n",
    "        self.inputs  = np.asarray(self.inputs,  dtype=np.int64)\n",
    "        self.targets = np.asarray(self.targets, dtype=np.int64)\n",
    "\n",
    "    def __len__(self): return len(self.targets)\n",
    "\n",
    "    def _neg(self, pos):\n",
    "        negs = np.random.choice(ALL_ITEM_IDS, size=self.n_neg, replace=False)\n",
    "        while (negs == pos).any():\n",
    "            dup = negs == pos\n",
    "            negs[dup] = np.random.choice(ALL_ITEM_IDS, size=dup.sum(), replace=False)\n",
    "        return negs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        hist = torch.tensor(self.inputs[idx], dtype=torch.long)\n",
    "        pos  = torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "        negs = torch.tensor(self._neg(self.targets[idx]), dtype=torch.long)\n",
    "        return hist, pos, negs\n",
    "\n"
   ],
   "id": "6e56658ac26b07e2",
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "id": "a7aba75c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:25.216007Z",
     "start_time": "2025-06-30T21:03:25.199017Z"
    }
   },
   "source": [
    "# ======== SASRec 模型 ======== #\n",
    "class SASRec(nn.Module):\n",
    "    def __init__(self,\n",
    "                 cover_embs=None,\n",
    "                 n_items=N_ITEMS,\n",
    "                 dim=EMBED_DIM,\n",
    "                 n_heads=N_HEADS,\n",
    "                 n_layers=N_LAYERS,\n",
    "                 max_len=MAX_SEQ_LEN,\n",
    "                 pad_idx=PAD_IDX,\n",
    "                 dropout=DROPOUT):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # ① ID embedding（可训练）\n",
    "        self.id_emb   = nn.Embedding(n_items, dim, padding_idx=pad_idx)\n",
    "        # ② cover embedding（冻结）\n",
    "        self.cover_emb = nn.Embedding.from_pretrained(cover_embs, freeze=True)\n",
    "\n",
    "        # ③ 将拼接后的 (dim+128) → dim 的 1×1 线性层\n",
    "        self.in_proj = nn.Linear(dim + 128, dim, bias=False)\n",
    "\n",
    "\n",
    "        self.pos_emb  = nn.Embedding(max_len, dim)\n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=dim*4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation='gelu')\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "\n",
    "        # 为 causal mask 预生成上三角矩阵\n",
    "        self.register_buffer(\n",
    "            \"mask\", torch.triu(torch.ones(max_len, max_len), diagonal=1).bool())\n",
    "\n",
    "    def forward(self, seq):                       # seq : (B,T)\n",
    "        B, T = seq.size()\n",
    "        id_emb   = self.id_emb(seq)              # (B, T, D)\n",
    "        cover_vec= self.cover_emb(seq)           # (B, T, 128)\n",
    "        x = torch.cat([id_emb, cover_vec], -1)   # (B, T, D+128)\n",
    "        x = self.in_proj(x)                      # (B, T, D) —— 轻量压缩\n",
    "\n",
    "\n",
    "        pos_ids = torch.arange(T, device=seq.device).unsqueeze(0).expand(B, -1)\n",
    "        x = x + self.pos_emb(pos_ids)        # 加位置编码\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # causal attention mask\n",
    "        causal_mask = self.mask[:T, :T]\n",
    "        x = self.encoder(x, src_key_padding_mask=(seq == PAD_IDX), mask=causal_mask)\n",
    "        h = x[:, -1, :]                           # 取最后位置向量 (B,D)\n",
    "        return h\n",
    "\n",
    "    def score(self, h, items):\n",
    "        # item embedding 拼接后投影\n",
    "        item_id_vec = self.id_emb(items)         # (B, D)\n",
    "        item_cover_vec = self.cover_emb(items)   # (B, 128)\n",
    "        item_vec = torch.cat([item_id_vec, item_cover_vec], dim=-1)\n",
    "        item_vec = self.in_proj(item_vec)        # (B, D)\n",
    "\n",
    "        # === 加入 L2 规范化 ===\n",
    "        h = F.normalize(h, p=2, dim=-1)               # (B, D)\n",
    "        item_vec = F.normalize(item_vec, p=2, dim=-1) # (B, D)\n",
    "\n",
    "        return (h.unsqueeze(-2) * item_vec).sum(-1)   # cosine similarity\n"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "id": "f21704ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:25.266151Z",
     "start_time": "2025-06-30T21:03:25.258149Z"
    }
   },
   "source": [
    "# ======== BPR 损失 ======== #\n",
    "def bpr_loss(pos_s, neg_s):\n",
    "    return -torch.log(torch.sigmoid(pos_s.unsqueeze(-1) - neg_s) + 1e-8).mean()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "id": "07e268c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:25.317810Z",
     "start_time": "2025-06-30T21:03:25.305941Z"
    }
   },
   "source": [
    "# ======== 训练函数 ======== #\n",
    "def train_sasrec_bpr(train_df, test_df=None):\n",
    "    ds = SASRecBPRDataset(train_df)\n",
    "    loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                        num_workers=0, pin_memory=True)\n",
    "\n",
    "    model = SASRec(cover_embs=COVER_EMBS).to(DEVICE)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        tot_loss, t0 = 0.0, time.time()\n",
    "        for step, (hist, pos, neg) in enumerate(loader, 1):\n",
    "            hist, pos, neg = hist.to(DEVICE), pos.to(DEVICE), neg.to(DEVICE)\n",
    "            h = model(hist)\n",
    "            pos_s = model.score(h, pos)\n",
    "            neg_s = model.score(h, neg)\n",
    "            loss  = bpr_loss(pos_s, neg_s)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            tot_loss += loss.item()*hist.size(0)\n",
    "            if step % 100 == 0 or step == 1:\n",
    "                print(f\"[Ep {ep}] step {step}/{len(loader)} | loss {loss.item():.4f}\", flush=True)\n",
    "\n",
    "        print(f\"Ep {ep} done | avg loss {tot_loss/len(ds):.4f} | {time.time()-t0:.1f}s\\n\", flush=True)\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:25.361439Z",
     "start_time": "2025-06-30T21:03:25.353438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ======== 构建 (user → 历史张量) 缓存 ======== #\n",
    "def build_hist_cache(df):\n",
    "    cache = {}\n",
    "    for uid, items in df.groupby('user_id')['item_id']:\n",
    "        seq = items.tolist()[-MAX_SEQ_LEN:]\n",
    "        seq = [PAD_IDX]*(MAX_SEQ_LEN - len(seq)) + seq\n",
    "        cache[uid] = torch.tensor(seq, dtype=torch.long).unsqueeze(0)\n",
    "    return cache"
   ],
   "id": "9bc49db2aee62849",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:25.404137Z",
     "start_time": "2025-06-30T21:03:25.395297Z"
    }
   },
   "cell_type": "code",
   "source": "num_users",
   "id": "acd41e7d550823fb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "id": "10c90823",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:03:25.454773Z",
     "start_time": "2025-06-30T21:03:25.440889Z"
    }
   },
   "source": [
    "\n",
    "def evaluate_ranking(\n",
    "        test_df,              # DataFrame, 必含 user_id / item_id\n",
    "        train_df,             # DataFrame, 用来构建用户→已交互物品集合\n",
    "        score_fn,             # callable(users_tensor, items_tensor) → np.array\n",
    "        num_items,            # 物品总数\n",
    "        k=10,                 # Hit@K / NDCG@K\n",
    "        num_neg=10000,          # 每个正样本采多少负样本\n",
    "        user_col='user_id',\n",
    "        item_col='item_id',\n",
    "        seed=42\n",
    "    ):\n",
    "    \"\"\"\n",
    "    不依赖具体模型，只要提供 score_fn 就能评估。\n",
    "    score_fn: 接收 (user_tensor, item_tensor) 并返回同长度的 Numpy 分数向量。\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # 用户历史，用于采负样本 & 过滤\n",
    "    train_user_dict = (\n",
    "        train_df.groupby(user_col)[item_col].apply(set).to_dict()\n",
    "    )\n",
    "\n",
    "    hits, ndcgs = [], []\n",
    "\n",
    "    for _, row in test_df.iterrows():\n",
    "        u = int(row[user_col])\n",
    "        pos_item = int(row[item_col])\n",
    "\n",
    "        # ---------- 负采样 ----------\n",
    "        neg_items = set()\n",
    "        while len(neg_items) < num_neg:\n",
    "            neg = random.randint(0, num_items - 1)\n",
    "            if neg not in train_user_dict.get(u, set()) and neg != pos_item:\n",
    "                neg_items.add(neg)\n",
    "\n",
    "        item_candidates = list(neg_items) + [pos_item]\n",
    "\n",
    "        # ---------- 评分 ----------\n",
    "        users_t  = torch.LongTensor([u] * len(item_candidates))\n",
    "        items_t  = torch.LongTensor(item_candidates)\n",
    "        scores   = score_fn(users_t, items_t)        # ← 只依赖 score_fn\n",
    "        rank_idx = np.argsort(scores)[::-1]          # 降序\n",
    "        ranked_items = [item_candidates[i] for i in rank_idx]\n",
    "\n",
    "        # ---------- 指标 ----------\n",
    "        if pos_item in ranked_items[:k]:\n",
    "            hits.append(1)\n",
    "            rank_pos = ranked_items.index(pos_item)\n",
    "            ndcgs.append(1 / np.log2(rank_pos + 2))\n",
    "        else:\n",
    "            hits.append(0)\n",
    "            ndcgs.append(0)\n",
    "\n",
    "    hit_rate = float(np.mean(hits))\n",
    "    ndcg     = float(np.mean(ndcgs))\n",
    "    return hit_rate, ndcg"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "id": "cd29a40c68841c9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:35:46.695237Z",
     "start_time": "2025-06-30T21:03:25.491608Z"
    }
   },
   "source": [
    "    # ------------------ 训练 ------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model  = train_sasrec_bpr(train_df, test_df)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ep 1] step 1/508 | loss 0.7014\n",
      "[Ep 1] step 100/508 | loss 0.6991\n",
      "[Ep 1] step 200/508 | loss 0.6710\n",
      "[Ep 1] step 300/508 | loss 0.6583\n",
      "[Ep 1] step 400/508 | loss 0.6399\n",
      "[Ep 1] step 500/508 | loss 0.5916\n",
      "Ep 1 done | avg loss 0.6560 | 190.8s\n",
      "\n",
      "[Ep 2] step 1/508 | loss 0.5756\n",
      "[Ep 2] step 100/508 | loss 0.5705\n",
      "[Ep 2] step 200/508 | loss 0.5631\n",
      "[Ep 2] step 300/508 | loss 0.5554\n",
      "[Ep 2] step 400/508 | loss 0.5440\n",
      "[Ep 2] step 500/508 | loss 0.5611\n",
      "Ep 2 done | avg loss 0.5510 | 190.9s\n",
      "\n",
      "[Ep 3] step 1/508 | loss 0.5235\n",
      "[Ep 3] step 100/508 | loss 0.5029\n",
      "[Ep 3] step 200/508 | loss 0.5107\n",
      "[Ep 3] step 300/508 | loss 0.5192\n",
      "[Ep 3] step 400/508 | loss 0.5047\n",
      "[Ep 3] step 500/508 | loss 0.5132\n",
      "Ep 3 done | avg loss 0.5181 | 190.0s\n",
      "\n",
      "[Ep 4] step 1/508 | loss 0.5191\n",
      "[Ep 4] step 100/508 | loss 0.5208\n",
      "[Ep 4] step 200/508 | loss 0.5210\n",
      "[Ep 4] step 300/508 | loss 0.5091\n",
      "[Ep 4] step 400/508 | loss 0.5022\n",
      "[Ep 4] step 500/508 | loss 0.4900\n",
      "Ep 4 done | avg loss 0.5084 | 191.6s\n",
      "\n",
      "[Ep 5] step 1/508 | loss 0.4888\n",
      "[Ep 5] step 100/508 | loss 0.4883\n",
      "[Ep 5] step 200/508 | loss 0.5035\n",
      "[Ep 5] step 300/508 | loss 0.5060\n",
      "[Ep 5] step 400/508 | loss 0.5000\n",
      "[Ep 5] step 500/508 | loss 0.5100\n",
      "Ep 5 done | avg loss 0.5042 | 193.3s\n",
      "\n",
      "[Ep 6] step 1/508 | loss 0.5115\n",
      "[Ep 6] step 100/508 | loss 0.5087\n",
      "[Ep 6] step 200/508 | loss 0.4730\n",
      "[Ep 6] step 300/508 | loss 0.5052\n",
      "[Ep 6] step 400/508 | loss 0.5055\n",
      "[Ep 6] step 500/508 | loss 0.4923\n",
      "Ep 6 done | avg loss 0.5022 | 189.0s\n",
      "\n",
      "[Ep 7] step 1/508 | loss 0.4914\n",
      "[Ep 7] step 100/508 | loss 0.4951\n",
      "[Ep 7] step 200/508 | loss 0.4992\n",
      "[Ep 7] step 300/508 | loss 0.5059\n",
      "[Ep 7] step 400/508 | loss 0.4945\n",
      "[Ep 7] step 500/508 | loss 0.5035\n",
      "Ep 7 done | avg loss 0.5003 | 198.4s\n",
      "\n",
      "[Ep 8] step 1/508 | loss 0.4890\n",
      "[Ep 8] step 100/508 | loss 0.5076\n",
      "[Ep 8] step 200/508 | loss 0.4904\n",
      "[Ep 8] step 300/508 | loss 0.5174\n",
      "[Ep 8] step 400/508 | loss 0.5011\n",
      "[Ep 8] step 500/508 | loss 0.5008\n",
      "Ep 8 done | avg loss 0.5002 | 199.0s\n",
      "\n",
      "[Ep 9] step 1/508 | loss 0.5010\n",
      "[Ep 9] step 100/508 | loss 0.4770\n",
      "[Ep 9] step 200/508 | loss 0.5169\n",
      "[Ep 9] step 300/508 | loss 0.4741\n",
      "[Ep 9] step 400/508 | loss 0.5045\n",
      "[Ep 9] step 500/508 | loss 0.5002\n",
      "Ep 9 done | avg loss 0.4987 | 199.1s\n",
      "\n",
      "[Ep 10] step 1/508 | loss 0.4817\n",
      "[Ep 10] step 100/508 | loss 0.4823\n",
      "[Ep 10] step 200/508 | loss 0.5014\n",
      "[Ep 10] step 300/508 | loss 0.4950\n",
      "[Ep 10] step 400/508 | loss 0.5036\n",
      "[Ep 10] step 500/508 | loss 0.5032\n",
      "Ep 10 done | avg loss 0.4983 | 195.0s\n",
      "\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "id": "900b523c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:35:46.776635Z",
     "start_time": "2025-06-30T21:35:46.769683Z"
    }
   },
   "source": [
    "def make_popularity_score_fn(train_df, item_col='item_id'):\n",
    "    item_cnt = Counter(train_df[item_col])\n",
    "    default_score = min(item_cnt.values()) - 1  # 给没出现过的物品一个更低分\n",
    "    def _score_fn(users_t, items_t):\n",
    "        return np.array([item_cnt.get(int(i), default_score) for i in items_t])\n",
    "    return _score_fn"
   ],
   "outputs": [],
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "id": "994e2156",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:35:46.790666Z",
     "start_time": "2025-06-30T21:35:46.784666Z"
    }
   },
   "source": [
    "def random_score_fn(users_t, items_t):\n",
    "    # 随机给每个 items_t 一个分数；users_t 不使用，但必须接收\n",
    "    return np.random.rand(len(items_t))"
   ],
   "outputs": [],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "id": "4ef6ceb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T22:11:29.475429Z",
     "start_time": "2025-06-30T22:11:29.464439Z"
    }
   },
   "source": [
    "def make_score_fn(model, hist_cache):\n",
    "    \"\"\"\n",
    "    内存优化版：将 item embedding 缓存在 CPU，仅在使用时搬到 GPU。\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        id_mat    = model.id_emb.weight.data.cpu()       # (n_items+1, D)\n",
    "        cover_mat = model.cover_emb.weight.data.cpu()    # (n_items+1, 128)\n",
    "\n",
    "        # ——— 将 in_proj 暂时移动到 CPU 计算后再还原 ——— #\n",
    "        in_proj_cpu = model.in_proj.cpu()  # 临时版本\n",
    "\n",
    "        fused_mat = in_proj_cpu(\n",
    "            torch.cat([id_mat, cover_mat], dim=1)\n",
    "        ).cpu()  # 强制保持在 CPU\n",
    "\n",
    "        model.in_proj.to(device)  # 还原原模型的 in_proj 到 CUDA\n",
    "\n",
    "        fused_mat = F.normalize(fused_mat, p=2, dim=-1)\n",
    "\n",
    "        print(f\"[INFO] fused_mat.shape = {fused_mat.shape}, \"\n",
    "              f\"CPU memory = {fused_mat.element_size() * fused_mat.nelement() / 1e6:.2f} MB\")\n",
    "    @torch.no_grad()\n",
    "    def score_fn(user_t, item_t):\n",
    "        \"\"\"\n",
    "        user_t : (m,) 全为同一个用户 ID\n",
    "        item_t : (m,) 待评分 item ids\n",
    "        \"\"\"\n",
    "        uid = int(user_t[0])\n",
    "        hist_seq = hist_cache[uid].to(device)  # (1, T)\n",
    "\n",
    "        h = model(hist_seq)                    # (1, D)\n",
    "        h = F.normalize(h, p=2, dim=-1)        # (1, D)\n",
    "\n",
    "        # ——— 仅搬运用到的 item 表到 GPU ——— #\n",
    "        item_vec = fused_mat[item_t.cpu()]     # (m, D) on CPU\n",
    "        item_vec = item_vec.to(device)         # 搬到 CUDA\n",
    "\n",
    "        scores = (h * item_vec).sum(-1)        # (m,)\n",
    "        return scores.cpu().numpy()\n",
    "\n",
    "    return score_fn\n"
   ],
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def make_score_fn_GPU(model, hist_cache):\n",
    "    \"\"\"\n",
    "    正确设备统一版本，完全在 CUDA 上处理，显存可控。\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # —— 全在 CUDA 上构建 item 表征 —— #\n",
    "    with torch.no_grad():\n",
    "        id_mat    = model.id_emb.weight.to(device)         # (n_items+1, D)\n",
    "        cover_mat = model.cover_emb.weight.to(device)      # (n_items+1, 128)\n",
    "\n",
    "        fused_mat = model.in_proj(\n",
    "            torch.cat([id_mat, cover_mat], dim=1)          # → (n_items+1, D+128)\n",
    "        )  # → (n_items+1, D)\n",
    "        fused_mat = F.normalize(fused_mat, p=2, dim=-1)\n",
    "\n",
    "        print(f\"[INFO] fused_mat on CUDA: shape = {fused_mat.shape}, \"\n",
    "              f\"GPU memory = {fused_mat.element_size() * fused_mat.nelement() / 1e6:.2f} MB\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def score_fn(user_t, item_t):\n",
    "        uid = int(user_t[0])\n",
    "        hist_seq = hist_cache[uid].to(device)     # (1, T)\n",
    "\n",
    "        h = model(hist_seq)                       # (1, D)\n",
    "        h = F.normalize(h, p=2, dim=-1)\n",
    "\n",
    "        item_vec = fused_mat[item_t.to(device)]   # (m, D)\n",
    "        scores = (h * item_vec).sum(-1)\n",
    "        return scores.cpu().numpy()\n",
    "\n",
    "    return score_fn"
   ],
   "id": "c468cfa4306d3bca"
  },
  {
   "cell_type": "code",
   "id": "bbd4f9d7af8b5c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T23:10:53.293412Z",
     "start_time": "2025-06-30T22:11:31.944372Z"
    }
   },
   "source": [
    "\n",
    "# ----------------SASRec （或其他模型）------------\n",
    "score_fn_SASRec = make_score_fn(model,hist_cache=build_hist_cache(train_df))\n",
    "hit_SASRec, ndcg_SASRec = evaluate_ranking(\n",
    "    test_df, train_df, score_fn_SASRec,\n",
    "    num_items=num_items, k=10\n",
    ")\n",
    "print(f\"SASRec   Hit@10={hit_SASRec:.4f}  NDCG@10={ndcg_SASRec:.4f}\")\n",
    "\n",
    "# ---------------- baseline：Popular ----------------\n",
    "pop_score_fn  = make_popularity_score_fn(train_df)\n",
    "hit_pop, ndcg_pop = evaluate_ranking(\n",
    "    test_df, train_df, pop_score_fn,\n",
    "    num_items=num_items, k=10\n",
    ")\n",
    "print(f\"Popular  Hit@10={hit_pop:.4f}  NDCG@10={ndcg_pop:.4f}\")\n",
    "\n",
    "# ---------------- baseline：Random -----------------\n",
    "hit_rand, ndcg_rand = evaluate_ranking(\n",
    "    test_df, train_df, random_score_fn,\n",
    "    num_items=num_items, k=10\n",
    ")\n",
    "print(f\"Random   Hit@10={hit_rand:.4f}  NDCG@10={ndcg_rand:.4f}\")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fused_mat.shape = torch.Size([19221, 64]), CPU memory = 4.92 MB\n",
      "SASRec   Hit@10=0.9846  NDCG@10=0.9845\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[62], line 11\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# ---------------- baseline：Popular ----------------\u001B[39;00m\n\u001B[0;32m     10\u001B[0m pop_score_fn  \u001B[38;5;241m=\u001B[39m make_popularity_score_fn(train_df)\n\u001B[1;32m---> 11\u001B[0m hit_pop, ndcg_pop \u001B[38;5;241m=\u001B[39m evaluate_ranking(\n\u001B[0;32m     12\u001B[0m     test_df, train_df, pop_score_fn,\n\u001B[0;32m     13\u001B[0m     num_items\u001B[38;5;241m=\u001B[39mnum_items, k\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m\n\u001B[0;32m     14\u001B[0m )\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPopular  Hit@10=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhit_pop\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m  NDCG@10=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mndcg_pop\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# ---------------- baseline：Random -----------------\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[48], line 43\u001B[0m, in \u001B[0;36mevaluate_ranking\u001B[1;34m(test_df, train_df, score_fn, num_items, k, num_neg, user_col, item_col, seed)\u001B[0m\n\u001B[0;32m     41\u001B[0m users_t  \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mLongTensor([u] \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mlen\u001B[39m(item_candidates))\n\u001B[0;32m     42\u001B[0m items_t  \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mLongTensor(item_candidates)\n\u001B[1;32m---> 43\u001B[0m scores   \u001B[38;5;241m=\u001B[39m score_fn(users_t, items_t)        \u001B[38;5;66;03m# ← 只依赖 score_fn\u001B[39;00m\n\u001B[0;32m     44\u001B[0m rank_idx \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margsort(scores)[::\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]          \u001B[38;5;66;03m# 降序\u001B[39;00m\n\u001B[0;32m     45\u001B[0m ranked_items \u001B[38;5;241m=\u001B[39m [item_candidates[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m rank_idx]\n",
      "Cell \u001B[1;32mIn[50], line 5\u001B[0m, in \u001B[0;36mmake_popularity_score_fn.<locals>._score_fn\u001B[1;34m(users_t, items_t)\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_score_fn\u001B[39m(users_t, items_t):\n\u001B[1;32m----> 5\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39marray([item_cnt\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;28mint\u001B[39m(i), default_score) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m items_t])\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 62
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DP1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
